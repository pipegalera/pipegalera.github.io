<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
    <title>~&#x2F;.pipe_galera</title>
    <subtitle>This is an example description</subtitle>
    <link rel="self" type="application/atom+xml" href="https://pipegalera.github.io/atom.xml"/>
    <link rel="alternate" type="text/html" href="https://pipegalera.github.io"/>
    <generator uri="https://www.getzola.org/">Zola</generator>
    <updated>2024-05-10T00:00:00+00:00</updated>
    <id>https://pipegalera.github.io/atom.xml</id>
    <entry xml:lang="en">
        <title>Course Notes - Neural Networks and Deep Learning</title>
        <published>2024-05-10T00:00:00+00:00</published>
        <updated>2024-05-10T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/course-deeplearning-neural-networks/"/>
        <id>https://pipegalera.github.io/mostly_books/course-deeplearning-neural-networks/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/course-deeplearning-neural-networks/">&lt;h2 id=&quot;1-introduction-to-deep-learning&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-introduction-to-deep-learning&quot; aria-label=&quot;Anchor link for: 1-introduction-to-deep-learning&quot;&gt;1. Introduction to Deep Learning&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-1-what-is-a-neural-network&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-1-what-is-a-neural-network&quot; aria-label=&quot;Anchor link for: 1-1-what-is-a-neural-network&quot;&gt;1.1 What is a Neural Network?&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Neural Networks are collection of connected units or nodes called &quot;neurons&quot;, which loosely model the neurons in a biological brain. The combination of neurons &lt;em&gt;learn&lt;&#x2F;em&gt; to perform tasks by considering examples, without being programmed with task-specific rules.&lt;&#x2F;p&gt;
&lt;p&gt;As an example, consider that we have to predict the price of houses using their size. We have 6 houses in the dataset. Every neuron applies the function in grey:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;All the neuron does is taking the size of the house as input, apply a transformation equation called ReLU and the output is the estimated the price.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;ReLU, or Rectified Linear Unit, is a simple function that takes the input and returns the same value if positive, otherwise returns zero:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;relu&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;input&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if input &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;		&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return input
&lt;&#x2F;span&gt;&lt;span&gt;	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;else&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;		&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;All the neurons pass ReLU, or other similar kind of functions called “activation functions”.&lt;&#x2F;p&gt;
&lt;p&gt;A &lt;strong&gt;Neural Network (NN)&lt;&#x2F;strong&gt; is created by stacking neurons together. Each of the neurons stacked implements ReLU, or some other slightly non-linear function.&lt;&#x2F;p&gt;
&lt;p&gt;A NN needs a large number of inputs and their output, a lot of examples to find the pattern that explains the output.&lt;&#x2F;p&gt;
&lt;p&gt;In the example, a large dataset of houses prices associated with their size. The &quot;&lt;strong&gt;hidden layers&lt;&#x2F;strong&gt;&quot; are made by the neurons themselves.&lt;&#x2F;p&gt;
&lt;p&gt;The first layer, the &lt;strong&gt;input layer&lt;&#x2F;strong&gt;, and the hidden layers are connected: every input feature is connected to every &quot;hidden&quot; feature.&lt;&#x2F;p&gt;
&lt;p&gt;The NN feeds the features size, the number of bedroom, zipcode, wealth and so to &quot;hidden units&quot; that explains the prize. This hidden units are made by the neurons without explicitly telling them. Therefore, some times you can find these models refered as &quot;&lt;strong&gt;black boxes&lt;&#x2F;strong&gt;” due to the process not being explicit in the hidden layers:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For example, the NN is modelling “family size” given the combination of size of size and number of bedrooms.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-2-supervised-learning-with-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-2-supervised-learning-with-neural-networks&quot; aria-label=&quot;Anchor link for: 1-2-supervised-learning-with-neural-networks&quot;&gt;1.2 Supervised Learning with Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;There are different types of neural networks besides the standard architecture seen before. This classical standard architecture is still useful for tabular data, such as user information to predict online shopping. But there are other kind of problems that require other kind of neural networks.&lt;&#x2F;p&gt;
&lt;p&gt;For example Convolution Neural Network (CNN) used often for image application. Recurrent Neural Networks (RNN) are used for one-dimensional sequence data such as translating English to Chinese or a temporal component such as text transcript. Hybrid Neural Networks architecture can be used for autonomous driving model.&lt;&#x2F;p&gt;
&lt;p&gt;For now the only difference that we have to know is between structured and unstructured data:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Structured data&lt;&#x2F;strong&gt;: it has a defined label, such as price and size. Also called tabular data.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Unstructured data&lt;&#x2F;strong&gt;: it does not have a define meaning by itself. Like a pixel, raw audio or text.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;2-neural-network-basics&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-neural-network-basics&quot; aria-label=&quot;Anchor link for: 2-neural-network-basics&quot;&gt;2. Neural Network Basics&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;2-1-logistic-regression-as-neural-network&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-1-logistic-regression-as-neural-network&quot; aria-label=&quot;Anchor link for: 2-1-logistic-regression-as-neural-network&quot;&gt;2.1 Logistic Regression as Neural Network&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In a &lt;strong&gt;binary classification problem&lt;&#x2F;strong&gt;, the result is a discrete value output: a 1 or a 0. For example, trying to explain a catastrophe survival rate: a person survived (1) or did not (0).&lt;&#x2F;p&gt;
&lt;p&gt;The feature matrix shape is made &quot;stacking&quot; the number of features($n_x$) in different columns, one for every observation ($m$): $X.shape = (n_x, m)$.&lt;&#x2F;p&gt;
&lt;p&gt;The output shape is a 1 by $m$ dimensional matrix: $y.shape = (1,m)$.&lt;&#x2F;p&gt;
&lt;p&gt;The prediction $\hat{y}$ (0 or 1) is determined by the probability of a real $y$ given factors $X$:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;4.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The probability is calculated by a &lt;code&gt;sigmoid&lt;&#x2F;code&gt; activation function that takes a linear regression $z$ as input:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;6.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Lastly, $Z$ is just a linear regresion.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;5.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Here I took the notation of &quot;Betas&quot; because this is what I am used to in Economics, but from here on I’ll refer at them as “Weights” and “bias” instead. It is the same with different notation.&lt;&#x2F;p&gt;
&lt;p&gt;Please notice that a linear regression that goes into a sigmoid activation is a “Logistic regression” with different wording.&lt;&#x2F;p&gt;
&lt;p&gt;Think about a Neural Network as a collection of “neurons” or nodes that take inputs and apply a sigmoid function. The sigmoid function is simply a logistic transformation that wraps the features($n_x$) so the values of $y$ are forced to be 0 or 1.&lt;&#x2F;p&gt;
&lt;p&gt;NN = a lot of logistic regressions averaged.&lt;&#x2F;p&gt;
&lt;p&gt;Example:&lt;&#x2F;p&gt;
&lt;p&gt;We want to calculate the probability of tomorrow being sunny (1) or rainy (0). All the days in the training set have associated temperatures ($X$) to those days. Since sunny days tend to be warmer, the linear regression will give $\hat{y}$ closer to 1 when the temperatures are high, and close to zero when it rains. Sigmoid transforms the closer numbers to 1 to exactly 1 and the closer to 0 to exactly 0, and that’s the prediction $\hat{y}$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-2-evaluating-the-model-with-log-loss&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-2-evaluating-the-model-with-log-loss&quot; aria-label=&quot;Anchor link for: 2-2-evaluating-the-model-with-log-loss&quot;&gt;2.2 Evaluating the model with Log-Loss&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;✍️ NOTE: added info from https:&#x2F;&#x2F;mlu-explain.github.io&#x2F;logistic-regression . It assumes familiarity with linear algebra.&lt;&#x2F;p&gt;
&lt;p&gt;For now, lets continue with a simple logistic regression as a base NN.&lt;&#x2F;p&gt;
&lt;p&gt;Please notice that $z$ calculation (linear regression) only relies on $X$ that is a matrix given by the data itself and the weights $W$.&lt;&#x2F;p&gt;
&lt;p&gt;How do we set the $W$ then and how does it now that $W$ are well defined?&lt;&#x2F;p&gt;
&lt;p&gt;To explain it in an intuitive way, we can use a “comparison mechanism” to set $W$. The mechanism or function compares the $\hat{y}$ and $y$ difference. If they are the same the $W$ are good for the data, and if not it tries again with new $W$.&lt;&#x2F;p&gt;
&lt;p&gt;This comparison mechanism or equation is called “Loss function”. For logistic regression, a suitable loss function is Log-Loss, also called binary cross-entropy:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;7.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Notice that the equation is only a function of $y$ and $\hat{y}$ - it “compares” them.&lt;&#x2F;p&gt;
&lt;p&gt;It is made that way so if the real label and the predicted label are the same, it equals zero. The worse the model the higher the result is (try replacing $y=0$ and $&#x2F;haty=1$ for all $i$).&lt;&#x2F;p&gt;
&lt;p&gt;That’s why it tells how well the logistics regression is doing. The lower the function value, the better the logistic regression is predicting labels.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-3-gradient-descent-and-logistic-regression&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-3-gradient-descent-and-logistic-regression&quot; aria-label=&quot;Anchor link for: 2-3-gradient-descent-and-logistic-regression&quot;&gt;2.3 Gradient Descent and Logistic Regression&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;From Log-Loss, the logistic regression “knows” which coefficients $W$ are better for the inputs $X$, but... How it sets the values of $W$ in the first place, and how exactly “update” them ?&lt;&#x2F;p&gt;
&lt;p&gt;A common way to estimate coefficients is to use gradient descent. The gradient calculates where the Log-Loss function is increasing, so going in the opposite direction leads us to the minimum of our function.&lt;&#x2F;p&gt;
&lt;p&gt;In simple terms:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Let’s start with random weights $W$ for every observation.&lt;&#x2F;li&gt;
&lt;li&gt;The weights determine $z$, that gives you a $\hat{y}$. With $\hat{y}$ and $y$, the Log-Loss will drop real number.&lt;&#x2F;li&gt;
&lt;li&gt;Let’s change the $W$ a tiny amount (think +0.00001) and calculate again the Log-Loss. The number went up or down?&lt;&#x2F;li&gt;
&lt;li&gt;If the number went down keep increasing the $W$ value, otherwise change direction (decrease tiny bit and see the Log-Loss).&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This only works because the shape of the Log-Loss equation is convex, so the slope of the curve guides to the minimum of the equation.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;8.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Given it’s shape, the slope of Log-Loss can be used to determine the best set of weights.&lt;&#x2F;p&gt;
&lt;p&gt;Programmatically, the scope is calculated by derivation of the function given a set of $W$. And the gradient descent algorithm is designed to iterate over tiny increments for every observation.&lt;&#x2F;p&gt;
&lt;p&gt;At each iteration, the $W$ value is updated by a tiny amount (the gradient), scaled by the step size $\alpha$ (learning rate). The bigger the learning rate, the higher the steps.&lt;&#x2F;p&gt;
&lt;p&gt;Aside from gradient descent, Maximum Likelihood Estimation (MLE) can be used to estimate the $W$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-4-vectorization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-4-vectorization&quot; aria-label=&quot;Anchor link for: 2-4-vectorization&quot;&gt;2.4 Vectorization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;As a side note, gradient descent calculations demonstrate why the rise of Deep Learning could not have happened in the past. It needs a huge computational power by hardware that we are lucky to have today. Notice that the logistic function output, the change in the coefficients, and the Log-Loss function have to be calculated again and again.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;9.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We also have had progress in the way we do the computation iteration.&lt;&#x2F;p&gt;
&lt;p&gt;In the section above I mentioned: &lt;em&gt;“And the algorithm iterates over tiny increments for every observation.”&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The iteration doesn’t use loops since it would be too computationally costly - it uses vectorization.&lt;&#x2F;p&gt;
&lt;p&gt;Gradient descent needs to compute over and over $z$ to get the prediction for all the values of $i$.&lt;&#x2F;p&gt;
&lt;p&gt;This can be done via loop or via vectorization. For loops takes +1000 times more time:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;numpy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;np
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;time
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Data
&lt;&#x2F;span&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;rand&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100000000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;W &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;rand&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100000000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# For loop
&lt;&#x2F;span&gt;&lt;span&gt;tic &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;time.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;time&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in range&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100000000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;  z &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+= &lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;W&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;z&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;toc &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;time.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;time&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Loop took: &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;((&lt;&#x2F;span&gt;&lt;span&gt;toc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;tic&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) + &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ms&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;--&amp;gt; &lt;&#x2F;span&gt;&lt;span&gt;Loop took: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18357.781887054443&lt;&#x2F;span&gt;&lt;span&gt;ms
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Vectorization
&lt;&#x2F;span&gt;&lt;span&gt;tic &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;time.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;time&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;z &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X,W&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;z&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;toc &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;time.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;time&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Vectorization took: &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;((&lt;&#x2F;span&gt;&lt;span&gt;toc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;tic&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) + &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ms&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;--&amp;gt; &lt;&#x2F;span&gt;&lt;span&gt;Vectorization took: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18.301010131835938&lt;&#x2F;span&gt;&lt;span&gt;ms
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Section 3 it will show that by stacking $X$ and $W$ vectorisation the neural network is more efficient computationally.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-5-broadcasting&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-5-broadcasting&quot; aria-label=&quot;Anchor link for: 2-5-broadcasting&quot;&gt;2.5 Broadcasting&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Another convenient process that happens in the background is “broadcasting”. It’s a matrix transformation that Python automatically applies to do faster matrix multiplications.&lt;&#x2F;p&gt;
&lt;p&gt;Python transform the constant (or 1x1 matrix) “$b$” and expand to a “$1xm$” matrix when we use it in matrix operations.&lt;&#x2F;p&gt;
&lt;p&gt;As a &quot;General Principle&quot;: When you sum, subtract, divide or multiply $(m,n)$ matrix with a $(1,n)$ matrix the $(1,n)$ matrix will be expanded to a $(m,n)$ matrix by copying the row $m$ times to match the shape.&lt;&#x2F;p&gt;
&lt;p&gt;This allows to write quite a flexible code, but it also allows to start creating product matrices that create bugs difficult to track.&lt;&#x2F;p&gt;
&lt;p&gt;TIP: Specify the matrix shape and don&#x27;t use rank 1 matrices. Use $np.random.randn(5,1)$ instead of $np.random.randn(5)$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-6-neural-network-from-the-scratch-in-python&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-6-neural-network-from-the-scratch-in-python&quot; aria-label=&quot;Anchor link for: 2-6-neural-network-from-the-scratch-in-python&quot;&gt;2.6 Neural Network from the scratch in Python&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In the exercise section it propose a simple logistic regression model identifying cats from non-cats images. They provide already the images.&lt;&#x2F;p&gt;
&lt;p&gt;Instead of the pre-made data given, I took images from Cats and Dogs from Kaggle dataset: https:&#x2F;&#x2F;www.kaggle.com&#x2F;datasets&#x2F;erkamk&#x2F;cat-and-dog-images-dataset?resource=download&lt;&#x2F;p&gt;
&lt;p&gt;Visually, the model looks like:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;10.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;1. Create arrays of cats and dogs from Kaggle dataset&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;extract_images&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;foldername&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Training data from Kaggle Cats and Dogs dataset
&lt;&#x2F;span&gt;&lt;span&gt;    all_images &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= []
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;filename &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;os.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;listdir&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;train&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;foldername&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;        f &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;os.path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;join&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;train&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;foldername&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, filename&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;os.path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;isfile&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;            image &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;Image.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;open&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;            image_array &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;image&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;            all_images.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;append&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;image_array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Arry format and normalize pixels
&lt;&#x2F;span&gt;&lt;span&gt;    all_images &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;all_images&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) &#x2F; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;255
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;all_images
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;cats &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;extract_images&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Cat&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dogs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;extract_images&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Dog&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;labels &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;read_pickle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;train&#x2F;y.pickle&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;2. Create a train and test set&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The data is not divided into test and train, so you have to create the sets.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;concatenate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;((&lt;&#x2F;span&gt;&lt;span&gt;cats,dogs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;labels&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;reshape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Shuffle all the indexes
&lt;&#x2F;span&gt;&lt;span&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;seed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4208&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;indexes &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;choice&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1000&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;replace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# First 800 in the training set, 200 resting in the test set
&lt;&#x2F;span&gt;&lt;span&gt;train_index &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;indexes&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;800&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;test_index &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;indexes&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;800&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;X_train &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;train_index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;y_train &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;train_index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.T
&lt;&#x2F;span&gt;&lt;span&gt;X_test &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;test_index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;y_test &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;test_index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.T
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Number of training examples: m_train = &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_train.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Number of testing examples: m_test = &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_test.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;X_train shape: &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_train.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;y_train shape: &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;y_train.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;X_test shape: &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_test.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;y_test shape: &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;y_test.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Flatten the arrays to (num_px∗num_px∗3, 1)
&lt;&#x2F;span&gt;&lt;span&gt;X_train &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_train.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;reshape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_train.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.T
&lt;&#x2F;span&gt;&lt;span&gt;X_test  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_test.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;reshape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_test.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.T
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;X_train new shape: &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_train.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;X_test new shape: &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_test.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The print statements should say:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Number&lt;&#x2F;span&gt;&lt;span&gt; of training examples: m_train = 800
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Number&lt;&#x2F;span&gt;&lt;span&gt; of testing examples: m_test = 200
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;X_train&lt;&#x2F;span&gt;&lt;span&gt; shape: (800, 64, 64, 3)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;y_train&lt;&#x2F;span&gt;&lt;span&gt; shape: (1, 800)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;X_test&lt;&#x2F;span&gt;&lt;span&gt; shape: (200, 64, 64, 3)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;y_test&lt;&#x2F;span&gt;&lt;span&gt; shape: (1, 200)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;X_train&lt;&#x2F;span&gt;&lt;span&gt; new shape: (12288, 800)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;X_test&lt;&#x2F;span&gt;&lt;span&gt; new shape: (12288, 200)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;3. Components of the Neural Network&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Define Sigmoid function&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sigmoid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;z&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F; (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;exp&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(-&lt;&#x2F;span&gt;&lt;span&gt;z&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Initialize weights&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. In further lectures they initiate randomly (&lt;code&gt;np.random.rand()&lt;&#x2F;code&gt;).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;initialize_with_zeros&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dim&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;		w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;expand_dims&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;zeros&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dim&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;float&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;w, b
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Forward Propagation and calculate derivatives&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;propagate&lt;&#x2F;code&gt; function.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The inputs $w$ and $b$ are initially zeros, $X$ and $Y$ is the data and its labels.&lt;&#x2F;li&gt;
&lt;li&gt;It creates a vector with all the weights and bias: $A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$&lt;&#x2F;li&gt;
&lt;li&gt;It defines the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)}))$&lt;&#x2F;li&gt;
&lt;li&gt;It derives $J$ that later will be used to update $w$ and $b$.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;propagate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;&amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Arguments:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    b -- bias, a scalar
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    X -- data of size (num_px * num_px * 3, number of examples)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Y -- true &amp;quot;label&amp;quot; vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Return:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    grads -- dictionary containing the gradients of the weights and bias
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;            (dw -- gradient of the loss with respect to w, thus same shape as w)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;            (db -- gradient of the loss with respect to b, thus same shape as b)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    cost -- negative log-likelihood cost for logistic regression
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;#
&lt;&#x2F;span&gt;&lt;span&gt;    m &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    A &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sigmoid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w.T, X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) + &lt;&#x2F;span&gt;&lt;span&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    cost &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;m&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)* (&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;Y,np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;log&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;A&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)+ &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;log&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;A&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Find grad (derivation of the function)
&lt;&#x2F;span&gt;&lt;span&gt;    dw &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;m &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;A&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.T&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    db &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;m &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sum&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;A&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Format
&lt;&#x2F;span&gt;&lt;span&gt;    cost &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;squeeze&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    grads &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= {&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;dw&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: dw,
&lt;&#x2F;span&gt;&lt;span&gt;             &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;db&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: db&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;grads, cost
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Minimize Log-Loss function (J)&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;optimize&lt;&#x2F;code&gt; function does gradient descent.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Runs &lt;code&gt;propagate&lt;&#x2F;code&gt; and takes the derivatives of $w$ and $b$ ($dw$, $db$) to update new values for $w$ and $b$.&lt;&#x2F;li&gt;
&lt;li&gt;It runs as many times as we want given by the parameter $num_itrations$.&lt;&#x2F;li&gt;
&lt;li&gt;Every new updated value of $w$ and $b$ is closer and closer to the optimal value that minimize the cost function (remember that the derivate is the slope os the cost function)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;optimize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;num_iterations&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;learning_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.009&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;print_cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;&amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Arguments:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    b -- bias, a scalar
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    X -- data of shape (num_px * num_px * 3, number of examples)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Y -- true &amp;quot;label&amp;quot; vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    num_iterations -- number of iterations of the optimization loop
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    learning_rate -- learning rate of the gradient descent update rule
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Returns:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    params -- dictionary containing the weights w and bias b
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;copy.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;deepcopy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;copy.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;deepcopy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    costs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= []
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in range&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;num_iterations&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;        grads, cost &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;propagate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w, b, X, Y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Retrieve derivatives from grads
&lt;&#x2F;span&gt;&lt;span&gt;        dw &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;grads&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;dw&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;        db &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;grads&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;db&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# update weights and bias
&lt;&#x2F;span&gt;&lt;span&gt;        w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span&gt;learning_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;dw
&lt;&#x2F;span&gt;&lt;span&gt;        b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span&gt;learning_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span&gt;db
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Record the costs
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;% &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;            costs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;append&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Print the cost every 100 training iterations
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;print_cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Cost after iteration &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;%i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;%f&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;%(&lt;&#x2F;span&gt;&lt;span&gt;i, cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    params &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= {&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: w,
&lt;&#x2F;span&gt;&lt;span&gt;              &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    grads &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= {&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;dw&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: dw,
&lt;&#x2F;span&gt;&lt;span&gt;             &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;db&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: db&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;params, grads, costs
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Calculate predictions&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The previous function will output the learned $w$ and $b$. We are able to use $w$ and $b$ to predict the labels for a dataset $X$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;predict()&lt;&#x2F;code&gt; function:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Takes the optimized $w$ and $b$ from &lt;code&gt;optimize&lt;&#x2F;code&gt; function.&lt;&#x2F;li&gt;
&lt;li&gt;Calculates prediction: $\hat{Y} = A = \sigma(w^T X + b)$&lt;&#x2F;li&gt;
&lt;li&gt;Converts the entries of a into 0 (if activation &amp;lt;= 0.5) or 1 (if activation &amp;gt; 0.5) and stores the predictions in a vector &lt;code&gt;Y_prediction&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;&amp;#39;&amp;#39;&amp;#39;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Arguments:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    b -- bias, a scalar
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    X -- data of size (num_px * num_px * 3, number of examples)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Returns:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Y_prediction -- a numpy array (vector) containing all predictions (0&#x2F;1) for the examples in X
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    &amp;#39;&amp;#39;&amp;#39;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    m &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    Y_prediction &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;zeros&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;((&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, m&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;w.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;reshape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    A &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sigmoid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w.T,X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) + &lt;&#x2F;span&gt;&lt;span&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in range&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;A.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;A&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;,i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;            Y_prediction&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;,i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;else&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;            Y_prediction&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;,i&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;Y_prediction
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;4. Logistic Regression Model combining all functions&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Builds the logistic regression model by calling the functions.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_TRAIN&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y_TRAIN&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_TEST&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y_TEST&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;num_iterations&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2000&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;learning_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;print_cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;&amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;		Arguments:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    print_cost -- Set to True to print the cost every 100 iterations
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    Returns:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    d -- dictionary containing information about the model.
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;    &amp;quot;&amp;quot;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    dim &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_TRAIN&lt;&#x2F;span&gt;&lt;span&gt;.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    w, b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;initialize_with_zeros&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dim&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    params, grads, costs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;optimize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w, b, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_TRAIN&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y_TRAIN&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                                    num_iterations,
&lt;&#x2F;span&gt;&lt;span&gt;                                    learning_rate,
&lt;&#x2F;span&gt;&lt;span&gt;                                    print_cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;params&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;params&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    y_prediction_test &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w, b, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_TEST&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    y_prediction_train &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;w, b, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_TRAIN&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Print train&#x2F;test Errors
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;print_cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;:
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Train accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt; %&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;mean&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;abs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;y_prediction_train &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y_TRAIN&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)) * &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Test accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt; %&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;mean&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;abs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;y_prediction_test &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;Y_TEST&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)) * &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    d &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= {&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;costs&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: costs,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Y_prediction_test&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: y_prediction_test,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Y_prediction_train&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; : y_prediction_train,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;w&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; : w,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; : b,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;learning_rate&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot; : learning_rate,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;num_iterations&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;: num_iterations&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;d
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;logistic_regression_model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_train,
&lt;&#x2F;span&gt;&lt;span&gt;                                  y_train,
&lt;&#x2F;span&gt;&lt;span&gt;                                  X_test,
&lt;&#x2F;span&gt;&lt;span&gt;                                  y_test,
&lt;&#x2F;span&gt;&lt;span&gt;                                  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;num_iterations&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10000&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                                  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;learning_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.001&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                                  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;print_cost&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The model should print (if you kept the seed):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;[..............]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Cost&lt;&#x2F;span&gt;&lt;span&gt; after iteration 9600: 0.401098
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Cost&lt;&#x2F;span&gt;&lt;span&gt; after iteration 9700: 0.399880
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Cost&lt;&#x2F;span&gt;&lt;span&gt; after iteration 9800: 0.398671
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Cost&lt;&#x2F;span&gt;&lt;span&gt; after iteration 9900: 0.397471
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;train&lt;&#x2F;span&gt;&lt;span&gt; accuracy: 86.125 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;%
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;test&lt;&#x2F;span&gt;&lt;span&gt; accuracy: 59.5 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;%
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;3-shallow-neural-network&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-shallow-neural-network&quot; aria-label=&quot;Anchor link for: 3-shallow-neural-network&quot;&gt;3. Shallow Neural Network&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;3-1-neural-networks-overview-and-vectorized-implementation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-1-neural-networks-overview-and-vectorized-implementation&quot; aria-label=&quot;Anchor link for: 3-1-neural-networks-overview-and-vectorized-implementation&quot;&gt;3.1 Neural Networks Overview and Vectorized Implementation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Until now, we saw the case of logistic regression as a single layer Neural Network.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;10.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;“Deep” from Deep Learning refers to the layers of a neural network - They are “deep” hidden layers that are stacked. The only layer that receives the $X$ values as inputs is the &lt;code&gt;Input layer&lt;&#x2F;code&gt;. After that, the output of each layer is passed to the next one.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;11.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Each node computes $a$, the output of the sigmoid activation function passed through the linear regression. This calculation can be vectorized into matrix operations:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;12.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;By vectorization, sigmoid transformation doesn’t have to loop each node - all $i$ weights and bias can stacked horizontally. The output $Z^1$ of the first layer goes through the activation function and $A^1$ passes to the second layer as input.&lt;&#x2F;p&gt;
&lt;p&gt;The 2-layer NN above only needs to calculate $&#x2F;sigmoid(Z^1)$ and $&#x2F;sigmoid(Z^2)$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-activation-functions&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-2-activation-functions&quot; aria-label=&quot;Anchor link for: 3-2-activation-functions&quot;&gt;3.2 Activation functions&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;When you build your Neural Network, one of the choices you get to make is what activation function to use in the hidden layers.&lt;&#x2F;p&gt;
&lt;p&gt;Tanh or ReLU are recommended as a default, but different ones for your application.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Tanh&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The sigmoid function goes within zero and one. An activation function that almost always works better than the sigmoid function is the tangent function or also called &lt;strong&gt;hyperbolic tangent function (Tanh)&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;14.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It goes between 1 and - 1. The tanh function is almost always strictly superior. The main difference is that using tanh instead of sigmoid outputs the data with mean zero (centering the data), which makes the learning for the next layer easier.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;one exception&lt;&#x2F;strong&gt; is for the output layer because if y is either 0 or 1, then it makes sense for y hat to be a number, the one to output that&#x27;s between 0 and 1 rather than between minus 1 and 1.&lt;&#x2F;p&gt;
&lt;p&gt;One of the downsides of both the sigmoid function and the tanh function is that if $z$ is either very large or very small, then the gradient or the derivative or the slope of this function becomes very small.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;ReLU&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Another choice that is very popular in machine learning is what&#x27;s called the &lt;strong&gt;rectify linear unit (ReLU)&lt;&#x2F;strong&gt;. So the value function looks like:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;15.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The derivative is 1 as $z$ is positive. And the derivative or slope is 0 when $z is negative. Is an increasingly popular default choice of activation function.&lt;&#x2F;p&gt;
&lt;p&gt;In practice, using the ReLU activation function, your neural network will often &lt;strong&gt;learn much faster&lt;&#x2F;strong&gt; than tanh or sigmoid activation function. The main reason is that there is less of these effects of the slope of the function going to 0, which slows down learning.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Leaky ReLU Function&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It is similar to ReLU, but instead of returning zero for negative inputs, it returns a small negative value. This helps to avoid the &quot;dying ReLU&quot; problem, where some neurons can become permanently inactive during training.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;16.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-3-gradient-descent&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-3-gradient-descent&quot; aria-label=&quot;Anchor link for: 3-3-gradient-descent&quot;&gt;3.3 Gradient Descent&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The left equations describe the derivation, the right ones the equivalent in python vectorization.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;13.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Once the parameters are set, they update based on a learning rate $$\alpha: \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-4-random-initialization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-4-random-initialization&quot; aria-label=&quot;Anchor link for: 3-4-random-initialization&quot;&gt;3.4 Random Initialization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;When you change your neural network, it&#x27;s important to initialize the weights randomly. Random initialization of the model is a common practice.&lt;&#x2F;p&gt;
&lt;p&gt;We can initialize the weights randomly and the bias at zero.&lt;&#x2F;p&gt;
&lt;p&gt;If we initialize weights and bias to zeros, all activations functions ($a_{i,j,...}$) will be equal:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;17.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;All the neurons units would start off computing the same function and therefore all the hidden neurons completely identical. Using more hidden layers would be useless, after the first layer all the sequent layers calculate the same with the same weight optimisation.&lt;&#x2F;p&gt;
&lt;p&gt;Instead, NNs starts with random initialization (bias still can be zero).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_initialization&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dim&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;		w &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;expand_dims&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;randn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dim&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;float&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;w, b
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;0.01&lt;&#x2F;code&gt; or a very small number is chosen to not end up in the extremes of the activation function at the beginning, making the calculation of the derivations in very small steps (not that much slope).&lt;&#x2F;p&gt;
&lt;p&gt;With a small number is more likely that $w$ starts in the range of the activation function that there is more slope (think in the middle of the tanh function), and the derivations are in bigger steps making a quicker convergence.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;4-deep-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-deep-neural-networks&quot; aria-label=&quot;Anchor link for: 4-deep-neural-networks&quot;&gt;4. Deep Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Deep Neural Networks have the same structure of “shallow models” with more layers.&lt;&#x2F;p&gt;
&lt;p&gt;The number of layers is another hyperparameter that we can modify to improve the accuracy of the model.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-1-forward-propagation-in-deep-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-1-forward-propagation-in-deep-networks&quot; aria-label=&quot;Anchor link for: 4-1-forward-propagation-in-deep-networks&quot;&gt;4.1 Forward Propagation in Deep Networks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Same as before. In &lt;strong&gt;Forward propagation&lt;&#x2F;strong&gt;, the weight vectors and bias vectors of the next layer depends on the vectors of the previous one:&lt;&#x2F;p&gt;
&lt;p&gt;$$Z^{[1]} =  W^{[1]} X + b^{[1]}\tag{1}$$
$$A^{[1]} = \tanh(Z^{[1]})\tag{2}$$
$$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\tag{3}$$
$$A^{[2]} = \tanh(Z^{[2]})\tag{4}$$
$$Z^{[3]} = W^{[2]} A^{[2]} + b^{[3]}\tag{5}$$
$$A^{[3]} = \tanh(Z^{[3]})\tag{6}$$
$$... \tag{7}$$
$$\hat{Y} = A^{[n]} = \sigma(Z^{[n]})\tag{8}$$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-getting-the-dimensions-of-a-neural-network&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-2-getting-the-dimensions-of-a-neural-network&quot; aria-label=&quot;Anchor link for: 4-2-getting-the-dimensions-of-a-neural-network&quot;&gt;4.2 Getting the dimensions of a Neural Network&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The shape of the matrix derivates are equal to the original matrixes.&lt;&#x2F;p&gt;
&lt;p&gt;For the vectorized implementation, instead of going 1 by 1 observation, it stack the observations columnise. Therefore the shapes are:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-deeplearning-neural-networks&#x2F;.&#x2F;images&#x2F;18.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Generalizing:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;$W^i$ will always have a shape $(n^l, n^{(l-1)})$&lt;&#x2F;li&gt;
&lt;li&gt;$Z^i$ will always have a shape $(n^l,1)$&lt;&#x2F;li&gt;
&lt;li&gt;$b^i$ will always have a shape $(n^l,1)$&lt;&#x2F;li&gt;
&lt;li&gt;$X^i$ will always have a shape $(n^{(l-1)},1)$&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Otherwise the matrix multiplication will be either wrong or they won&#x27;t run at all in code.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-3-parameters-vs-hyperparameters&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-3-parameters-vs-hyperparameters&quot; aria-label=&quot;Anchor link for: 4-3-parameters-vs-hyperparameters&quot;&gt;4.3 Parameters vs Hyperparameters&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Parameters: These are the parameters in the model that must be determined using the training data set. These are the fitted parameters $W$ and $b$.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Hyperparameters: These are adjustable parameters that must be tuned in order to obtain a model with optimal performance: learning rate, number of iterations, the choice of the activation function, number of hidden layers, number of units in each hidden layer...&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>The Art of Clean Code, by Christian Mayer</title>
        <published>2024-05-01T00:00:00+00:00</published>
        <updated>2024-05-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-clean-code/"/>
        <id>https://pipegalera.github.io/mostly_books/book-clean-code/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-clean-code/">&lt;p&gt;The book is a mix between how to write clean code (75%) and how to be a productive coder (25%). More than rules set in stone, it encourages applying minimalism to coding.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-clean-code&#x2F;.&#x2F;images&#x2F;cover.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It reminded me of Cal Newport&#x27;s work focused on coding and building tech products. Gladly, it is not as technical (or controversial) as Clean Code by Robert Cecil Martin or enter the category of self-help books with random productivity tips.&lt;&#x2F;p&gt;
&lt;p&gt;This is definitely one of the best books I&#x27;ve read on coding.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-reduce-complexity-using-software-development-life-cycle-sdlc&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-reduce-complexity-using-software-development-life-cycle-sdlc&quot; aria-label=&quot;Anchor link for: 1-reduce-complexity-using-software-development-life-cycle-sdlc&quot;&gt;1. Reduce complexity using Software Development Life Cycle (SDLC)&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Complexity describes a property of a whole system or entity with parts that makes the system difficult to explain, understand, or analyze.&lt;&#x2F;p&gt;
&lt;p&gt;This causes struggle, overwhelm, and confusion.&lt;&#x2F;p&gt;
&lt;p&gt;The answer to complexity is simplicity. Seek simplicity and focus in every stage of the coding cycle. Take a radically minimalistic position in every area you encounter .&lt;&#x2F;p&gt;
&lt;p&gt;Complexity in organizations appears when they accumulate too many processes, and complexity starts to clog the system. Innovation finds fewer vehicles for change because it’s unable to break through the complexity.&lt;&#x2F;p&gt;
&lt;p&gt;The goal of Software Development Life Cycle (SDLC) is to reduce this complexity by compartmentalize the abstraction of creating software into defined steps.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-clean-code&#x2F;.&#x2F;images&#x2F;sdlc.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Planning&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The purpose of this phase is to determine how the product will look. A successful planning phase leads to a strictly defined set of required features to deliver to the end user.&lt;&#x2F;p&gt;
&lt;p&gt;Factors:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The cost of building a feature.&lt;&#x2F;li&gt;
&lt;li&gt;The risk of not being able to successfully implement the feature.&lt;&#x2F;li&gt;
&lt;li&gt;The expected value for the end user.&lt;&#x2F;li&gt;
&lt;li&gt;Marketing and sales implications.&lt;&#x2F;li&gt;
&lt;li&gt;Maintainability.&lt;&#x2F;li&gt;
&lt;li&gt;Scalability.&lt;&#x2F;li&gt;
&lt;li&gt;Legal restrictions…&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This phase is crucial because it can save you from wasting massive amounts of energy later.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Defining&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Translating the results from the planning phase into properly specified software requirements.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Designing&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Draft the architecture of the system. Create a crystal-clear picture of how the final software product will look and how it’s built.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Building&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Where the transformation from the architectural draft to the software product happens.Your ideas transform into tangible results.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Testing&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;You must test the behavior of your software product for different users&#x27; inputs and usage patterns.&lt;&#x2F;p&gt;
&lt;p&gt;If all your unit tests successfully pass, you haven’t yet completed the testing phase. You must test the correct interaction of the units as they’re building a greater whole. You must design real-world tests.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-getting-the-right-minset-for-a-tech-career&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-getting-the-right-minset-for-a-tech-career&quot; aria-label=&quot;Anchor link for: 2-getting-the-right-minset-for-a-tech-career&quot;&gt;2. Getting the right minset for a tech-career&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;the-80-20-principle&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#the-80-20-principle&quot; aria-label=&quot;Anchor link for: the-80-20-principle&quot;&gt;The 80&#x2F;20 Principle&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The 80&#x2F;20 or Pareto Principle refers to the idea that a majority of effects come from a minority of causes.&lt;&#x2F;p&gt;
&lt;p&gt;Not all the code is created equal. &lt;strong&gt;A minority of code has a dominating impact on the user experience, while much of the code has little impact.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We define the term success metrics as measurements of the behaviors that lead to more success in your field. The 80&#x2F;20 approach allows you to identify the activities on which you must focus. &lt;strong&gt;Be lazy with all activities but one&lt;&#x2F;strong&gt;, your success metric exercise (e.g. write more words).&lt;&#x2F;p&gt;
&lt;p&gt;Pareto distribution disproportionately rewards focus on one specialized skill. In programming, the results tend to be much more heavily skewed toward the top than in most other fields. Instead of 80&#x2F;20, the distribution often looks more like 90&#x2F;10. For example, a great programmer can solve some problems that the average programmer simply cannot solve, and that pays off.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;One of the vital few activities to focus is to write more lines of code&lt;&#x2F;strong&gt;. The more lines you write, the better coder you’ll become. Here’s an 80&#x2F;20 activity you can follow every day: &lt;strong&gt;track the number of lines you code every&lt;&#x2F;strong&gt; day and optimize it. Make it a game to at least match your average every day.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;figure-out-your-success-metrics&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#figure-out-your-success-metrics&quot; aria-label=&quot;Anchor link for: figure-out-your-success-metrics&quot;&gt;Figure out your success metrics&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Define your industry first. Identify what the most successful professionals in your industry are doing exceptionally well&lt;&#x2F;strong&gt; and what tasks you can do every day to push you closer toward the top 20 percent. If you’re a coder, your success metric may be the number of lines of code written.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;smile&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#smile&quot; aria-label=&quot;Anchor link for: smile&quot;&gt;Smile.&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Smiling is a highly leveraged activity with massive impact and little cost.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;don-t-do-things-that-reduce-value&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#don-t-do-things-that-reduce-value&quot; aria-label=&quot;Anchor link for: don-t-do-things-that-reduce-value&quot;&gt;Don’t do things that reduce value.&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Eating badly, not exercising, playing excessively video games.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;figure-out-your-big-goals-in-life&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#figure-out-your-big-goals-in-life&quot; aria-label=&quot;Anchor link for: figure-out-your-big-goals-in-life&quot;&gt;Figure out your big goals in life&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;White them down.&lt;&#x2F;p&gt;
&lt;div class=&quot;note-container&quot;&gt;
    
            &lt;div class=&quot;note-header&quot;&gt;
                
                    &lt;div class=&quot;note-icon&quot;&gt;&lt;p&gt;Sidenote&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
                
            &lt;&#x2F;div&gt;
            &lt;div class=&quot;note-content&quot;&gt;&lt;p&gt;This is not in the book but its the same idea&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
        
    &lt;&#x2F;div&gt;
&lt;p&gt;Multiple sources (e.g. Andrew Huberman) points out that for a change in habit to be persistent it has to be internalized and come from within. Humans intrinsically operate in a cost-efficient way, meaning that &lt;strong&gt;if the goal is not tangible you will not incur in the cost and pain that goes into coding.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Coding is basically solving problems with code. ChatGPT can help you with the code part, but &lt;strong&gt;it takes a lot of effort to identify and engineer solutions.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;You have to visualize yourself once you already have the goal achieved (e.g. the job, business, skill.) - what it entitled, what lifestyle change or benefits it brought to your life, etc. Otherwise, your brain will push it away as it inputs calories&#x2F;resources to learn and produce no output.&lt;&#x2F;p&gt;
&lt;p&gt;That’s why kids want to play Minecraft for 3 hours and don’t want to study Math for 3 hours. The reward of the former is instant and of the latter takes +20 years to start collect the reward. Adults are not that different, we also have brains that optimize resources.&lt;&#x2F;p&gt;
&lt;p&gt;The mental goal has to happen before the physical one to anticipate the reward. &lt;strong&gt;Without a clear goal it is difficult to move forward in anything tech-related&lt;&#x2F;strong&gt;, as there are thousands of different frameworks and tools that you can possibly learn that pull you in different directions.&lt;&#x2F;p&gt;
&lt;p&gt;As an example, full stack development is completely different from machine learning engeneering and uses very different tools and libraries. Both take you probably thousands of hours to learn non-superficial specific knowledge, &lt;strong&gt;you cannot focus on multiple objectives and collect Pareto rewards from multiple specific skills&lt;&#x2F;strong&gt;. Even if you are very skilful, people that specialize only in one subfield will learn and apply the knowledge at a pace that you cannot follow, simply because we all humans have the same amount of hours in a day and there are also other very skilful people besides you.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Choose a path and clear goals instead of digging into the new trendy shiny tech of the moment and being pulled into new directions every day.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Look for ways to achieve the same things with fewer resources.&lt;&#x2F;li&gt;
&lt;li&gt;Reflect on your own successes.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;What did you do that led to great results? How can you do more of those things?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Reflect on your failures.&lt;&#x2F;li&gt;
&lt;li&gt;Read more books in your industry.&lt;&#x2F;li&gt;
&lt;li&gt;Spend much of your time improving and tweaking existing products.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If you create new products all the time without improving and optimizing the old ones, you’ll always have subaverage products.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-build-a-minimum-viable-product&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-build-a-minimum-viable-product&quot; aria-label=&quot;Anchor link for: 3-build-a-minimum-viable-product&quot;&gt;3. Build a minimum viable product&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;An MVP is a version of your product stripped of all except the most necessary features.&lt;&#x2F;p&gt;
&lt;p&gt;The idea behind building an MVP is to combat problems that arise when you program in stealth mode. &lt;strong&gt;Stealth mode is when you work on a project to completion without seeking any feedback from potential users.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-clean-code&#x2F;.&#x2F;images&#x2F;mvp_stealth.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Developing MVP products fights:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;The loss of motivation&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The longer you work on your project, the bigger your doubts grow. Loss of motivation can kill your project entirely. On the other hand, if you release an early version of the tool, encouraging words from an early adopter could keep you motivated enough to persevere.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Focus&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Creating an environment of more immediate feedback helps refocus your attention to the things that add value.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Faulty planning&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;By stripping all unnecessary features your planning mistakes will be fewer, and your progress will be more predictable. Fewer things will go wrong.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Lack of response&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;The most likely outcome of any software project is silence.&lt;&#x2F;strong&gt; If you don’t get any feedback from the real world during development, you start to drift away from reality, working on features nobody will use.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Wrong Assumptions&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You start a project with a bunch of assumptions, such as who the users will be. These assumptions are often wrong, and without external testing you blindly creating products your actual audience does not want.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Unnecessary Complexity&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You can be introducing features no only do not add value but harm the project adding complexity. If you introduce a set of features bundled without feedback, you don’t know whether the market would’ve accepted, or even preferred, any subset of features instead of the pack&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-clean-code&#x2F;.&#x2F;images&#x2F;stealth.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;how-to-build-mvps&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-build-mvps&quot; aria-label=&quot;Anchor link for: how-to-build-mvps&quot;&gt;How to build MVPs&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Formulate an explicit hypothesis&lt;&#x2F;strong&gt; and create a product that validates only this hypothesis. That it’s the core feature.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Decide the simplest way to validate this hypothesis and gain some insight quickly&lt;&#x2F;strong&gt;. For example, build a user interface without any sophisticated backend functionality, set up a website with an input field for the functionality and drive some traffic to it by sharing your idea in social media or by spending a small amount on ads.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Does it solve a need in the real world?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Does it have any traction (e.g. get organic users without promo or ads days after deployed) ?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;After some traction or signs of market fit, build a second MVP that adds to the next most important feature.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;If the first MVP did not generate any traction, you failed quickly. This is great because you did it without spending hours and hours improving the front and back-end for an idea that has no demand.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Add new features one at a time and test them.&lt;&#x2F;strong&gt; Only if a feature can prove that it improves key user metrics does it remain the product.&lt;&#x2F;p&gt;
&lt;p&gt;The term to describe this strategy of searching for the right product via a series of MVPs is called &lt;strong&gt;rapid prototyping&lt;&#x2F;strong&gt;. By this strategy you release early and often in order to find product-market fit.&lt;&#x2F;p&gt;
&lt;p&gt;The final step of the MVP software creation process is &lt;strong&gt;split testing&lt;&#x2F;strong&gt;. Rather than releasing the iterations with new features to your entire user base, you launch the new product to a fraction of users and observe the implicit and explicit response. This is also called data-driven software development.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;how-to-know-if-an-mvp-is-ready-to-launch&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-know-if-an-mvp-is-ready-to-launch&quot; aria-label=&quot;Anchor link for: how-to-know-if-an-mvp-is-ready-to-launch&quot;&gt;How to know if an MVP is ready to launch&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;An MVP comes from rigorous focus on one core functionality rather than from lazy product creation. &lt;strong&gt;An MVP is not a half-ass product&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Functionality&lt;&#x2F;strong&gt;. The product provides a clearly formulated function to the user, and it does it well.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Design&lt;&#x2F;strong&gt;. The product is well designed and focused, and its design supports the value that your product offers to your target niche.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Reliability&lt;&#x2F;strong&gt;. The product must be robust. Otherwise, your learnings from the MVP will be corrupted by the negative user feedback based on its unreliability.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Usability&lt;&#x2F;strong&gt;. The MVP must be easy to use, attention is scarce.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;4-write-clean-and-simple-code&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-write-clean-and-simple-code&quot; aria-label=&quot;Anchor link for: 4-write-clean-and-simple-code&quot;&gt;4. Write Clean and Simple Code&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;div class=&quot;note-container&quot;&gt;
    
            &lt;div class=&quot;note-header&quot;&gt;
                
                    &lt;div class=&quot;note-icon&quot;&gt;&lt;p&gt;Sidenote&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
                
            &lt;&#x2F;div&gt;
            &lt;div class=&quot;note-content&quot;&gt;&lt;p&gt;This chapter have 17 principles in the book, here are condensed&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
        
    &lt;&#x2F;div&gt;
&lt;p&gt;&lt;strong&gt;Coders spend the vast majority of their time reading old code in order to write new code.&lt;&#x2F;strong&gt; If the old code is easy to real, this will speed the process considerable.&lt;&#x2F;p&gt;
&lt;p&gt;Dirty code is less time-consuming in the short term and for small code projects - if there were no benefits to writing dirty code, nobody would do it.&lt;&#x2F;p&gt;
&lt;p&gt;However, the more you work on a team and work on serious software the more positive impact has writing clean code.&lt;&#x2F;p&gt;
&lt;p&gt;Code is still written mainly by humans, and likely must pass many levels of human judgement before it is deployed. &lt;strong&gt;Assume that others will read your code.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-clean-code&#x2F;.&#x2F;images&#x2F;lines_code.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-think-about-the-big-picture&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-think-about-the-big-picture&quot; aria-label=&quot;Anchor link for: 1-think-about-the-big-picture&quot;&gt;1. Think about the big picture&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Do you need all the separate files and modules?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you divide large and complicated files into simpler ones?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can your generalize code and turn it into a library?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you use existing libraries to get rid of many lines of code?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you use catching to avoid recomputing the same results over and over again?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you use more straightforward and suitable algorithms?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you remove premature optimizations?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you use another programming language that would be more suitable for the problem?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-use-the-standard-format-and-descriptive-names&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-use-the-standard-format-and-descriptive-names&quot; aria-label=&quot;Anchor link for: 2-use-the-standard-format-and-descriptive-names&quot;&gt;2. Use the standard format and descriptive names&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Use meaningful variable names, indentation, whitespace, comments, and line lengths.&lt;&#x2F;p&gt;
&lt;p&gt;Overusing indentation can decrease the readability of your code, stick to the standard for the language.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;You don’t want your untradictional naming conventions to distract those reading your code.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Choose descriptive, unambiguous, and &lt;strong&gt;pronounceable&lt;&#x2F;strong&gt; names.&lt;&#x2F;p&gt;
&lt;p&gt;For example, the variable name &lt;code&gt;mth_rate&lt;&#x2F;code&gt; (monthly rate) may be descriptive and unambiguous, but it’s not pronounceable.&lt;&#x2F;p&gt;
&lt;p&gt;Use named constants, not &lt;strong&gt;magic numbers&lt;&#x2F;strong&gt;. For example instead of using 0.8, describe &lt;code&gt;CONVERSION_RATE = 0.8&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Adhere to Standards and be consistent, for example use Black formatter for Python.&lt;&#x2F;p&gt;
&lt;p&gt;A system should behave in the way most users expect it to behave. Don’t be unconventional in the way you code or display information to the user without a very good reason. Being “artistic&#x2F;creative” will decrease maintainability of your code or conversion rates of your website.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-use-the-right-amount-of-comment&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-use-the-right-amount-of-comment&quot; aria-label=&quot;Anchor link for: 3-use-the-right-amount-of-comment&quot;&gt;3. Use the right amount of comment&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Comment your code if you think that it can be useful for the reader. In some cases, comments actually reduce clarity and confuse the readers. If the class of function is clear, comments are redundant.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Don’t use inline comments.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Don’t add obvious comments.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Don’t comment out old code, remove it.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Use documentation functionality.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;4-single-responsibility-principle&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-single-responsibility-principle&quot; aria-label=&quot;Anchor link for: 4-single-responsibility-principle&quot;&gt;4. Single Responsibility Principle&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The single responsibility principle means that every function should have one main task. Only one responsibility.&lt;&#x2F;p&gt;
&lt;p&gt;Many beginner coder write large monolithic code functions, or so-calle &lt;em&gt;God object&lt;&#x2F;em&gt;, that do everything in a centralized manner. This objects are messy, cluttered, and difficult to debug.&lt;&#x2F;p&gt;
&lt;p&gt;The opposite is also bad: do not write 2 functions that do have the same task. For example, don’t write &lt;code&gt;conversion_rate_euro&lt;&#x2F;code&gt; and &lt;code&gt;conversion_rate_dollar&lt;&#x2F;code&gt;, write &lt;code&gt;conversion_rate&lt;&#x2F;code&gt; that can take any currency as input. &lt;strong&gt;Don’t repeat yourself (DRY)&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-don-t-code-if-you-can-avoid-it&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-don-t-code-if-you-can-avoid-it&quot; aria-label=&quot;Anchor link for: 5-don-t-code-if-you-can-avoid-it&quot;&gt;5. Don’t code if you can avoid it&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Write code only if you’re 100 percent sure it’s necessary.&lt;&#x2F;strong&gt; You should really need a feature before you even consider implementing it.&lt;&#x2F;p&gt;
&lt;p&gt;Use naive algorithms and straightforward methods to establish a benchmark, then analyze which new feature or performance optimization would yield superior results for the overall application.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-boy-scout-rule-and-refactoring&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-boy-scout-rule-and-refactoring&quot; aria-label=&quot;Anchor link for: 6-boy-scout-rule-and-refactoring&quot;&gt;6. Boy Scout Rule and Refactoring&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Boy Scout Rule: &lt;strong&gt;Leave the campground cleaner than you found it.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Spending time to clean up your code to reduce complexity is almost always efficient. The process of improving your code is called refactoring.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Refactor your code before releasing any new feature.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Refactoring can be done explaining your code to a colleague or by your own. If you’re an introverted code, you can explain your code to a rubber duck instead - a technique known as &lt;strong&gt;rubber duck debugging&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;5-premature-optimization-is-the-root-of-all-evil&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-premature-optimization-is-the-root-of-all-evil&quot; aria-label=&quot;Anchor link for: 5-premature-optimization-is-the-root-of-all-evil&quot;&gt;5. Premature optimization is the root of all evil&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Premature optimization its the act of spending valuable resources (time, effort, loans of code) on unnecessary code optimization.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;types-of-premature-optimization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#types-of-premature-optimization&quot; aria-label=&quot;Anchor link for: types-of-premature-optimization&quot;&gt;Types of premature optimization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Code Functions.&lt;&#x2F;strong&gt; Be wary of spending time optimizing functions before you know how much those functions will be used.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Features.&lt;&#x2F;strong&gt; Avoid adding features that aren’t strictly necessary and wasting time optimizing those features.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Planning. Trying to find solutions to problems that haven’t yet occurred.&lt;&#x2F;strong&gt; You’ll never finish your project and will remain stuck in the ivory tower of theory.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Scalability.&lt;&#x2F;strong&gt; Do not design a distributed architecture that dynamically adds virtual machines handle peak load if you have no idea if you have even one person audience.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Test Design.&lt;&#x2F;strong&gt; In cases that the functions need unpredictable human-based input, real-world users are the only test that matters.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Optimizing Object-Oriented World Building.&lt;&#x2F;strong&gt; Don’t optimize your code to model a world with more details that the application actually needs.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;measure-first-improve-second&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#measure-first-improve-second&quot; aria-label=&quot;Anchor link for: measure-first-improve-second&quot;&gt;Measure first, improve second&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;You should always optimize only after you have begun to measure the performance of your non-optimized code, like memory footprint or speed. This is your benchmark.&lt;&#x2F;p&gt;
&lt;p&gt;Start with writing the most straightforward, naive code possible as the MVP. Document your measurements in a spreadsheet and measure the performance against this benchmark all the subsequent alternative solutions.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;pareto-is-king&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#pareto-is-king&quot; aria-label=&quot;Anchor link for: pareto-is-king&quot;&gt;Pareto is king&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Optimize the crucial feature that creates most of the bottlenecks or problems.&lt;&#x2F;p&gt;
&lt;p&gt;An important rule of performance tuning: performance optimization is fractal. As soon as you’ve removed the bottleneck, you’ll find another bottleneck lurking around. Bottlenecks will always be in any system, so you will always find new ways to optimize de codebase.
That’s why &lt;strong&gt;you need to measure your code performance and decide when it’s time to stop optimising.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;algorithmic-optimization-wins&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#algorithmic-optimization-wins&quot; aria-label=&quot;Anchor link for: algorithmic-optimization-wins&quot;&gt;Algorithmic optimization wins&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Ask yourself:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Can you find better algorithms that are already proven?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you tweak existing algorithms for your specific problem?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Can you improve the data structures?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;all-hail-the-cache&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#all-hail-the-cache&quot; aria-label=&quot;Anchor link for: all-hail-the-cache&quot;&gt;All Hail the Cache&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Remove unnecessary computation by storing the result in a cache. In Python, we can make a simple cache by creating a dictionary where you associate each function input with the function output.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Perform computations in advance (‘offline’) and store their results in the cache.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This is a great strategy for web applications where you can fill up a large cache one, or once a day, and then serve the result of your pre computations to the users. For them, your calculations seem blazingly fast. Mapping services heavily use this trick to speed up the shortest path computations.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Perform computations as they appear (‘online’) and store their results in the cache.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As there&#x27;s usually a memory limit on the number of cache entries you can save, you’ll need a sensible cache replacement policy. A common one is first in, first out (FIFO) where a new entry replaces the oldest cache entry.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;less-is-more&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#less-is-more&quot; aria-label=&quot;Anchor link for: less-is-more&quot;&gt;Less is more&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Consider removing bottlenecks by eliminating the feature that it&#x27;s causing them.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Simpler products require simpler solutions. It is important to think about the &lt;strong&gt;opportunity cost&lt;&#x2F;strong&gt;. Is the trouble worth the time, effort and resources necessary to fix it? Can you use that time to improve the system in more relevant ways?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;know-when-to-stop&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#know-when-to-stop&quot; aria-label=&quot;Anchor link for: know-when-to-stop&quot;&gt;Know when to stop&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;At some point, improving performance is just a waste of your time.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;6-flow&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-flow&quot; aria-label=&quot;Anchor link for: 6-flow&quot;&gt;6. Flow&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Flow is a state of pure concentration and focus - What some people might call “being in the zone”. &lt;strong&gt;Experiencing flow is being in a state of complete immersion in the task at hand, focused and concentrated.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;You forget about time as you are in the zone and completing the activity is its own reward.&lt;&#x2F;p&gt;
&lt;p&gt;In a state of flow, your body releases five feel-good neurochemical pleasure drugs such as endorphins, dopamine, and serotonin.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;how-flow-feels&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-flow-feels&quot; aria-label=&quot;Anchor link for: how-flow-feels&quot;&gt;How Flow feels&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;When in a state of flow, you sense:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Deep sense of concentration (&lt;strong&gt;focus lock&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;You feel a bias toward action, moving forward your current task (&lt;strong&gt;action driven&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;You become less aware of yourself, and you shut down your inner critics, doubts, and fears. You think less about yourself (&lt;strong&gt;ego killed&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Sense of control about the present situation, giving a calm confidence (&lt;strong&gt;task control&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;You lose the ability to experience time passing (&lt;strong&gt;time lapse&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The labor of the activity is all you want to do. There may no external reward, but being immersed in the activity is intrinsically rewarding in itself (&lt;strong&gt;internal chemical rewards&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;reaching-a-state-of-flow&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#reaching-a-state-of-flow&quot; aria-label=&quot;Anchor link for: reaching-a-state-of-flow&quot;&gt;Reaching a state of Flow&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;No matter what you do, tech or otherwise, to reach a state of Flow you need:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Clear Goals&lt;&#x2F;strong&gt; . You must have a clear goal toward which the smaller actions are oriented. Every action naturally leads to the next, which leads to the next, so there must be an end goal.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Feedback mechanism&lt;&#x2F;strong&gt;. Feedback is a precondition for flow. To implement more flow, seek more feedback. A feedback mechanism rewards desired behaviour and punishes undesired behaviour. By feedback we learn to take specific actions and avoid others. Receiving feedback is vital for this way of learning.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Balance Opportunity and Capacity&lt;&#x2F;strong&gt;. Constantly seek harder challenges without reaching anxiety levels and increase your skill level accordingly&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-clean-code&#x2F;.&#x2F;images&#x2F;flow.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;reaching-a-state-of-flow-learning-tech&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#reaching-a-state-of-flow-learning-tech&quot; aria-label=&quot;Anchor link for: reaching-a-state-of-flow-learning-tech&quot;&gt;Reaching a state of Flow learning tech&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;You can split your learning time into 70% working on a practical fun project of your choice and 30% reading books and tutorials or watching educational courses.Choose the ratio that works for you but it has to be at least 2:1 for active learning.&lt;&#x2F;p&gt;
&lt;p&gt;Both for active and passive learning: Eliminate distractions during your flow time and block out your coding time in large chunks.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;70% Active learning&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Always have a &lt;strong&gt;practical code project&lt;&#x2F;strong&gt; in the works rather than spending your time in a state of unfocused learning. If you don&#x27;t know what project to do, &lt;em&gt;think about the intersection between technology and your personal hobbies&lt;&#x2F;em&gt;*. What are your specific interests in a topic outside tech that you can use tech to solve a small issue or inconvenience?&lt;&#x2F;p&gt;
&lt;p&gt;Working this way makes sure you always work on fun projects to you. Long projects have to be somehow meaningful to you, otherwise is likely you drop them.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;30% Passive learning&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Try to &lt;strong&gt;consume high-quality information&lt;&#x2F;strong&gt;. For example, read a book instead of Medium article. The better the inputs, the better the outputs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;7-unix-principles&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-unix-principles&quot; aria-label=&quot;Anchor link for: 7-unix-principles&quot;&gt;7. Unix principles&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Make each function do one thing well.&lt;&#x2F;li&gt;
&lt;li&gt;Simple is better than complex.&lt;&#x2F;li&gt;
&lt;li&gt;Small is beautiful.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The basic idea of Unix philosophy is to build simple, clear, concise, modular code that is easy to extend and maintain.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;benefits-of-unix-principles&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#benefits-of-unix-principles&quot; aria-label=&quot;Anchor link for: benefits-of-unix-principles&quot;&gt;Benefits of Unix principles&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It reduces complexity, making the code easier to mantain.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The codebase will be easier to comprehend, debug and more fixable.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The codebase is more resilient and less prompt to unintended global effects.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Improves testability, as you can desing test for small components indendently.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The modularitity allows prototiping as soon as possible.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;unix-principles&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#unix-principles&quot; aria-label=&quot;Anchor link for: unix-principles&quot;&gt;Unix principles&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Build a prototype as soon as possible.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Choose portability over efficiency.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Portability is the ability of a system or a program to be moved from one environment to another and still function properly.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Store Data in flat files&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You should used optimized data representations only if you’re sure you need them. Using a database with a specialized format would reduce portability and add unnecessary complexity.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Use software leverage to your advantage.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;You should leverage the collective wisdom of generations of videos before you: use libraries and other people code and advise.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Maker every program a filter.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;A filter transforms an input to an output using a specific filtering mechanism. Think of functions taking inputs and somehow transforming or filtering the data into output.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The gold standard for filters is a homogeneous input&#x2F;output mapping&lt;&#x2F;strong&gt; where one type of inputs is mapped to the same type of output.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;average&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;args&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return sum&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;args&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;args&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;average&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;#2.0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Instead of &lt;strong&gt;heterogeneous input&#x2F;output&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;average&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;args&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Note that it returns a print statement, not a float
&lt;&#x2F;span&gt;&lt;span&gt;	&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;sum&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;args&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;args&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;average&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;#2.0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The first example inputs integers and outputs an string (heterogeneous) and the second inputs integers and output integers (homogeneous).&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Worse is better&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Developing code with less functionality is often the better approach in practice.&lt;&#x2F;strong&gt; A crude and straightforward solution to a problem gives you a first-mover advantage, attracts quick feedback from the early adopters and gains momentum and attention early in the software development process.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Clean code is better than clever code.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Clever code comes at the cost of expressing your ideas with clean code. You should not build and design programs to run in isolation. Design programs to connect with other programs.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;make-your-code-robust&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#make-your-code-robust&quot; aria-label=&quot;Anchor link for: make-your-code-robust&quot;&gt;Make your code robust&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Control access rights. Individual developers should not be able to contribute to the application without verifying with at least one addition person.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Use version control (Git).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Back up your application data regularly.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Repair what you can - but fail early and noisily. This means that you should not avoid making bugs visible if there is any. Silent bugs are application (and reputation) killers.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Avoid hand-hacking: write programs to write programs if you can. Code that can be generated automatically should be, because humans are notoriously prone to failures, especially in an activity that’s repetitive and boring.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;8-less-is-more-in-design&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#8-less-is-more-in-design&quot; aria-label=&quot;Anchor link for: 8-less-is-more-in-design&quot;&gt;8. Less is more in Design&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;use-familiar-designs&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#use-familiar-designs&quot; aria-label=&quot;Anchor link for: use-familiar-designs&quot;&gt;Use familiar designs&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;When thinking about your users, remember this: &lt;strong&gt;if you confuse them, you’ll lose them&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Use UX frameworks with bounded elements, borders, whitespaces, and intuitive. Most popular frameworks use theming following &lt;em&gt;Material Design&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Lacking boundaries and an intuitively familiar layout, the non-material design is often more confusing to the reader.&lt;&#x2F;p&gt;
&lt;p&gt;Whitespaces are necessary. Whitespace is a blank or empty space surrounding all the other elements in a design composition. They improve clarity and result in a more focused UX.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;addition-by-subtraction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#addition-by-subtraction&quot; aria-label=&quot;Anchor link for: addition-by-subtraction&quot;&gt;Addition by subtraction&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Remove Design Elements&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Go over all design elements and ask: Can I remove it?&lt;&#x2F;p&gt;
&lt;p&gt;The reduced clutter and increased focus are likely to increase the conversion rate of the order page through an improved UX.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Remove Features&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Over time, applications tend to accumulate features, a phenomenon known as &lt;strong&gt;feature creep&lt;&#x2F;strong&gt;. As a result, more and more focus must be shifted toward maintaining existing features. Feature creep leads to bloated soft-ware leads to technical debt.&lt;&#x2F;p&gt;
&lt;p&gt;Again, think about opportunity cost and if it’s possible to apply the Pareto principle and focus on the important features removing the rest.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reduce variations of fonts and colors&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Variations increase cognitive friction.&lt;&#x2F;strong&gt; Effective minimalist design often focuses on only one or two font types, one or two colors, and one or two font sizes.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;9-focus&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-focus&quot; aria-label=&quot;Anchor link for: 9-focus&quot;&gt;9. Focus&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;To be productive, you must &lt;strong&gt;reduce entropy. Entropy defines the degree of randomness, disorder, and uncertainty in a system. High entropy means high randomness and chaos. Low entropy means order and predictability.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;You are a creator and a builder. You take raw resources and move them from a state of high entropy into a state of low entropy using focused effort. That’s it.&lt;&#x2F;p&gt;
&lt;p&gt;As to achieve flow, you need a goal or a plan. Without a plan, all the efforts are unfocused since they do not go toward a specific goal - you are pulled in different directions.&lt;&#x2F;p&gt;
&lt;p&gt;The chapters of the book can be seen as ways of working to reduce entropy (e.g. focus on few features, build mvps that deliver only core features, write maintainable code, reduce and eliminate unnecessary codebase).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;To implement focus in your work, ask yourself these questions:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;On which software project do I want to focus my efforts?&lt;&#x2F;li&gt;
&lt;li&gt;Which features do I want to focus on to create my MVP?&lt;&#x2F;li&gt;
&lt;li&gt;What is the minimal number of design elements I can implement to test the viability of my product?&lt;&#x2F;li&gt;
&lt;li&gt;Who will use my product and why?&lt;&#x2F;li&gt;
&lt;li&gt;What can I remove from my code?&lt;&#x2F;li&gt;
&lt;li&gt;Do my functions do one thing only?&lt;&#x2F;li&gt;
&lt;li&gt;How can I achieve the same result in less time?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;end-words&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#end-words&quot; aria-label=&quot;Anchor link for: end-words&quot;&gt;End words&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The meaty part is in the Chapters 4, 5, and 6 where the author explains strategies to write clean code, avoid over engineering and get into an state of flow. Chapter 7 was completely avoidable in my opinion.&lt;&#x2F;p&gt;
&lt;p&gt;There are tons of advice suggested in &amp;lt;150 pages, but even if you only take 2 or 3 ideas it is worth reading it.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Using my desktop GPU remotely via Tailscale</title>
        <published>2024-03-17T00:00:00+00:00</published>
        <updated>2024-03-17T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/posts/ubuntu-server-tailscale/"/>
        <id>https://pipegalera.github.io/posts/ubuntu-server-tailscale/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/posts/ubuntu-server-tailscale/">&lt;p&gt;In this post I’ll explain how I connect to my gaming pc via SSH with my laptop completely remotely (and securely).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;ubuntu-server-tailscale&#x2F;.&#x2F;images&#x2F;remote_desktop.png&quot; alt=&quot;&quot; &#x2F;&gt;
&lt;em&gt;My MacBook Pro connected from the public library wifi to my home pc.&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;No need for port routing, gateway router config, fail2ban...not even touching ssh&#x2F;config!&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-case-problem&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-case-problem&quot; aria-label=&quot;Anchor link for: 1-case-problem&quot;&gt;1. Case problem&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Some months ago I wrote &lt;a href=&quot;https:&#x2F;&#x2F;pipegalera.com&#x2F;posts&#x2F;aws-instance-vscode&#x2F;&quot;&gt;a post&lt;&#x2F;a&gt; about connecting VSCode to an AWS instance. I was using AWS instances this way to train small ml models for panel data. You can also train models in your laptop, but it takes much longer without a good GPU.&lt;&#x2F;p&gt;
&lt;p&gt;The problem is that it can become expensive running instances with GPUs fairly quick. It sucked that every model or little experiment I run cost money.&lt;&#x2F;p&gt;
&lt;p&gt;Then it hit me... Why am I paying for a GPU when I do have a powerful GPU in my gaming pc?&lt;&#x2F;p&gt;
&lt;p&gt;It must be a way to connect my laptop to my home pc and use it as a server.&lt;&#x2F;p&gt;
&lt;p&gt;And yes it is, and it’s fairly easy.&lt;&#x2F;p&gt;
&lt;p&gt;I have to admit It took me 2 weeks to discover “the easy way” described in this post. Following the steps below should take ~2 hours to get everything up and running.&lt;&#x2F;p&gt;
&lt;p&gt;The only thing you need is a laptop (client), a home pc (server) and a $10 smart plug (e.g. &lt;em&gt;TP-Link Tapo Smart WLAN Socket&lt;&#x2F;em&gt;).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-installing-ubuntu-server&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-installing-ubuntu-server&quot; aria-label=&quot;Anchor link for: 2-installing-ubuntu-server&quot;&gt;2. Installing Ubuntu Server&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This covers installation and server configuration.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;bios-options&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#bios-options&quot; aria-label=&quot;Anchor link for: bios-options&quot;&gt;Bios options&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I needed to go to my MOBO bios settings (click &lt;code&gt;Del&lt;&#x2F;code&gt; when the pc is turning on) and change 2 options before installing Ubuntu, otherwise the installation crashed:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Security -&amp;gt; Secure Boot -&amp;gt; Disable&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;Advanced -&amp;gt; Settings -&amp;gt; Integrated graphics configuration -&amp;gt; Disable&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;After that, reboot.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;download-the-latest-ubuntu-server-lts-from-their-website&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#download-the-latest-ubuntu-server-lts-from-their-website&quot; aria-label=&quot;Anchor link for: download-the-latest-ubuntu-server-lts-from-their-website&quot;&gt;Download the latest Ubuntu Server LTS from &lt;a href=&quot;https:&#x2F;&#x2F;ubuntu.com&#x2F;download&#x2F;server&quot;&gt;their website&lt;&#x2F;a&gt;&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Burn it into a usb stick with &lt;a href=&quot;https:&#x2F;&#x2F;etcher.balena.io&#x2F;&quot;&gt;etcher balena&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;rufus.ie&#x2F;en&#x2F;&quot;&gt;rufus&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;settings-during-installation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#settings-during-installation&quot; aria-label=&quot;Anchor link for: settings-during-installation&quot;&gt;Settings during installation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Boot the pc from the USB stick and install Ubuntu Server in in a second hard drive or a partition. It doesn’t matter much, but I recommend using a second drive if you have an spare one around.&lt;&#x2F;p&gt;
&lt;p&gt;Settings:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt; Search for third-party drivers
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;[ ]&lt;&#x2F;span&gt;&lt;span&gt; Set up this disk as an LVM group
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;- &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt; Install OpenSSH server
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After the installation is done, remove the usb stick and boot into the Ubuntu server.&lt;&#x2F;p&gt;
&lt;p&gt;You might need to change the boot priorities in the bios if the initial reboot goes into Windows or whatever OS you have alongside Ubuntu Server.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;set-some-basic-initial-package-installation-and-connect-to-the-wifi&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#set-some-basic-initial-package-installation-and-connect-to-the-wifi&quot; aria-label=&quot;Anchor link for: set-some-basic-initial-package-installation-and-connect-to-the-wifi&quot;&gt;Set some basic initial package installation and connect to the WiFi&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To install the dependencies the server must be connected to the internet via ethernet cable.&lt;&#x2F;p&gt;
&lt;p&gt;The first thing I did is increasing the font of the terminal, so I didn’t have to squish my eyes every time I look at the screen:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; nano &#x2F;etc&#x2F;default&#x2F;console-setup
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;---&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span&gt;  FONTFACE=&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Terminus&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;---&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span&gt;  FONTSIZE=&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;16x32&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; update-initramfs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -u
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Reboot by typing &lt;code&gt;reboot&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;After rebooting, I updated the system and prepared the network options.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt-get update
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;systemctl&lt;&#x2F;span&gt;&lt;span&gt; disable systemd-networkd-wait-online.service
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;systemctl&lt;&#x2F;span&gt;&lt;span&gt; mask systemd-networkd-wait-online.service
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt install network-manager
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The above commands:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Update the system.&lt;&#x2F;li&gt;
&lt;li&gt;Avoid the system waiting 2 minutes every booth for the ethernet connection (server will be connected via WiFi).&lt;&#x2F;li&gt;
&lt;li&gt;Avoid the system to go to sleep&#x2F;suspend&#x2F;hibernate causing ssh to time out.&lt;&#x2F;li&gt;
&lt;li&gt;Install a network manager to connect via WiFi.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Once the network manager is installed I followed network-manager basic guide to connect to the WiFi: https:&#x2F;&#x2F;ubuntu.com&#x2F;core&#x2F;docs&#x2F;networkmanager&#x2F;configure-wifi-connections&lt;&#x2F;p&gt;
&lt;p&gt;Done with the basic configuration.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;install-the-other-packages-you-need&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#install-the-other-packages-you-need&quot; aria-label=&quot;Anchor link for: install-the-other-packages-you-need&quot;&gt;Install the other packages you need&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;This depends on the use of the server, for me it’s usually:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The proprietary NVIDIA GPU drivers&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Check the latest driver available at &lt;a href=&quot;https:&#x2F;&#x2F;www.nvidia.com&#x2F;en-us&#x2F;drivers&#x2F;unix&#x2F;&quot;&gt;this NVIDIA website&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; add-apt-repository ppa:graphics-drivers&#x2F;ppa
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; ubuntu-drivers install nvidia:550
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;Miniconda&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;wget&lt;&#x2F;span&gt;&lt;span&gt; https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-latest-Linux-x86_64.sh
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;bash&lt;&#x2F;span&gt;&lt;span&gt; Miniconda3-latest-Linux-x86_64.sh&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -b
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;cd&lt;&#x2F;span&gt;&lt;span&gt; miniconda3&#x2F;bin
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;.&#x2F;conda&lt;&#x2F;span&gt;&lt;span&gt; init bash
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;A few utility packages&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt install neofetch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; update-pciids
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt install net-tools
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;optional-for-dual-boot-with-windows-in-the-same-hard-drive&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#optional-for-dual-boot-with-windows-in-the-same-hard-drive&quot; aria-label=&quot;Anchor link for: optional-for-dual-boot-with-windows-in-the-same-hard-drive&quot;&gt;Optional. For dual boot with Windows in the same hard drive&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I use the pc for gaming in Windows 11, not only as a server. So I needed a way to select Windows if I wanted to play games when the pc boots.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure Ubuntu Server is the first option to boot in the bios.&lt;&#x2F;li&gt;
&lt;li&gt;Activate the GRUB menu in the Ubuntu Server and setting it to 10 seconds.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; nano &#x2F;etc&#x2F;default&#x2F;grub
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;---&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span&gt;  GRUB_TIMEOUT=10
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;---&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span&gt;  GRUB_TIMEOUT_STYLE=menu
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; update-grub
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This way it will boot on the menu and it’ll give 10 secs to select Ubuntu Server or Windows 11.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-connecting-your-laptop-to-your-server-with-tailscale&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-connecting-your-laptop-to-your-server-with-tailscale&quot; aria-label=&quot;Anchor link for: 3-connecting-your-laptop-to-your-server-with-tailscale&quot;&gt;3. Connecting your laptop to your server with Tailscale&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;ubuntu-server-tailscale&#x2F;.&#x2F;images&#x2F;tailscale.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;setting-up-a-tailscale-ssh&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#setting-up-a-tailscale-ssh&quot; aria-label=&quot;Anchor link for: setting-up-a-tailscale-ssh&quot;&gt;Setting up a Tailscale SSH&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;Install Tailscale in the client (e.g.laptop):&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;https:&#x2F;&#x2F;tailscale.com&#x2F;download&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Install Tailscale in the server (Ubuntu Server):&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;curl&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -fsSL&lt;&#x2F;span&gt;&lt;span&gt; https:&#x2F;&#x2F;tailscale.com&#x2F;install.sh &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sh
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Run &lt;code&gt;sudo tailscale up&lt;&#x2F;code&gt; and it will give you a website to visit in your client (e.g. &lt;em&gt;https&#x2F;&#x2F;login.tailscale.com&#x2F;a&#x2F;1204ecba01999&lt;&#x2F;em&gt;). After login in to the website, it should say &quot;Success.&quot; in the server terminal.&lt;&#x2F;p&gt;
&lt;p&gt;Run &lt;code&gt;sudo tailscale set --shh&lt;&#x2F;code&gt; to set the ssh profile.&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Into your Tailscale profile (https:&#x2F;&#x2F;login.tailscale.com&#x2F;admin&#x2F;machines) it should appear both machines, and the tag &lt;code&gt;SSH&lt;&#x2F;code&gt; under your server info&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;ubuntu-server-tailscale&#x2F;.&#x2F;images&#x2F;tailscale_screenshot.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Try typing &lt;code&gt;ssh root@{the server name}&lt;&#x2F;code&gt; from the terminal and check that the client connects:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;ubuntu-server-tailscale&#x2F;.&#x2F;images&#x2F;connection.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;After setting up Tailscale I could securely log in into my home pc from other networks.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;set-up-vscode-for-ssh&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#set-up-vscode-for-ssh&quot; aria-label=&quot;Anchor link for: set-up-vscode-for-ssh&quot;&gt;Set up VSCode for SSH&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Once the connection is created, to set up VSCode:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Install the &lt;a href=&quot;https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=Tailscale.vscode-tailscale&quot;&gt;Tailscale VSCode extension&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Click on the side panel Tailscale icon. Your server will appear, right click on it -&amp;gt; &lt;code&gt;Add to SSH config file&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;code.visualstudio.com&#x2F;docs&#x2F;remote&#x2F;ssh-tutorial&quot;&gt;Install Remote-SSH&lt;&#x2F;a&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Open Command Palette(&lt;code&gt;Cmd+Shift+P&lt;&#x2F;code&gt;) and search “Connect host” and the server name will pop up to connect.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;4-automatic-turn-on-remotely-with-a-smart-plug&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-automatic-turn-on-remotely-with-a-smart-plug&quot; aria-label=&quot;Anchor link for: 4-automatic-turn-on-remotely-with-a-smart-plug&quot;&gt;4. Automatic turn on remotely with a smart plug&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;ubuntu-server-tailscale&#x2F;.&#x2F;images&#x2F;tapo.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This is the cherry on top of the cake.&lt;&#x2F;p&gt;
&lt;p&gt;I can always turn off the pc server remotely by typing &lt;code&gt;poweroff&lt;&#x2F;code&gt; in the terminal . But &lt;strong&gt;how do I turn the server on if the home pc is not powered on?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We&#x27;ll use a trick with a smart plug and a bios functionality for that.&lt;&#x2F;p&gt;
&lt;p&gt;Most bios have a &lt;em&gt;Wake on power&lt;&#x2F;em&gt; mode that basically turn on the computer once they receive electric power from. On my MSI mobo bios, the option is: &lt;code&gt;Advanced -&amp;gt; APM Configuration -&amp;gt; &quot;Restore AC Power Loss&quot; -&amp;gt; Enable&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;With this setting, the pc will turn on every time it receives electric current.&lt;&#x2F;p&gt;
&lt;p&gt;By using a smart plug I can control the electric current that goes to the plug from the phone app - So I can power on the computer from the phone.&lt;&#x2F;p&gt;
&lt;p&gt;Example use:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Remotely, I go to the smart plug app and activate the plug (0 to 1 electricity). The computer turn on and I can access via SSH.&lt;&#x2F;li&gt;
&lt;li&gt;I turn it off using &lt;code&gt;poweroff&lt;&#x2F;code&gt; since I don’t need it.&lt;&#x2F;li&gt;
&lt;li&gt;I got an idea and need the server GPU again. I go to app, turn off the plug and on again and the computer turns on.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;It’s more difficult to explain than to do. It honestly takes one or two taps on the phone and the server pc is turned on.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;5-final-thoughts-and-benefits-vs-cloud-computing&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-final-thoughts-and-benefits-vs-cloud-computing&quot; aria-label=&quot;Anchor link for: 5-final-thoughts-and-benefits-vs-cloud-computing&quot;&gt;5. Final thoughts and benefits vs cloud computing&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;It was surprisingly easy setting up the server once I discovered Tailscale. This solution for running “small” models remotely is:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Way cheaper than cloud&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The cost of electricity is negligible since new NVIDIA GPUs are very efficient and the server can be turn on and off on command. Smart plugs cost &amp;lt;$10 and can be reused for other smart-home stuff.&lt;&#x2F;p&gt;
&lt;p&gt;This obviously only applies if you have a home pc to use as a server, but it doesn&#x27;t need a fancy GPU - reusing any GPU will cost less than paying for a GPU to a third party.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Headache free&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I had little nightmares about forgetting to turn off the instance and waking up the next day with a hole in my wallet.&lt;&#x2F;p&gt;
&lt;p&gt;I can let run a couple of experiments freely while doing other things without having a constant thought of having to go back to close the AWS.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Dead simple&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I open the smart plug app on the phone to turn on the server and open VSCode on my laptop. 30 seconds and I&#x27;m in.&lt;&#x2F;p&gt;
&lt;p&gt;That’s it.&lt;&#x2F;p&gt;
&lt;p&gt;No need to go to AWS to open an instance, not changing the SSH config to the instance dynamic ip, no Terraform, no products I do not need.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Increased productivity&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I can make stupid experiments, weird test and tinkering in general with models - which translates into more coding hours.&lt;&#x2F;p&gt;
&lt;p&gt;Zero remorse. No more &lt;em&gt;&quot;if run this model for 20 hours and it doesn’t converge it will be a waste of money&quot;&lt;&#x2F;em&gt; mindset.&lt;&#x2F;p&gt;
&lt;p&gt;Since I set up the server I’m constantly trying new stuff and writing more code.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;I hope you enjoyed the reading and give home servers a try. It was very fun to learn about internet networking and very practical to learn how to access my GPU everywhere.&lt;&#x2F;p&gt;
&lt;p&gt;Have a good day!&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>How to set up a AWS EC2 Instance in VSCode</title>
        <published>2024-01-05T00:00:00+00:00</published>
        <updated>2024-01-05T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/posts/aws-instance-vscode/"/>
        <id>https://pipegalera.github.io/posts/aws-instance-vscode/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/posts/aws-instance-vscode/">&lt;p&gt;I will explain how to connect to a remote machine to VSCode. The remote machine will be a ubuntu server in a AWS instance that, in my case, will be used to run lightweight ml models.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;vscode.png&quot; alt=&quot;where to put the key&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-create-an-aws-instance&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-create-an-aws-instance&quot; aria-label=&quot;Anchor link for: 1-create-an-aws-instance&quot;&gt;1. Create an AWS instance&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;First, just google AWS, create an account, and you will see the option to create an instance in the main page.&lt;&#x2F;p&gt;
&lt;p&gt;Free accounts have a free tier. Selecting everything with the &quot;Free tier eligible&quot; &lt;em&gt;should&lt;&#x2F;em&gt; be free, but please take care what you launch anyway and do not select a 98 Ram machine with a Tesla V100 GPU and whatnot unless you know what you doing.&lt;&#x2F;p&gt;
&lt;p&gt;Here I will set up a cheap machine (like $0.1&#x2F;hour):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;aws_instance.png&quot; alt=&quot;AWS instance summary&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;OS: Ubuntu Server 22.4 64-bit (x86)&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Image: t3.large or t3.xlarge (8&#x2F;16 Gb of RAM)&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Key pair: New key -&amp;gt; place the pem file in the directory under your ~&#x2F;.ssh&#x2F; folder (I called mine &lt;em&gt;mlops_key&lt;&#x2F;em&gt;):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;ssh_folder.png&quot; alt=&quot;where to put the key&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Network settings: Allow SSH traffic from “Anywhere”. As long as you configure the key pair you should be okay.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Storage: 30 Gb&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;If the SSH key file was placed correctly, you can access the machine via ssh using the Public PIv4 address you can find in your instance summary website:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ssh&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;~&lt;&#x2F;span&gt;&lt;span&gt;&#x2F;.ssh&#x2F;mlops_key.pem ubuntu@&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;Instance public IPv4 address&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;aws_connection.png&quot; alt=&quot;AWS connection&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;P.S. I did have a problem the first time I tried to connect with &quot;permissions being too open&quot;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;permission_too_open.png&quot; alt=&quot;AWS instance summary&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;That can be solved by running &lt;code&gt;chmod 600 ~&#x2F;.ssh&#x2F;mlops_key.pem&lt;&#x2F;code&gt; &lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;9270734&#x2F;ssh-permissions-are-too-open&quot;&gt;(Source)&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We would like to avoid typing the IP of the virtual OS every time we connect to the instance and simply connect typing &lt;em&gt;&quot;ssh mlops&quot;&lt;&#x2F;em&gt;, for example.&lt;&#x2F;p&gt;
&lt;p&gt;To create this shortcut, modify the config file under ssh or your local machine:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Host&lt;&#x2F;span&gt;&lt;span&gt; mlops
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;HostName &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;Instance public IPv4 address&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;User&lt;&#x2F;span&gt;&lt;span&gt; ubuntu
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;IdentityFile &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;~&lt;&#x2F;span&gt;&lt;span&gt;&#x2F;.ssh&#x2F;mlops_key.pem
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;StrictHostKeyChecking&lt;&#x2F;span&gt;&lt;span&gt; no
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;E.g.:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;automatic_connection.png&quot; alt=&quot;Automatic connection&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-install-stuff-in-the-ubuntu-os&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-install-stuff-in-the-ubuntu-os&quot; aria-label=&quot;Anchor link for: 2-install-stuff-in-the-ubuntu-os&quot;&gt;2. Install stuff in the ubuntu OS&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Docker&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Update apt
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt update
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Install docker and docker-compose
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt install docker.io docker-compose
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# To us docker without using sudo continuously
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; usermod&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -aG&lt;&#x2F;span&gt;&lt;span&gt; docker &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;$&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;USER
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;Miniconda&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Download miniconda
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;wget&lt;&#x2F;span&gt;&lt;span&gt; https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-latest-Linux-x86_64.sh
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Run the installer
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;bash&lt;&#x2F;span&gt;&lt;span&gt; Miniconda3-latest-Linux-x86_64.sh&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -b
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Add Anaconda to the system path
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;cd&lt;&#x2F;span&gt;&lt;span&gt; miniconda3&#x2F;bin
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;.&#x2F;conda&lt;&#x2F;span&gt;&lt;span&gt; init bash
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;After this, “which python” should point out to the anaconda version:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;which_python.png&quot; alt=&quot;Which python&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-configure-vscode&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-configure-vscode&quot; aria-label=&quot;Anchor link for: 3-configure-vscode&quot;&gt;3. Configure VSCode&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Open VSCode locally and install the following:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Install Remote-SSH: https:&#x2F;&#x2F;code.visualstudio.com&#x2F;docs&#x2F;remote&#x2F;ssh-tutorial&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Install Jupyter: https:&#x2F;&#x2F;marketplace.visualstudio.com&#x2F;items?itemName=ms-toolsai.jupyter&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Open Command Palette(&lt;code&gt;Cmd+Shift+P&lt;&#x2F;code&gt;) and search &quot;Connect host&quot; and it will pop up:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;vscode_ssh.png&quot; alt=&quot;Host name&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It will automatically read the Host from the .ssh&#x2F;config file:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;host_name.png&quot; alt=&quot;Host name&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;And that&#x27;s it, you have VSCode connected to remote ubuntu machine.&lt;&#x2F;p&gt;
&lt;p&gt;Clicking open folder will load a tree view. From there you can easily organize files, drag&amp;amp;drop files from your local computer to the folders in the remote machine, or create new files. Like you would locally.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;aws-instance-vscode&#x2F;.&#x2F;images&#x2F;vscode.png&quot; alt=&quot;where to put the key&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Please remember to stop the instance in AWS to avoid getting charged while not using it.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Four Thousand Weeks, by Oliver Burkeman</title>
        <published>2023-10-10T00:00:00+00:00</published>
        <updated>2023-10-10T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-four-thousand-weeks/"/>
        <id>https://pipegalera.github.io/mostly_books/book-four-thousand-weeks/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-four-thousand-weeks/">&lt;p&gt;The book explores a better way to handle time and it shares ideas from wise thinkers who emphasized understanding time instead of controlling it.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;41uw2Gp4x4L._SY445_SX342_.jpg&quot; alt=&quot;book cover&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;personal-opinion-best-and-worst-of-the-book&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#personal-opinion-best-and-worst-of-the-book&quot; aria-label=&quot;Anchor link for: personal-opinion-best-and-worst-of-the-book&quot;&gt;Personal opinion: Best and worst of the book&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;It &lt;strong&gt;acknowledges the central lie of most self-help books&lt;&#x2F;strong&gt;: &lt;em&gt;&quot;Be more productive so you will have more time to do the things you really love&quot;&lt;&#x2F;em&gt;. The reality is that you cannot do it all, you won’t, and you will die without doing all you once dreamed you might do. You have to choose.&lt;&#x2F;li&gt;
&lt;li&gt;Chapters are &lt;strong&gt;repetitive&lt;&#x2F;strong&gt;, constantly ruminating with the central idea of the book. It could have been at least 100 pages shorter without lacking any sustenance.&lt;&#x2F;li&gt;
&lt;li&gt;I found incredibly &lt;strong&gt;sad&lt;&#x2F;strong&gt; the hint message &lt;em&gt;&quot;Life is too short so we just need to settle with whatever, it doesn&#x27;t matter anyway.&quot;&lt;&#x2F;em&gt; . Life can be short and matter, aspiring to live a good life is not fruitless.&lt;&#x2F;li&gt;
&lt;li&gt;The author takes the concept of time too far. An example is the section about distractions, which was insightful, until the author attribute the human lack of focus with &lt;em&gt;seeking relief from the discomfort of confronting limitation.&lt;&#x2F;em&gt; Not every human condition has to be linked with &lt;em&gt;&quot;confronting finitude&quot;&lt;&#x2F;em&gt;. If the only tool you have is a hammer, you tend to see every problem as a nail.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;1-fast-society&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-fast-society&quot; aria-label=&quot;Anchor link for: 1-fast-society&quot;&gt;1. Fast Society&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Since the beginning of the modern era of acceleration, people have been responding not with satisfaction at all the time saved but with &lt;strong&gt;increasing agitation that they can’t make things move faster still&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;As the world gets faster and faster, we come to believe that our happiness, or our financial survival, depends on our being able to work and move and make things happen at superhuman speed. We grow anxious about not keeping up—so to quell the anxiety, &lt;strong&gt;to try to achieve the feeling that our lives are under control, we move faster&lt;&#x2F;strong&gt;. But this only generates an addictive spiral. We push ourselves harder to get rid of anxiety, but the result is actually more anxiety, because the faster we go, the clearer it becomes that we’ll never succeed in getting ourselves or the rest of the world to move as fast as we feel is necessary.&lt;&#x2F;p&gt;
&lt;p&gt;Speed addiction tends to be socially celebrated. Your friends are more likely to praise you for being “driven.”&lt;&#x2F;p&gt;
&lt;p&gt;The modern world provides an inexhaustible supply of things that seem worth doing, and so there &lt;strong&gt;arises an inevitable and unbridgeable gap between what you’d ideally like to do and what you actually can do&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Busyness has been rebranded as “hustle” or relentless work not as a burden to be endured but as an exhilarating lifestyle choice, worth boasting about on social media.&lt;&#x2F;p&gt;
&lt;p&gt;Time feels like an unstoppable conveyor belt, bringing us new tasks as fast as we can dispatch the old ones; and becoming “more productive” just seems to cause the belt to speed up.&lt;&#x2F;p&gt;
&lt;p&gt;It turns out that when people make enough money to meet their needs, they just find new things to need and new lifestyles to aspire to; &lt;strong&gt;they never quite manage to keep up with the Joneses, because whenever they’re in danger of getting close, they nominate new and better Joneses with whom to try to keep up. As a result, they work harder and harder, and soon busyness becomes an emblem of prestige. Which is clearly completely absurd: for almost the whole of history, the entire point of being rich was not having to work so much.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Our days are spent trying to &lt;strong&gt;“get through”&lt;&#x2F;strong&gt; tasks, in order to get them &lt;em&gt;“out of the way,”&lt;&#x2F;em&gt; with the result that we live mentally in the future, waiting for when we’ll finally get around to what really matters.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Even the winners in our achievement-obsessed culture&lt;&#x2F;strong&gt;, the ones who make it to the elite universities, then reap the highest salaries find that &lt;strong&gt;their reward is the unending pressure to work with “crushing intensity” in order to maintain the income and status&lt;&#x2F;strong&gt; that have come to seem like prerequisites for the lives they want to lead.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-facing-the-reality-of-time&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-facing-the-reality-of-time&quot; aria-label=&quot;Anchor link for: 2-facing-the-reality-of-time&quot;&gt;2. Facing the reality of time&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;It’s hard to imagine a crueler arrangement: not only are our four thousand weeks constantly running out, but the fewer of them we have left, the faster we seem to lose them.&lt;&#x2F;p&gt;
&lt;p&gt;We are forced to acknowledge that there are hard choices to be made: which balls to let drop, which people to disappoint, which cherished ambitions to abandon, which roles to fail at.&lt;&#x2F;p&gt;
&lt;p&gt;Maybe you can’t keep your current job while also seeing enough of your children; maybe making sufficient time in the week for your creative calling means you’ll never have an especially tidy home, or get quite as much exercise as you should, and so on.&lt;&#x2F;p&gt;
&lt;p&gt;Instead, &lt;strong&gt;in an attempt to avoid these unpleasant truths, we deploy the strategy that dominates most conventional advice on how to deal with busyness: we tell ourselves we’ll just have to find a way to do more—to try to address our busyness, you could say, by making ourselves busier still.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It’s painful to confront how limited your time is, because it means that tough choices are inevitable and that you won’t have time for all you once dreamed you might do.&lt;&#x2F;p&gt;
&lt;p&gt;No other time management technique that’s half as effective as just facing the way things truly are. The more you confront the facts of finitude instead—and work with them, rather than against them—the more productive, meaningful, and joyful life becomes.&lt;&#x2F;p&gt;
&lt;p&gt;I tried to align my daily actions with my goals, and my goals with my core values. Using these techniques often made me feel &lt;strong&gt;as if I were on the verge of ushering in a golden era of calm, undistracted productivity and meaningful activity. But it never arrived.&lt;&#x2F;strong&gt; Instead, I just got more stressed and unhappy.&lt;&#x2F;p&gt;
&lt;p&gt;We don’t want to feel the anxiety that might arise if we were to ask ourselves whether we’re on the right path, or what ideas about ourselves it could be time to give up. We don’t want to risk getting hurt in relationships or failing professionally; we don’t want to accept that we might never succeed in pleasing our parents or in changing certain things we don’t like about ourselves—and we certainly don’t want to get sick and die.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The more you believe you might succeed in “fitting everything in,” the more commitments you naturally take on&lt;&#x2F;strong&gt;, and the less you feel the need to ask whether each new commitment is truly worth a portion of your time—and so your days inevitably fill with more activities you don’t especially value.&lt;&#x2F;p&gt;
&lt;p&gt;Most of us do, most of the time, instead of confronting our finitude, which is to indulge in avoidance and denial, or what Heidegger calls “falling.” Rather than taking ownership of our lives, we seek out distractions, or lose ourselves in busyness and the daily grind, so as to try to forget our real predicament.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-distractions&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-distractions&quot; aria-label=&quot;Anchor link for: 3-distractions&quot;&gt;3. Distractions&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;What you pay attention to will define, for you, what reality is.&lt;&#x2F;p&gt;
&lt;p&gt;The elephant-in-the-room problem with everything I’ve been arguing so far about time and time management. That problem is distraction. After all, &lt;strong&gt;it hardly matters how committed you are to making the best use of your limited time if, day after day, your attention gets wrenched away by things on which you never wanted to focus.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Achieving total sovereignty over your attention is almost certainly impossible.&lt;&#x2F;p&gt;
&lt;p&gt;As you surface from &lt;strong&gt;an hour inadvertently frittered away on Facebook, you’d be forgiven for assuming that the damage, in terms of wasted time, was limited to that single misspent hour. But you’d be wrong.&lt;&#x2F;strong&gt; Because the attention economy is designed to prioritize whatever’s most compelling—instead of whatever’s most true, or most useful—it systematically distorts the picture of the world we carry in our heads at all times. &lt;strong&gt;It influences our sense of what matters&lt;&#x2F;strong&gt;, what kinds of threats we face, how venal our political opponents are, and thousands of other things—and all these distorted judgments then influence how we allocate our offline time as well.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;4-short-life&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-short-life&quot; aria-label=&quot;Anchor link for: 4-short-life&quot;&gt;4. Short life&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Convenience culture seduces us into imagining that we might find room for everything important by eliminating only life’s tedious tasks. But it’s a lie.&lt;&#x2F;strong&gt; You have to choose a few things, sacrifice everything else, and deal with the inevitable sense of loss that results.&lt;&#x2F;p&gt;
&lt;p&gt;The original Latin word for “decide,” decidere, means “to cut off,” as in slicing away alternatives; it’s a close cousin of words like “homicide” and “suicide.”&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Every decision to use a portion of time on anything represents the sacrifice of all the other ways&lt;&#x2F;strong&gt; in which you could have spent that time, but didn’t and to willingly make that sacrifice is to take a stand, without reservation, on what matters most to you.&lt;&#x2F;p&gt;
&lt;p&gt;This dream of somehow one day getting the upper hand in our relationship with time is the most forgivable of human delusions because the alternative is so unsettling. But unfortunately, it’s the alternative that’s true: the struggle is doomed to fail. Because your quantity of time is so limited, you’ll never reach the commanding position of being able to handle every demand that might be thrown at you or pursue every ambition that feels important.&lt;&#x2F;p&gt;
&lt;p&gt;You’ll be obliged to make tough choices instead. And because you can’t dictate, or even accurately predict, so much of what happens with the finite portion of time you do get, you’ll never feel that you’re securely in charge of events, immune from suffering.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;You’ll do what you can, you won’t do what you can’t, and the tyrannical inner voice insisting that you must do everything is simply mistaken.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The problem with trying to make time for everything that feels important or just for enough of what feels important is that you definitely never will. &lt;strong&gt;The reason isn’t that you haven’t yet discovered the right time management tricks or applied sufficient effort, or that you need to start getting up earlier, or that you’re generally useless.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The only route to psychological freedom is to let go of the limit-denying fantasy of getting it all done and instead to &lt;strong&gt;focus on doing a few things that count.&lt;&#x2F;strong&gt;. Negligent emailers frequently find that forgetting to reply ends up saving them time: people find alternative solutions to the problems they were nagging you to solve, or the looming crisis they were emailing about never materializes.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;5-instrumentalization-of-time&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-instrumentalization-of-time&quot; aria-label=&quot;Anchor link for: 5-instrumentalization-of-time&quot;&gt;5. Instrumentalization of time&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Inevitably, we become obsessed with “using time well,” whereupon we discover an unfortunate truth: &lt;strong&gt;the more you focus on using time well, the more each day begins to feel like something you have to get through&lt;&#x2F;strong&gt;, en route to some calmer, better, more fulfilling point in the future, which never actually arrives. The problem is one of instrumentalization.&lt;&#x2F;p&gt;
&lt;p&gt;When your relationship with time is almost entirely instrumental, the present moment starts to lose its meaning.&lt;&#x2F;p&gt;
&lt;p&gt;Focus exclusively on where you’re headed, at the expense of focusing on where you are with the result that you &lt;strong&gt;find yourself living mentally in the future, locating the “real” value of your life at some time that you haven’t yet reached.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A more fruitful approach to the challenge of living more fully in the moment starts from noticing that you are, in fact, always already living in the moment anyway, whether you like it or not.&lt;&#x2F;p&gt;
&lt;p&gt;This is also why it can be so unexpectedly calming to take actions you’d been fearing or delaying—to finally hand in your notice at work, become a parent, address a festering family issue, or close on a house purchase. &lt;strong&gt;When you can no longer turn back, anxiety falls away, because now there’s only one direction to travel: forward into the consequences of your choice.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;6-leisure&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-leisure&quot; aria-label=&quot;Anchor link for: 6-leisure&quot;&gt;6. Leisure&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;We actually have more leisure time than we did in previous decades&lt;&#x2F;strong&gt;, an average of about five hours per day for men, and only slightly less for women. But perhaps &lt;strong&gt;one reason we don’t experience life that way is that leisure no longer feels very leisurely&lt;&#x2F;strong&gt;. Instead, it too often feels like another item on the to-do list.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Enjoying leisure&lt;&#x2F;strong&gt; for its own sake—which you might have assumed was the whole point of leisure comes to feel as though &lt;strong&gt;it’s somehow not quite enough&lt;&#x2F;strong&gt;. It begins to feel as though you’re failing at life, in some indistinct way, if you’re not treating your time off as an investment in your future.&lt;&#x2F;p&gt;
&lt;p&gt;Rest is permissible, but only for the purposes of recuperation for work, or perhaps for some other form of self-improvement. &lt;strong&gt;It becomes difficult to enjoy a moment of rest for itself alone, without regard for any potential future benefits, because rest that has no instrumental value feels wasteful.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As long as you’re filling every hour of the day with some form of striving, you get to carry on believing that all this striving is leading you somewhere, &lt;strong&gt;to an imagined future state of perfection, a heavenly realm in which everything runs smoothly&lt;&#x2F;strong&gt;, your limited time causes you no pain, and you’re free of the guilty sense that there’s more you need to be doing in order to justify your existence.&lt;&#x2F;p&gt;
&lt;p&gt;Leisure, unlike almost everything else I do with my life, it’s not relevant to ask whether I’m any good at it: it doesn’t have a purpose, in the sense of an outcome you’re trying to achieve or somewhere you’re trying to get.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;7-cosmic-insignificance&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-cosmic-insignificance&quot; aria-label=&quot;Anchor link for: 7-cosmic-insignificance&quot;&gt;7. Cosmic insignificance&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;It’s fundamental to being human the understandable tendency to judge everything from the perspective you occupy, so that the few thousand weeks for which you happen to be around inevitably come to feel like the linchpin of history, to which all prior time was always leading up.&lt;&#x2F;p&gt;
&lt;p&gt;This overvaluing of your existence gives rise to an &lt;strong&gt;unrealistic definition of what it would mean to use your finite time well.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;From a truly cosmic view, it will soon be forgotten, like everything else.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This realization isn’t merely calming but &lt;strong&gt;liberating, because once you’re no longer burdened by such an unrealistic definition of a “life well spent,”&lt;&#x2F;strong&gt; you’re freed to consider the possibility that a far wider variety of things might qualify as meaningful ways to use your finite time.&lt;&#x2F;p&gt;
&lt;p&gt;Cosmic insignificance therapy is an invitation to face the truth about your irrelevance in the grand scheme of things.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;8-random-productivity-tips&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#8-random-productivity-tips&quot; aria-label=&quot;Anchor link for: 8-random-productivity-tips&quot;&gt;8. Random productivity tips.&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This is a very random section and totally dispensable section at the end of the book.&lt;&#x2F;p&gt;
&lt;p&gt;It seems that the publisher realised that the book offers no tools or practical advice more than &lt;em&gt;&quot;acknowledge that you are going to die&quot;&lt;&#x2F;em&gt;, and forced the author to add a sections in which he repeats the same productivity tips already exhausted in self-help books.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Pay yourself first when it comes to time - a.k.a prioritise what makes you happy.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Limit your work in progress and resist the allure of middling priorities - a.k.a focus in a few tasks each time.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Give the task the time it needs.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Hell Yeah or No, by Derek Sivers</title>
        <published>2023-10-07T00:00:00+00:00</published>
        <updated>2023-10-07T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-hell-yeah-or-no/"/>
        <id>https://pipegalera.github.io/mostly_books/book-hell-yeah-or-no/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-hell-yeah-or-no/">&lt;p&gt;Find all the chapters for free at https:&#x2F;&#x2F;sive.rs&#x2F;n .&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;51ZvJhDkGqL._SY522_.jpg&quot; alt=&quot;book cover&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;about-what-to-do&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#about-what-to-do&quot; aria-label=&quot;Anchor link for: about-what-to-do&quot;&gt;About what to do&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If you’re not feeling “Hell yeah, that would be awesome!” about something, say no. It’s an easier decision. &lt;strong&gt;Say no to almost everything&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;“Hell yeah or no”&lt;&#x2F;em&gt; is a filter you can use to decide what’s worth doing. But this is simpler and more serious. This is a decision to stop deciding. It’s one decision, in advance, that the answer to all future distractions is “no” until you finish what you started. It’s &lt;strong&gt;saying yes to one thing, and no to absolutely everything else&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Like everyone, I have a list of boring chores that need to be done but that I’ve been putting off for years. I never do them because I’m always more excited about something else. During my last unmotivated funk, I realized that because nothing is exciting me, that means nothing is exciting me more than this boring necessary stuff. It is the perfect time to do those dull tasks. The &lt;strong&gt;next time you’re feeling extremely unmotivated&lt;&#x2F;strong&gt;, do those things you never want to do anyway.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;What do you hate not doing?&lt;&#x2F;strong&gt; What makes you feel depressed, annoyed, or like your life has gone astray if you don’t do it enough?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;No matter what you tell the world or tell yourself, &lt;strong&gt;your actions reveal your real values&lt;&#x2F;strong&gt;. Your actions show you what you actually want.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Stop lying to yourself, and &lt;strong&gt;admit your real priorities&lt;&#x2F;strong&gt;. Start doing what you say you want to do, and see if it’s really true.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Before you start something, think of the ways it could end. Sometimes the smart choice is to say no to the whole game.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;about-mindset&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#about-mindset&quot; aria-label=&quot;Anchor link for: about-mindset&quot;&gt;About mindset&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If we hate doing something, we think of it as hard. We picture it having many annoying steps. If we love doing something it seems simple. We think of it as one fun step. Even &lt;strong&gt;if you say you want to do something, if you catch yourself thinking of it in many tedious steps, maybe you don’t really want to do it&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Most people overestimate what they can do in one year, and underestimate what they can do in ten years.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Getting out of a bad state of mind:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Ask myself what’s wrong in this very second.&lt;&#x2F;li&gt;
&lt;li&gt;Observe now. Act later.&lt;&#x2F;li&gt;
&lt;li&gt;Raise standards. Say no to anything less than great. When I’m down, I avoid anyone who doesn’t rejuvenate me.&lt;&#x2F;li&gt;
&lt;li&gt;Focus on my goal.&lt;&#x2F;li&gt;
&lt;li&gt;Do all the necessary stuff. These tasks are so mundane, but they help me to feel on top of things. When everyday responsibilities are done, my mind is less distracted.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;When I notice that I’m all stressed out about something or driving myself to exhaustion, I remember that bike ride and &lt;strong&gt;try dialing back my effort by 50 percent. It’s been amazing how often everything gets done just as well&lt;&#x2F;strong&gt; and just as fast, with what feels like half the effort.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;All the best, happiest, and most creatively productive times in my life have something in common: being disconnected&lt;&#x2F;strong&gt;. No internet. No TV. No phone. No people. Long uninterrupted solitude.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Constantly seeking inspiration is anti-inspiring&lt;&#x2F;strong&gt;. You have to pause the input and focus on your output. For every bit of inspiration you take in, use it and amplify it by applying it to your work. Then you’ll finally feel the inspiration you’ve been looking for. Breathe in. Breathe out. Breathe in. Breathe out.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Whatever scares you, go do it&lt;&#x2F;strong&gt;. Fear is just a form of excitement, and you know you should do what excites you.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;about-business&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#about-business&quot; aria-label=&quot;Anchor link for: about-business&quot;&gt;About business&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;You know that existing business that you wish you had thought of? Copy it. Why? Because &lt;strong&gt;we’re imperfect mirrors&lt;&#x2F;strong&gt;. Like a funhouse mirror that distorts what it reflects, your imitation will turn out much different from the original. Maybe even better.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Which then makes me realize that half of my effort wasn’t effort at all, but just unnecessary stress that made me feel like I was doing my best&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>The Pathless Path, by Paul Millerd</title>
        <published>2023-08-21T00:00:00+00:00</published>
        <updated>2023-08-21T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-pathless-path/"/>
        <id>https://pipegalera.github.io/mostly_books/book-pathless-path/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-pathless-path/">&lt;p&gt;The book is aimed to career-oriented people that reached (what can be seen as) professional success but do not feel meaning in their job.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;41ZCxV0jqxL._SY445_SX342_.jpg&quot; alt=&quot;book cover&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I liked the book a lot. It offers a great read for those considering a non-traditional career path and challenges your beliefs about a work-centered life and the default path as the one-and-only way of living.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;personal-opinion-best-and-worst-of-the-book&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#personal-opinion-best-and-worst-of-the-book&quot; aria-label=&quot;Anchor link for: personal-opinion-best-and-worst-of-the-book&quot;&gt;Personal opinion: Best and worst of the book&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;best&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#best&quot; aria-label=&quot;Anchor link for: best&quot;&gt;Best&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It is not a practical guide of how to leave your tedious job. What people might consider &quot;fluff&quot; is the best part of the book (e.g. his feels at different stages of leaving the default path, the references to Taggart or Callard philosophy, the different views of working as a part of life across history...) .&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;It gives a perspective of living a life outside of the 9-5 path, challenging your prior beliefs. My beliefs about how a good life should be are deeply rooted in finding a good job that you like and find meaning. This view is inherited from my family&#x2F;society, and I never really look at other ways of living outside this rather narrow focus on finding the right job that leads a good life.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;For anyone that it&#x27;s in the early stages of the &quot;pathless path&quot; or thinking about living a non-traditional life, this book is very useful. The story is inspirational and real (no Lamborghini with 24 years old working 4 hours per week). The author is honest displaying his fears, emotions, and struggles (e.g. lack of stable income), and offer tools to learn how to be comfortable with it and adapt. He walks the path.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;It forces you to think about what do you value in life, if you are prioritizing what you believe is important.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;worst&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#worst&quot; aria-label=&quot;Anchor link for: worst&quot;&gt;Worst&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;His experience relate better to you if you have business&#x2F;corporate experience. He tells his own professional career story, It&#x27;s not a catch &#x27;em all book for all audiences.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The path of &quot;meaningless corporate job to traveler content creator&quot; is becoming a bit boring to read. I wish someone wrote about corporate clerk becoming carpenter and moving to Utah. I guess it&#x27;s also &quot;survival bias&quot; + only writers write about their life.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;It is easier to go rogue when you have an impressive CV like the author, as the risk is minimum - you have plenty of savings and can always find a job if self-employment doesn&#x27;t work. It&#x27;s a privilege position. However, leaving behind a comfortable life in paper to face the uncertainty is precisely the story of the book.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;1-the-default-path&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-the-default-path&quot; aria-label=&quot;Anchor link for: 1-the-default-path&quot;&gt;1. The default path&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The default path is the path that most people take: go to university, find a good job with good salary and working conditions, do what you like on the weekends and free time.&lt;&#x2F;p&gt;
&lt;p&gt;Let me be clear. For me, this is a reasonable good life path.&lt;&#x2F;p&gt;
&lt;p&gt;A bad life path is working 12-hours shifts in a factory.&lt;&#x2F;p&gt;
&lt;p&gt;The default path however comes with drawbacks. Most jobs that provide good salaries they demand a lot of energy and responsibility. What was a nine-to-five job on paper is more 7:30 to 19:00 taking commuting and some time to stop thinking about your last meeting. By 19:00 you barely have energy for that much besides having dinner and prep for the next day. Your hobbies...better do them on the weekends.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;images1.memedroid.com&#x2F;images&#x2F;UPLOADED11&#x2F;5033ee798198d.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Most people embrace this lifestyle without that much hesitation, simply because everyone is doing it.&lt;&#x2F;p&gt;
&lt;p&gt;Why would you do something else? Your family and friends often comment on how lucky you are for your conventional job. The most common alternative to full-time is... being unemployed and sad faces when people ask you &lt;em&gt;&quot;So, what do you do?&quot;&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;If unhappy, try looking for another job.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The lack of different lifestyles that do not include a full-time job limit your beliefs of whats possible&lt;&#x2F;strong&gt;. It seems that everyone is embracing full-time working and so do you.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;I never realized how much I was constraining my imagination when I was only considering paths or jobs that already existed.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;What if it could be another way?&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;By my late 20s, I had oriented my entire life around work. I was always thinking about how I could get a better job or a higher salary. At the time I could not imagine any other existence. Where I lived, what I did, how I thought about money, and the people I hung out with were all connected with my work identity. [...] I just wanted a different relationship with work.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;One signal that you might be heading towards other paths is this slight uncomfortable sensation about full-time work. Nothing big enough to make drastic decisions, only a sensation that something is off.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Choosing to leave full‑time work was not a single bold decision but a slow and steady awakening that the path I was on was not my path. Going pathless is not a single act of bravery, It’s more like having a pebble in your shoe. Where you’re walking and something is off, and it’s mildly uncomfortable. It wasn’t enough of a feeling to make me do anything dramatic, but it threw me off just enough that I was forced to pay attention to my life in a different way. I became frustrated with the snail’s pace (of corporate) and that’s when the pebble in my shoe feeling became too powerful to ignore.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Reflecting on yourself can start revealing this &lt;em&gt;&quot;pebble in your shoe&quot;&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;. These simple questions can help to start a process of reflection:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Are you building a life around the things you value?&lt;&#x2F;li&gt;
&lt;li&gt;Are you trading pay-checks for work you don&#x27;t value?&lt;&#x2F;li&gt;
&lt;li&gt;Are you sacrificing excitement for control and stability?&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Maybe you actually only need a job change to get rid of the uncomfortable sensation, but if the answer to the previous questions are 3 &quot;Yes&quot; it would be foolish not to contemplate other life options.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-1-burnout-in-the-default-path&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-1-burnout-in-the-default-path&quot; aria-label=&quot;Anchor link for: 1-1-burnout-in-the-default-path&quot;&gt;1.1 Burnout in the default path&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Living slightly uncomfortable for 8+ hours per day while pretending all is alright comes with a cost: burnout.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-pathless-path&#x2F;.&#x2F;images&#x2F;this_is_fine.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The most usual cause of burnout is the emotional exhaustion for job-related stress.&lt;&#x2F;p&gt;
&lt;p&gt;But its not the only one.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Burnout can happen when a person does a job that they doesn&#x27;t align with their identity for prolonged time&lt;&#x2F;strong&gt;. This kind of burnout is caused by &quot;loss of self-identity&quot;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Too Smart for Burnout I told myself I was smarter than other people. I knew what I was good at. I always took all of my vacation days. Didn’t work crazy hours. Made time for friends and family. Changed jobs when I stopped learning. I had done all these things with the idea that this was how I would not only avoid burning out but that I would thrive. I wanted to hack the system and make it work for me. On my final day of work the feelings that flowed through my body told me I wasn’t so clever.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;When you spend most of your energy time doing something that do not value, think is meaningless, or a waste of time, it is only natural to link yourself with that emptiness and feel loss.&lt;&#x2F;p&gt;
&lt;p&gt;The distance between what you do and yourself can be underwhelming a source of burnout.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Being successful means keeping clients, customers, and managers happy while fitting into a company’s cultural norms. Unfortunately, success for the company does not always align with what is best for the person, and over time, a disconnect can emerge. This is what happened to me.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;At that last job, I wasn’t a team player and I could have tried harder to say the right things, dress the right way, or spend more time pleasing my manager. But I couldn’t do it. The norms of the organisation were pulling me too far away from the person I wanted to be and the energy I used to manage this disconnect undermined anything good I had to offer.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h3 id=&quot;1-2-prestige-in-the-default-path&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-2-prestige-in-the-default-path&quot; aria-label=&quot;Anchor link for: 1-2-prestige-in-the-default-path&quot;&gt;1.2 Prestige in the default path&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Why would you do a job that you don&#x27;t enjoy?&lt;&#x2F;p&gt;
&lt;p&gt;To impress others.&lt;&#x2F;p&gt;
&lt;p&gt;A classic soul sucking corporate job provides both:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intrinsic prestige&lt;&#x2F;strong&gt;: the attention or respect you receive when you mention where you work or what you do.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Extrinsic prestige&lt;&#x2F;strong&gt;: the paycheck from the job that can buy shinny objects that other people want.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Prestige is &lt;em&gt;“a powerful magnet that warps even your beliefs about what you enjoy.&quot;&lt;&#x2F;em&gt; (Peter Thiel)&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;My friends all had impressive plans after graduation, and I didn’t want to be left behind. People were impressed by the job I was taking at GE, and I liked how the attention made me feel. I felt smart. It didn’t matter that I had never worked in finance and had never spent any time in Ohio or the Midwest, where I would be located. Out of all the jobs I could get from my school, this was one of the best and the magnet of prestige convinced me that was what I wanted.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;There is nothing wrong for wanting to be loved and get attention. It&#x27;s only human. What its wrong is trading 8 hours per day for it.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Don&#x27;t get caught the &lt;em&gt;prestige trap&lt;&#x2F;em&gt;.&lt;&#x2F;strong&gt; Job titles and &quot;great&quot; companies cannot make you liking your job tasks. It only provides recognition and praise to cope with them.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-the-pathless-path&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-the-pathless-path&quot; aria-label=&quot;Anchor link for: 2-the-pathless-path&quot;&gt;2. The pathless path&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;So, what its the pathless path?&lt;&#x2F;p&gt;
&lt;p&gt;The pathless path is an alternative to the default path that encourages you to find ways to center your life around work that you value instead of a nine-to-five job.&lt;&#x2F;p&gt;
&lt;p&gt;The pathless path is &quot;pathless&quot; in the sense that nobody will tell you what to do. It is uncertain by nature. It&#x27;s a path that you decide to take without knowing what you will find on it.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;On the pathless path, the goal is not to find a job, make money, build a business, or achieve any other metric. It’s to actively and consciously search for the work that you want to keep doing.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Instead of copy‑and‑paste what our parents or friends had done, you have to figure out what to offer to the world and start experimenting off-road.&lt;&#x2F;p&gt;
&lt;p&gt;A typical one or two-week vacation it is not enough to provide distance from your &quot;work-self&quot;, so you can expect a process that takes months.&lt;&#x2F;p&gt;
&lt;p&gt;There is a catch with this plan. Even if you decide to embrace uncertainty, going pathless cost money. Its only cool to have sabbatical in another continent to figure things out if you can pay for it.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-controlled-risks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-controlled-risks&quot; aria-label=&quot;Anchor link for: 3-controlled-risks&quot;&gt;3. Controlled risks&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Jumping into the pathless path requires financial preparation. At least a bit of cushion to pay the bills (or travel, test, experiment) while you explore without a stable income.&lt;&#x2F;p&gt;
&lt;p&gt;The trick is preparing financially (and else) as much as possible while you have a stable income. Don&#x27;t be reckless, simply eliminate financial risks as much as possible.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Focusing not on being brave, but instead on eliminating risk, is common for people who take unconventional paths.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For most people, life is not based on all‑or‑nothing leaps of faith. That’s a lie we tell ourselves so that we can remain comfortable in our current state. We simplify life transitions down to single moments because the real stories are more complex, harder to tell and attract less attention&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Preparing will not avoid facing uncertainty. It is unquestionable that you will, and not only financially. However, it gives you the most chances to succeed and not to muddy your vision of what you like to do for what you have to do.&lt;&#x2F;p&gt;
&lt;p&gt;The author spent years to make the switch from full-time to freelance&#x2F;writer, so he had plenty of time to progressively explore while saving.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;As I started to test my boundaries, I split into two different versions of myself. One, “Default Path Paul,” focused on continuing my career, looking for the next job. The other, “Pathless Path Paul” was finding his footing and starting to pay attention to the clues that were showing up.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;You will still need faith on yourself&lt;&#x2F;strong&gt;. It is easy to think that you need an obscene amount of financial security to make &quot;the pathless jump&quot;. Once you remove the superfluous expenses, the purchases we use as coping mechanisms and all other &quot;misery tax&quot; we need to carry on with our jobs - the amount can be surprisingly low.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you don&#x27;t value money, you can pair making less with working less.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Beyond financial insecurity, making this kind of life changes requires overcoming the discomfort of not knowing what will happen when you do.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;4-uncertainty-in-the-pathless-path&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-uncertainty-in-the-pathless-path&quot; aria-label=&quot;Anchor link for: 4-uncertainty-in-the-pathless-path&quot;&gt;4. Uncertainty in the pathless path&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;I had embraced a question that would shape my decisions: “How do you design a life that doesn’t put work first?” The answer, my dear reader, is simple. You start underachieving at work.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;While leaving your job is very daunting, you might discover that there is not that much to lose once you write down exactly what you&#x27;d lose. To check if this is true, consider writing down Tim Ferriss’ &lt;em&gt;“fear setting”&lt;&#x2F;em&gt; exercise. It has the following six steps:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Write down the change you are making.&lt;&#x2F;li&gt;
&lt;li&gt;List the worst possible outcomes.&lt;&#x2F;li&gt;
&lt;li&gt;Identify actions you could take to mitigate those actions.&lt;&#x2F;li&gt;
&lt;li&gt;List some steps or actions you might take to get back to where you are today.&lt;&#x2F;li&gt;
&lt;li&gt;What could be some benefits of an attempt or partial success?&lt;&#x2F;li&gt;
&lt;li&gt;What is the cost of inaction in three months, 12 months, and in a few years?&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Once you consider the risks, or simply decided that you had enough of your boss at your job, a world of free time can open to you. It is up to you how to spend this time and what problems you would like to solve.&lt;&#x2F;p&gt;
&lt;p&gt;The bad news is that you will have no idea what are you going to do next, neither you should. Otherwise it wouldn&#x27;t be pathless.&lt;&#x2F;p&gt;
&lt;p&gt;The good news is that removing yourself from your job release a bunch of energy to explore.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;y.yarn.co&#x2F;8f75d78b-2000-48d7-8b5c-9ef85e2792dc_text.gif&quot; alt=&quot;Parks and recs&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Creative work runs on uncertainty&lt;&#x2F;em&gt;. With the extra time and energy, think in blocks of one to three months and within each block pick one or two things you want to prioritize and test.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Beyond money, the second most common concern people have about working less or building a life less centered around work is what they will do with their time. On the default path, we may not realize how much energy it requires to simply go through the motions and stay on the path.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The main mission is getting a better understanding of what really makes your life better.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Instead of being consumed with thoughts about work and my next step, I had time to continue to experiment, and in the space that emerged, a creative energy entered which started to become a central force in my life&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;5-finding-meaningful-work&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-finding-meaningful-work&quot; aria-label=&quot;Anchor link for: 5-finding-meaningful-work&quot;&gt;5. Finding meaningful work&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;My biggest barrier was my inability to imagine an alternative life. My creative experiments were exciting, but they didn’t suggest an obvious next step.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;There are some tips in the book to guide you in the right direction on finding work that you like:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Experiment with different types of jobs and work.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Try to write down on some characteristics that define the person you want to be. You can turn the characteristics into a self-assessment and review each of the characteristics with your life. In other words, are you heading to the kind of person you want to be or your best self?&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;In business school, I created a vision of the kind of leader and person I wanted to be, and five years later I realized I was headed in the wrong direction&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Ask an old friend when they have seen you in your best self.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Start thinking about what you &lt;strong&gt;don&#x27;t&lt;&#x2F;strong&gt; like from the work you&#x27;ve done, and the kind of jobs that involve that. &lt;em&gt;What a miserable life entail?&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Once you know what you don&#x27;t like, work backwards to find the things you like or used to like doing.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Exploring activities that you like is an essential step of finding meaningful work. Out of those activities, some of them can become job opportunities if there is people out there willing to pay for them.&lt;&#x2F;p&gt;
&lt;p&gt;Remember, first comes the exploring phase, then the business phase - Otherwise you will come at risk of starting another job you dislike.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-pathless-path&#x2F;.&#x2F;images&#x2F;venn_diagram.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-1-practical-example&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-1-practical-example&quot; aria-label=&quot;Anchor link for: 5-1-practical-example&quot;&gt;5.1 Practical example&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;I have in mind a good example of &quot;Finding meaningful work&quot; that it is incredibly simple. His name is Kris, and the guy makes &lt;a href=&quot;https:&#x2F;&#x2F;kriscables.com&#x2F;&quot;&gt;keyboards and usb cables&lt;&#x2F;a&gt;. I have no affiliation (or even know him), I just like his products.&lt;&#x2F;p&gt;
&lt;p&gt;In &lt;a href=&quot;https:&#x2F;&#x2F;www.headphonesty.com&#x2F;2018&#x2F;10&#x2F;interview-kris-cables&#x2F;&quot;&gt;an interview&lt;&#x2F;a&gt; he tells his story about how he started his business:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The generic black cable wasn’t really a great fit for a custom mechanical keyboard that I was building. I tried searching for one but didn’t find anything appealing to my taste and wallet.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;I decided to buy materials and learn&#x2F;build a few. My friends really liked the work and bought from me some too. After sharing my work on the local forum, I was asked to build more. This is how Kris Cables was born.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;You can tell that he likes what he does by the way he talks about his craft in his Discord channel, or how he interacts with his costumers. Kris is good at it because he likes it.&lt;&#x2F;p&gt;
&lt;p&gt;He is also quite successful I assume, since he has literally cues of people waiting for what he does. If you want a product made by him you will have to wait a couple of months in the line:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-pathless-path&#x2F;.&#x2F;images&#x2F;kris.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I like his business story because it cannot get more simple. He liked usb cables and keyboards and tried to build some, that&#x27;s it. Then the business came with the opportunity that he saw in the market, but that was an extension of exploring topics he liked.&lt;&#x2F;p&gt;
&lt;p&gt;Kris is only an example of the hundred of thousands of people that found a way to make his craft his job.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The “real work of your life” is searching for the things you want to commit to and that make your life meaningful. Once you find them, you can dedicate your time to creating the environment to make those things happen.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Reflect, prepare, face uncertainty, and go have some fun.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>How to set up Hydrogen in Pulsar IDE</title>
        <published>2023-08-02T00:00:00+00:00</published>
        <updated>2023-08-02T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/posts/pulsar/"/>
        <id>https://pipegalera.github.io/posts/pulsar/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/posts/pulsar/">&lt;h2 id=&quot;i-missed-atom-broken-heart-tl-dr-skip-to-next-section&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#i-missed-atom-broken-heart-tl-dr-skip-to-next-section&quot; aria-label=&quot;Anchor link for: i-missed-atom-broken-heart-tl-dr-skip-to-next-section&quot;&gt;I missed Atom 💔 (&lt;strong&gt;tl;dr&lt;&#x2F;strong&gt; skip to next section.)&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;I switched my IDE to VSCode from Atom around 5 years ago for my usual data science related work. However, year after year I kept missing the Hydrogen IDE package for one reason or another. If you don&#x27;t know what I&#x27;m talking about, in essence Hydrogen is jupyter notebook&#x27; cells embedded in python files:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;pulsar&#x2F;.&#x2F;images&#x2F;hydrogen.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This was a game changer for me using Python for data science during my first years. As free GPU platforms based on notebooks were getting more traction (e.g. Google Colab, Kaggle, Amazon SageMaker, Databricks) I ended up rotating to VSCode and Jupyter notebooks. They really &lt;em&gt;hooked me&lt;&#x2F;em&gt; with free Tesla V100s GPUs.&lt;&#x2F;p&gt;
&lt;p&gt;Flash forward 2022 and after much testing with other tools, I wanted to come back to the good-old Atom+Hydrogen combo. Easy change I expected... oh well. Atom was closing doors and finally &lt;a href=&quot;https:&#x2F;&#x2F;github.blog&#x2F;2022-06-08-sunsetting-atom&quot;&gt;shut down&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Luckily, the open-source community is amazing, and started to develop an Atom fork called Pulsar.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;pulsar&#x2F;.&#x2F;images&#x2F;pulsar.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In all the honesty, at the early stages of Pulsar (Late 2022) the IDE was very buggy and unusable. Hydrogen wasn&#x27;t working either in my machine. Trying to find solutions was difficult as they weren&#x27;t precisely popping up in Stackoverflow or their Github repo. The project was basically brand new.&lt;&#x2F;p&gt;
&lt;p&gt;The developers kept improving Pulsar, fixing bugs, and the Atom community jumped on solving issues. By the time I&#x27;m writing this post (Q2 2023), I happily say that I moved completely to Pulsar as my IDE 🎉.&lt;&#x2F;p&gt;
&lt;p&gt;Everything that I use simply work as it used to in Atom. Setting up Hydrogen was more tricky than expected, but I made it work and the instructions are below.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-to-set-up-hydrogen-in-pulsar&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-set-up-hydrogen-in-pulsar&quot; aria-label=&quot;Anchor link for: how-to-set-up-hydrogen-in-pulsar&quot;&gt;How to set up Hydrogen in Pulsar&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The following instructions have been tested in Linux (Pop OS Distro) and macOS (Ventura 13.4).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-install-pulsar&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-install-pulsar&quot; aria-label=&quot;Anchor link for: 1-install-pulsar&quot;&gt;1. Install Pulsar&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Simply follow the &lt;a href=&quot;https:&#x2F;&#x2F;pulsar-edit.dev&#x2F;download.html&quot;&gt;official instructions&lt;&#x2F;a&gt; for your particular OS system. For Windows&#x2F;macos simply download the executable installer. For Linux, I downloaded the image (&lt;em&gt;pulsar_1.105.2023053023_amd64.deb&lt;&#x2F;em&gt;) and run:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; apt install .&#x2F;pulsar_1.105.2023052200_amd64.deb
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Make sure it&#x27;s installed by running &lt;code&gt;pulsar --version&lt;&#x2F;code&gt;. Something like the following should show:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;pulsar&#x2F;.&#x2F;images&#x2F;pulsar_version.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-install-hydrogen-and-hydrogen-plugin&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-install-hydrogen-and-hydrogen-plugin&quot; aria-label=&quot;Anchor link for: 2-install-hydrogen-and-hydrogen-plugin&quot;&gt;2. Install Hydrogen and Hydrogen plugin&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;⚠️ Do not install Hydrogen from the package manager within the Pulsar app.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Open any terminal and install Hydrogen by running:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;pulsar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -p&lt;&#x2F;span&gt;&lt;span&gt; install https:&#x2F;&#x2F;github.com&#x2F;nteract&#x2F;hydrogen&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -t&lt;&#x2F;span&gt;&lt;span&gt; v2.16.5
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pulsar-edit&#x2F;package-backend&#x2F;blob&#x2F;main&#x2F;docs&#x2F;reference&#x2F;Admin_Actions.md#hydrogen&quot;&gt;Source&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Install also 2 plugins for Hydrogen:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Hydrogen Launcher&lt;&#x2F;strong&gt;: enable opening terminals and Jupyter consoles connected to Hydrogen.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;pulsar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -p&lt;&#x2F;span&gt;&lt;span&gt; install hydrogen-launcher
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Hydrogen-python&lt;&#x2F;strong&gt;: provides various Python-specific features.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;pulsar&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -p&lt;&#x2F;span&gt;&lt;span&gt; install hydrogen-python
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;3-correct-the-electron-version&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-correct-the-electron-version&quot; aria-label=&quot;Anchor link for: 3-correct-the-electron-version&quot;&gt;3. Correct the Electron version&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Pulsar has moved to a newer version of Electron than what Atom was on, so those native modules (e.g. Hydrogen) needed to be rebuilt for the new version. Change directory to the Hydrogen folder (tip: the step before prints the Hydrogen folder path), and rebuild electron:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;cd&lt;&#x2F;span&gt;&lt;span&gt; &#x2F;home&#x2F;pipegalera&#x2F;.pulsar&#x2F;packages&#x2F;Hydrogen
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;npx&lt;&#x2F;span&gt;&lt;span&gt; electron-rebuild&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -v&lt;&#x2F;span&gt;&lt;span&gt; 12.2.3
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.reddit.com&#x2F;r&#x2F;pulsaredit&#x2F;comments&#x2F;119tby8&#x2F;hydrogen_package_fails_to_activate_any_tips&#x2F;&quot;&gt;Source&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-create-a-virtual-environment-and-test&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-create-a-virtual-environment-and-test&quot; aria-label=&quot;Anchor link for: 4-create-a-virtual-environment-and-test&quot;&gt;4. Create a virtual environment and test&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Creating a virtual environment is a good test to make sure Hydrogen is working correctly. We would like Hydrogen to load the packages from the specific environment - not from python base env.&lt;&#x2F;p&gt;
&lt;p&gt;We&#x27;ll create a virtual environment and then run an example. Feel free to use your virtual environment manager of your choice, I personally use &lt;a href=&quot;https:&#x2F;&#x2F;docs.conda.io&#x2F;en&#x2F;latest&#x2F;miniconda.html&quot;&gt;miniconda&lt;&#x2F;a&gt; or &lt;a href=&quot;https:&#x2F;&#x2F;mamba.readthedocs.io&#x2F;en&#x2F;latest&#x2F;user_guide&#x2F;mamba.html&quot;&gt;mamba&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;In Linux, miniconda can be easily installed by running the below:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;wget&lt;&#x2F;span&gt;&lt;span&gt; https:&#x2F;&#x2F;repo.anaconda.com&#x2F;miniconda&#x2F;Miniconda3-py37_4.11.0-Linux-x86_64.sh
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;bash&lt;&#x2F;span&gt;&lt;span&gt; Miniconda3-py37_4.11.0-Linux-x86_64.sh
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If you are reading this from Windows or macOS I would recommend that simply download the miniconda or anaconda installer &lt;a href=&quot;https:&#x2F;&#x2F;conda.io&#x2F;projects&#x2F;conda&#x2F;en&#x2F;latest&#x2F;user-guide&#x2F;install&#x2F;index.html&quot;&gt;from the official page&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Once you have it installed, we will create a basic data science environment with jupyter, ipykernel, scikit-learn, and matplotlib:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Needed packages for Hydrogen to work
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; create&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n&lt;&#x2F;span&gt;&lt;span&gt; ds python=3.9
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; activate ds
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -y&lt;&#x2F;span&gt;&lt;span&gt; ipykernel jupyter
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;python&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -m&lt;&#x2F;span&gt;&lt;span&gt; ipykernel install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; --user --name&lt;&#x2F;span&gt;&lt;span&gt; ds&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; --display-name &lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Data Science kernel&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Basic packages for testing purposes
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;pip&lt;&#x2F;span&gt;&lt;span&gt; install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -U&lt;&#x2F;span&gt;&lt;span&gt; scikit-learn
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -y&lt;&#x2F;span&gt;&lt;span&gt; matplotlib
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;nteract.io&#x2F;kernels&quot;&gt;Source&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Done! We have all the pieces to run Hydrogen by now.&lt;&#x2F;p&gt;
&lt;p&gt;To test that Hydrogren is working, create an empty &lt;em&gt;test.py&lt;&#x2F;em&gt; file and copy&amp;amp;paste a &lt;a href=&quot;https:&#x2F;&#x2F;scikit-learn.org&#x2F;stable&#x2F;auto_examples&#x2F;release_highlights&#x2F;plot_release_highlights_1_1_0.html#sphx-glr-auto-examples-release-highlights-plot-release-highlights-1-1-0-py&quot;&gt;random example code from sklearn&lt;&#x2F;a&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;pulsar&#x2F;.&#x2F;images&#x2F;example_sklearn.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Now, select the code that you want to run and press &lt;code&gt;Shift+Enter&lt;&#x2F;code&gt;. Hydrogen will launch and it will give you the option to select the kernel&#x2F;environment to load. In this case &lt;em&gt;&quot;Data Science kernel&quot;&lt;&#x2F;em&gt;, the one we just have created:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;pulsar&#x2F;.&#x2F;images&#x2F;example_sklearn_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The code should run and print the results:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;pulsar&#x2F;.&#x2F;images&#x2F;example_sklearn_3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;And that&#x27;s it. I might change IDE again to VSCode when Copilot takes over the world, but for now Pulsar+Hydrogen is my favorite data science IDE 😊&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Zero to One, by Peter Thiel</title>
        <published>2023-05-28T00:00:00+00:00</published>
        <updated>2023-05-28T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-zero-to-one/"/>
        <id>https://pipegalera.github.io/mostly_books/book-zero-to-one/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-zero-to-one/">&lt;p&gt;Below are all the quotes and ideas I considered worth it from the book, gathered into 7 central themes.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;31Dj0fQiqYL._SY445_SX342_.jpg&quot; alt=&quot;book cover&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-wrong-silicon-valley-dogmas&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-wrong-silicon-valley-dogmas&quot; aria-label=&quot;Anchor link for: 1-wrong-silicon-valley-dogmas&quot;&gt;1. Wrong Silicon Valley dogmas&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The entrepreneurs who stuck with Silicon Valley learned four big lessons from the dot-com crash that still guide business thinking today:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Make incremental advances&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Anyone who claims to be able to do something great is suspect, and anyone who wants to change the world should be more humble. Small incremental steps are the only safe path forward.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Stay lean and flexible&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;All companies must be lean, which is code for unplanned. Planning is arrogant and inflexible. Instead, you should try things out, iterate, and treat entrepreneurship as agnostic experimentation.&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Improve on the competition&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Build your company by improving on recognizable products already offered by successful competitors.&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Focus on product, not sales&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;If your product requires advertising or salespeople to sell it, it&#x27;s not good enough: technology is primarily about product development, not distribution.&lt;&#x2F;p&gt;
&lt;p&gt;These lessons have become dogma in the startup world, those who would ignore them are presumed to invite the justified doom visited upon technology in the great crash of 2000. And yet the opposite principles are probable more correct:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;It is better to risk boldness than triviality&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;A bad plan is better than no plan&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Competitive markets destroy profits&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Sales matters just as much as product&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;2-build-a-monopoly&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-build-a-monopoly&quot; aria-label=&quot;Anchor link for: 2-build-a-monopoly&quot;&gt;2. Build a monopoly&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Capitalism is premised on the accumulation of capital, but under perfect competition all profits get competed away. The lesson for entrepreneurs is clear: if you want to create and capture lasting value, don&#x27;t build an undifferentiated commodity business.&lt;&#x2F;p&gt;
&lt;p&gt;Monopolist can afford to think about things other than making money, non-monopolist can&#x27;t.&lt;&#x2F;p&gt;
&lt;p&gt;Monopolies can keep innovating because profits enable them to make the long-term plans and to finance the ambitious research projects that firms locked in competition can&#x27;t dream of.&lt;&#x2F;p&gt;
&lt;p&gt;Monopoly is therefore not a pathology or an exception. Monopoly is the condition of every successful business.&lt;&#x2F;p&gt;
&lt;p&gt;Every monopoly is unique, but they usually share some combination of the following characteristics: proprietary technology, network effects, economies of scale, and branding.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;proprietary-technology&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#proprietary-technology&quot; aria-label=&quot;Anchor link for: proprietary-technology&quot;&gt;Proprietary technology&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;As a good rule of thumb, proprietary technology must be at least 10 times better than its closest substitute in some important dimension to lead to a real monopolistic advantage. Anything less than an order of magnitude better will probably be perceived as a marginal improvement and will be hard to sell, especially in an already crowded market.&lt;&#x2F;p&gt;
&lt;p&gt;Apple improved on anything that had come before by at least an order of magnitude: tablets went from unusable to useful.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;network-effect&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#network-effect&quot; aria-label=&quot;Anchor link for: network-effect&quot;&gt;Network Effect&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Network effects make a product more useful as more people use it. Network effects can be powerful, but you&#x27;ll never reap them unless your product is valuable to its very first users when the network is necessarily small.&lt;&#x2F;p&gt;
&lt;p&gt;This is why successful network businesses rarely get started by MBA types: the initial markets are so small that they often don&#x27;t even appear to be business opportunities at all.&lt;&#x2F;p&gt;
&lt;p&gt;It is much easier to reach a few thousand people who really need your product than to try to compete for the attention of millions of scattered individuals. The perfect target market for a startup is a small group of particular people concentrated together and served by a few or no competitors. Any big market is a bad choice, and a big market already served by competing companies is even worse.&lt;&#x2F;p&gt;
&lt;p&gt;The most successful companies make the core profession to first dominate a specific niche and then scale to adjacent markets a part of their founding narrative. As you craft a plan to expand to adjacent markets, don&#x27;t disrupt - avoid competition as much as possible.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;economies-of-scale&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#economies-of-scale&quot; aria-label=&quot;Anchor link for: economies-of-scale&quot;&gt;Economies of Scale&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Software startups can enjoy especially dramatic economies of scale because the marginal cost of producing another copy of the product is close to zero.&lt;&#x2F;p&gt;
&lt;p&gt;A good startup should have the potential for great scale built into its first design.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;branding&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#branding&quot; aria-label=&quot;Anchor link for: branding&quot;&gt;Branding&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;A company has a monopoly on its own brand by definition, so creating a strong brand is powerful way to claim a monopoly. However, no technology company can be built on branding alone.&lt;&#x2F;p&gt;
&lt;p&gt;Many have tried to learn from Apple&#x27;s success: paid advertising, branded stores, luxurious materials, playful keynote speeches, high prices, and even minimalist design are all susceptible to imitation. But these techniques for polishing the surface don&#x27;t work without a strong underlying substance. Apple has a complex suite of proprietary technologies, bot in hardware (like superior touchscreen materials) and software (like touchscreen interfaces purpose-designed for specific materials). It manufactures products at a scale large enough to dominate pricing for the materials it buys.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-sales-and-distribution&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-sales-and-distribution&quot; aria-label=&quot;Anchor link for: 3-sales-and-distribution&quot;&gt;3. Sales and distribution&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Customers will not come just because you build it. You have to make that happen.&lt;&#x2F;p&gt;
&lt;p&gt;In Silicon Valley, nerds are skeptical of advertising, marketing, and sales because they seem superficial and irrational. But advertising matters because it works.&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s better to think of distribution as something essential to the design of your product. If you&#x27;ve invented something new but you haven&#x27;t invented an effective way to sell it, you have a bad business - no matter how good the product.&lt;&#x2F;p&gt;
&lt;p&gt;Superior sales and distribution by itself can create monopoly, even with no product differentiation. The converse is not true.&lt;&#x2F;p&gt;
&lt;p&gt;Two metrics set the limits for effective distribution. The total net profit that you earn on average over the course of your relationship with a customer (Customer Lifetime Value, or CLV) must exceed the amount you spend on average to acquire a nre customer (Customer Acquisition Cost, or CAC)&lt;&#x2F;p&gt;
&lt;p&gt;Marketing and advertising work for relatively low-priced products that have mass appeal but lack any method of viral distribution.&lt;&#x2F;p&gt;
&lt;p&gt;A product is viral if its core functionality encourages users to invite their friends to become users too. This is how Facebook and PayPal both grew quickly: every time someone shares with a friend or makes a payment, they naturally invite more and more people into the network.&lt;&#x2F;p&gt;
&lt;p&gt;At PayPal we didn&#x27;t want to acquire more users at random, we wanted to get the most valuable users first.&lt;&#x2F;p&gt;
&lt;p&gt;Most businesses get zero distribution channels to work: poor sales rather than bad product is the most common cause of failure. If you can get just one distribution channel to work, you have a great business. If you try for several but don&#x27;t nail one, you&#x27;re finished.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;4-secrets-and-niches&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-secrets-and-niches&quot; aria-label=&quot;Anchor link for: 4-secrets-and-niches&quot;&gt;4. Secrets and niches&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Great companies have secrets: specific reasons for success that other people don&#x27;t see.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The best projects are likely to be overlooked, not trumpeted by a crowd.&lt;&#x2F;li&gt;
&lt;li&gt;The best problems to work on are often the ones nobody else even tries to solve.&lt;&#x2F;li&gt;
&lt;li&gt;The single most powerful pattern I have noticed is that successful people find value in unexpected places.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;What valuable company is nobody building? This question is harder than it looks, because your company could create a lot of values without becoming very valuable itself. Creating value is not enough - you also need to capture some of the value your create. Your niche or secret has to be profitable.&lt;&#x2F;p&gt;
&lt;p&gt;Most people act as if there were no secrets left to find. If you think something hard is impossible, you&#x27;ll never even start trying to achieve it. Belief in secrets is an effective truth.&lt;&#x2F;p&gt;
&lt;p&gt;When thinking about what kind of company to build, there are tow distinct questions to ask: What secrets is nature not telling you? What secrets are people not telling you?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;5-startup-culture-companies&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-startup-culture-companies&quot; aria-label=&quot;Anchor link for: 5-startup-culture-companies&quot;&gt;5. Startup Culture: Companies&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;New technology tends to come from new ventures-startups. It&#x27;s hard to develop new things in big organizations and it&#x27;s even harder to do it by yourself, you need a team.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Moving first is a tactic, not a goal. What really matters is generating cash flows in the future, so being the first mover doesn&#x27;t do you any good if someone else comes along and unseats you. It&#x27;s much better to be the last mover - that is, to make the last great development in a specific market and enjoy years or even decades of monopoly profits.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&quot;Thiel&#x27;s Law&quot;: a startup messed up at its foundation cannot be fixed.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Most conflicts in a startup erupt between ownership and control - that is between founders and investors on the board.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Most fights inside a company happen when colleagues compete for the same responsibilities - Defined roles reduced conflict.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Whenever an entrepreneur asks me to invest in this company, I ask him how much he intends to pay himself. A company does better the less it pays the CEO - that&#x27;s the one of the single clearest patterns I&#x27;ve noticed from investing in hundreds of startups. In no case should a CEO of an early-stage, venture-backed startup receive more than 150k per year in salary.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&quot;Company culture&quot; doesn&#x27;t exist apart from the company itself: no company has a culture, every company is a culture.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Startups have limited resources and small teams. They must work quickly and efficiently in order to survive, and that&#x27;s easier to do when everyone shares an understanding of the world.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Recruiting is a core competency for any company. It should never be outsourced.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;6-startup-culture-people&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-startup-culture-people&quot; aria-label=&quot;Anchor link for: 6-startup-culture-people&quot;&gt;6. Startup Culture: People&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Asperger&#x27;s-like social ineptitude seem to be at an advantage in Silicon Valley today. If you&#x27;re less sensitive to social cues, you&#x27;re less likely to do the same things as everyone else around you.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Heroes take their personal honor so seriously they will fight for things that don&#x27;t matter. This twisted logic is part of human nature, but it&#x27;s disastrous in business.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Search for definite people. A definite view favors firm convictions. Instead of pursuing many-sided mediocrity and calling it &quot;well-roundness&quot;, a definite person determines the one best thing to do and then does it.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Unless you have perfectly conventional beliefs, it&#x27;s rarely a good idea to tell everybody everything that you know. So who do you tell? Whoever you need to, and no more.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Anyone who prefers owning a part of your company to being paid in cash reveals a preference for the long term and a commitment to increasing your company&#x27;s value in the future.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Founders should share a prehistory before they start a company together - otherwise they&#x27;re just rolling dice.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;A board of three is ideal. You board should never exceed five people, unless your company is publicly held.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;We didn&#x27;t assemble a mafia by sorting though resumes and simply hiring the most talented people. I had seen the mixed results of that approach firsthand when I worked at a New York lay firm. The lawyers I worked with ran a valuable business, and they were impressive individuals one by one. But the relationships between them were oddly thin. They spent all day together but few of them seemed to have much to say to each other outside the office. Why work with a group of people wo don;t even like each other? Many seem to think it&#x27;s a sacrifice necessary for making money.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Job assignments aren&#x27;t just about the relationship&#x27;s between the owners and tasks, they are also about relationship&#x27;s between employees. The best thing I did as a manager at Paypal was to make every person in the company responsible for doing just one thing. Every employee&#x27;s one thing was unique, and everyone knew I would evaluate him only on that one thing.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Men and machines are good at fundamentally different things. People have intentionality , we form plans and make decisions in complicated situations. We&#x27;re less good at making sense of enormous amounts of data. Computers are exactly the opposite: they excel at efficient data processing, but they struggle to make basic judgments that would be simple for any human.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Why do so many people miss the power of complementarity? It starts in school. Software engineers tend to work on projects that replace human efforts because that&#x27;s they&#x27;re trained to do.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;7-seven-questions-that-every-business-must-answer&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-seven-questions-that-every-business-must-answer&quot; aria-label=&quot;Anchor link for: 7-seven-questions-that-every-business-must-answer&quot;&gt;7. Seven questions that every business must answer&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-the-engineering-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-the-engineering-question&quot; aria-label=&quot;Anchor link for: 1-the-engineering-question&quot;&gt;1. The Engineering Question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Can you create breakthrough technology instead of incremental improvements?&lt;&#x2F;p&gt;
&lt;p&gt;Companies must strive for 10x better because merely incremental improvements often end up meaning no improvement at all for the end user.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-the-timing-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-the-timing-question&quot; aria-label=&quot;Anchor link for: 2-the-timing-question&quot;&gt;2. The Timing Question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Is now the right time to start your particular business? You can&#x27;t dominate a sub-market if it&#x27;s fictional. Are the cultural, societal, and political contexts right for the product?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-the-monopoly-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-the-monopoly-question&quot; aria-label=&quot;Anchor link for: 3-the-monopoly-question&quot;&gt;3. The Monopoly Question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Are you starting with a big share of a small market? Customers won&#x27;t care about any particular technology unless it solvers a particular problem in a superior way.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-the-people-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-the-people-question&quot; aria-label=&quot;Anchor link for: 4-the-people-question&quot;&gt;4. The People Question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Do you have the right team?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-the-distribution-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-the-distribution-question&quot; aria-label=&quot;Anchor link for: 5-the-distribution-question&quot;&gt;5. The Distribution Question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Do you have a way to not just create but deliver your product? The product needs to reach the final consumer, otherwise you don&#x27;t have a product at all.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-the-durability-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-the-durability-question&quot; aria-label=&quot;Anchor link for: 6-the-durability-question&quot;&gt;6. The Durability question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Will your market position be defensible 10 and 20 years into the future? Rephrase the durability question and ask: what will stop China from wiping out my business?&lt;&#x2F;p&gt;
&lt;p&gt;If you focus on near-term growth above all else, you miss the most important question you should be asking: will this business still be around a decade from now? Numbers alone won&#x27;t tell you the answer, instead you must think critically about the qualitative characteristics of your business.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-the-secret-question&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-the-secret-question&quot; aria-label=&quot;Anchor link for: 7-the-secret-question&quot;&gt;7. The Secret Question&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Have you identified a unique opportunity that others don&#x27;t see?&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>The Dip, by Seth Godin</title>
        <published>2023-04-23T00:00:00+00:00</published>
        <updated>2023-04-23T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-the-dip/"/>
        <id>https://pipegalera.github.io/mostly_books/book-the-dip/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-the-dip/">&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;41DpNe1ihmL._SY445_SX342_.jpg&quot; alt=&quot;book cover&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;what-s-the-dip&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#what-s-the-dip&quot; aria-label=&quot;Anchor link for: what-s-the-dip&quot;&gt;What&#x27;s The Dip ?&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;The Dip is the long slog between starting and mastery. A long slog that’s actually a shortcut, because it gets you where you want to go faster than any other path.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;This period is characterized by frustration, hard work, and the need to push through obstacles in order to achieve success. Not everyone has the perseverance and determination needed to go through the dip. As most people quit before they reach the other side of the dip, the scarcity of people that make it makes them very valuable.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-the-dip&#x2F;.&#x2F;images&#x2F;Image_1.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Whatever you do for a living, or for fun, it’s probably somehow based on a system that’s based on quitting. Quitting creates scarcity; scarcity creates value.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;But facing a dip doesn&#x27;t mean that its always worth it to do through it - not all dips are created equal. Some dips are temporary, and pushing through them can lead to great rewards. Others are dead-ends, and it is better to quit and focus on something else.&lt;&#x2F;p&gt;
&lt;p&gt;Godin argues that quitting can be a strategic move in certain situations, and that knowing when to quit and when to push through &quot;the dip&quot; is essential for achieving success.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Winners quit all the time. They just quit the right stuff at the right time.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;how-to-know-what-dip-is-worth-it&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-know-what-dip-is-worth-it&quot; aria-label=&quot;Anchor link for: how-to-know-what-dip-is-worth-it&quot;&gt;How to know what dip is worth it ?&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Believe it or not, quitting is often a great strategy, a smart way to manage your life and your career. Sometimes, though, quitting is exactly the wrong thing to do. It turns out that there’s a pretty simple way to tell the difference.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;When you are in the dip, there are only two situations that you will face: either it gets better or it doesn&#x27;t. You should quit if it doesn&#x27;t get better - that&#x27;s it.&lt;&#x2F;p&gt;
&lt;p&gt;Seth Godin define most of the situations that doesn&#x27;t get better as &lt;em&gt;The Cul-de-Sac&lt;&#x2F;em&gt;:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Cul-de-Sac (French for “dead end”) is so simple it doesn’t even need a chart. It’s a situation where you work and you work and you work and nothing much changes. It doesn’t get a lot better, it doesn’t get a lot worse. It just is.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-the-dip&#x2F;.&#x2F;images&#x2F;Image_2.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;There’s no reason to keep investing in something that is not going to get better.&lt;em&gt;The Cul-de-Sac&lt;&#x2F;em&gt; are all of the relations that do not go anywhere, the business that are leading nowhere, or those jobs that are purposeless.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;That’s why they call those jobs dead-end jobs. There’s not a lot to say about the Cul-de-Sac except to realize that it exists and to embrace the fact that when you find one, you need to get off it, fast. That’s because a dead end is keeping you from doing something else. The opportunity cost of investing your life in something that’s not going to get better is just too high.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;You should quit if you’re on a dead-end path. You should quit if the project you’re working on has a Dip that isn’t worth the reward at the end. Quitting the projects that don’t go anywhere is essential if you want to stick out the right ones.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;He recommends to quit as fast as you recognize you are in a &lt;em&gt;Cul-de-Sac&lt;&#x2F;em&gt;. The biggest obstacle to success in life is our inability to quit these situations soon enough.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Quitting a job is not quitting your quest to make a living or a difference or an impact. Quitting a job doesn’t have to mean giving up. A job is just a tactic, a way to get to what you really want. As soon as your job hits a dead end, it makes sense to quit and take your quest to a bigger marketplace—because every day you wait puts your goal further away.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;beating-the-dip&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#beating-the-dip&quot; aria-label=&quot;Anchor link for: beating-the-dip&quot;&gt;Beating the Dip&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The Dip is hard, either when it&#x27;s worth it and when it&#x27;s not. But once you know in which kind of Dip, the author recommends to embrace it - as almost everything in life worth doing it has a Dip.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Successful people don’t just ride out the Dip. They don’t just buckle down and survive it. No, they lean into the Dip. They push harder, changing the rules as they go. Just because you know you’re in the Dip doesn’t mean you have to live happily with it. Dips don’t last quite as long when you whittle at them.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;By this point, either you know if you should quite (&lt;em&gt;Cul-de-Sac&lt;&#x2F;em&gt;) or keep going.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The challenge is simple: Quitting when you hit the Dip is a bad idea. If the journey you started was worth doing, then quitting when you hit the Dip just wastes the time you’ve already invested.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;One tip he mentions is reminding the purpose of the Dip.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Short-term pain has more impact on most people than long-term benefits do, which is why it’s so important for you to amplify the long-term benefits of not quitting. You need to remind yourself of life at the other end of the Dip&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h2 id=&quot;after-the-dip-being-the-best-in-the-world&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#after-the-dip-being-the-best-in-the-world&quot; aria-label=&quot;Anchor link for: after-the-dip-being-the-best-in-the-world&quot;&gt;After the Dip, being the Best in the World&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Alright, the right (not &lt;em&gt;Cul-de-Sac&lt;&#x2F;em&gt; ) Dip is worth it so you must keep going...but what it makes it worth it?&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The people who set out to make it through the Dip—the people who invest the time and the energy and the effort to power through the Dip—those are the ones who become the best in the world. They are breaking the system because, instead of moving on to the next thing, instead of doing slightly above average and settling for what they’ve got, they embrace the challenge.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Basically being &lt;em&gt;the Best in the World&lt;&#x2F;em&gt;. My first reaction reading this was that it&#x27;s basically BS. You do not need to be the best in the world to succeed, and trying to get into seems like a rat race. However... it makes sense once you add some constraints.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;The Best in the World? Anyone who is going to hire you, buy from you, recommend you, vote for you, or do what you want them to do is going to wonder if you’re the best choice. Best as in: best for them, right now, based on what they believe and what they know. And in the world as in: their world, the world they have access to. So if I’m looking for a freelance copy editor, I want the best copy editor in English, who’s available, who can find a way to work with me at a price I can afford. That’s my best in the world.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-the-dip&#x2F;.&#x2F;images&#x2F;Image_3.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;tips-on-quitting&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#tips-on-quitting&quot; aria-label=&quot;Anchor link for: tips-on-quitting&quot;&gt;Tips on quitting&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;Say no, quit before even staring.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;If you’re going to quit, quit before you start. Reject the system. Don’t play the game if you realize you can’t be the best in the world. [...] Please understand this: If you’re not able to get through the Dip in an exceptional way, you must quit. And quit right now. Because if your order book is 80 percent filled with prospects where you just sort of show up, you’re not only wasting your time, you’re also stealing your energy from the 20 percent of the calls where you have a chance to create a breakthrough.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Don&#x27;t get into the habit of switching lines.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;He never gets anywhere because he’s always switching lines, never able to really run for it. While starting up is thrilling, it’s not until you get through the Dip that your efforts pay off. Countless entrepreneurs have perfected the starting part, but give up long before they finish paying their dues. The sad news is that when you start over, you get very little credit for how long you stood in line with your last great venture.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;If you are going to quit &lt;em&gt;something&lt;&#x2F;em&gt;, you have nothing to lose. Try everything in that situation to change it from &lt;em&gt;Cul-de-Sac&lt;&#x2F;em&gt; into a Dip&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;When the pain gets so bad that you’re ready to quit, you’ve set yourself up as someone with nothing to lose. And someone with nothing to lose has quite a bit of power. You can go for broke. Challenge authority. Attempt unattempted alternatives. Lean into a problem; lean so far that you might just lean right through it.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-the-dip&#x2F;.&#x2F;images&#x2F;Image_4.jpeg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;
&lt;p&gt;The time to look for a new job is when you don’t need one.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Quitting as a short-term strategy is a bad idea. Quitting for the long term is an excellent idea.
a&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Plan &lt;em&gt;when&lt;&#x2F;em&gt; to quit.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;When should I quit? I need to decide now, not when I’m in the middle of it, and not when part of me is begging to quit. [...] Decide in advance when you’re going to quit. You can always quit later—so wait until you’re done panicking to decide. When the pressure is greatest to compromise, to drop out, or to settle, your desire to quit should be at its lowest.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I found this tip more insightful than it might seem. Deciding beforehand when to quite ensures that:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;You will make a rational decision, and not based on the emotions of the moment.&lt;&#x2F;li&gt;
&lt;li&gt;Embrace better the hard moments, as you have a marked day that you will quit.&lt;&#x2F;li&gt;
&lt;li&gt;Avoid any rationalization, or act of trying to convince yourself that the &lt;em&gt;Cul-de-Sac&lt;&#x2F;em&gt; is a Dip. Once you have done the worse part, it is difficult to quit. Even if the reward isn&#x27;t worth it.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;two-questions-to-answer-before-quitting&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#two-questions-to-answer-before-quitting&quot; aria-label=&quot;Anchor link for: two-questions-to-answer-before-quitting&quot;&gt;Two questions to answer before quitting&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;There are actually three, but only these two made sense for me.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Am I panicky?&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;Never quit something with great long-term potential just because you can’t deal with the stress of the moment.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Do not quit because its hard. Quit because the effort is leading nowhere.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;What measurable progress am I making ?&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;blockquote&gt;
&lt;p&gt;If you’re trying to succeed in a job or a relationship or at a task, you’re either moving forward, falling behind, or standing still. There are only three choices.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Moving forward involves tangible progress, something that you clearly can point out that it is improving and getting better.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Dockerizing a Python application</title>
        <published>2023-01-09T00:00:00+00:00</published>
        <updated>2023-01-09T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/posts/dockerizing-python-application/"/>
        <id>https://pipegalera.github.io/posts/dockerizing-python-application/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/posts/dockerizing-python-application/">&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;docker.webp&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In very simple terms, Docker makes sure than an API, a Dashboard in Streamlit&#x2F;Shiny, or a scheduled script can be reproducible.&lt;&#x2F;p&gt;
&lt;p&gt;It is a way to package your code - making it able to run it other hardware and systems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;servers-and-virtual-machines&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#servers-and-virtual-machines&quot; aria-label=&quot;Anchor link for: servers-and-virtual-machines&quot;&gt;Servers and Virtual machines&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The goal of Docker is deploy code that you have had written somewhere else. This &quot;somewhere else&quot; is usually a server. In short, a server its a computer running in other place. Its always on running a Windows&#x2F;Linux machine, automatically managed by the provider and usually have more resources than your local computer (e.g. 128 RAM&#x2F;GPU Nvidia T4s) or the ability to scale if the workload requires it.&lt;&#x2F;p&gt;
&lt;p&gt;Instead of having 1 server running 1 operating system, computers can emulate more than one machine. Imagine a computer having multiple partitions of Linux, Windows, with different version, programs, and capacities. A server can have hundreds of versions or &quot;virtual machines&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;virtual_machine.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Imagine the &quot;cloud&quot; as different gigantic rooms somewhere around the world, with server racks with thousands of computers, running even more hundred of thousands virtual machines. The famous cloud providers (Google Cloud, AWS or Azure VMs) are simply renting you a small space in this gigantic room to store and run your code and software.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;server_center.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Now that we tour from computer, to server, to virtual machines, to cloud providers... How this relates to Docker?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;basic-docker&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#basic-docker&quot; aria-label=&quot;Anchor link for: basic-docker&quot;&gt;Basic Docker&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The Docker program makes sure that you can run your code&#x2F;app&#x2F;dashboard in a virtual machine in the cloud, without need of your local configurations, with everyone having access to the same configuration.&lt;&#x2F;p&gt;
&lt;p&gt;It compartmentalize your code and requirements in chucks. These &quot;chunks&quot; or containers keep everything its needed for the code to be run.&lt;&#x2F;p&gt;
&lt;p&gt;The basic structure of Docker is:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Docker containers&lt;&#x2F;strong&gt;: these independent chunks with everything needed to run your code. From the OS, to packages, to the Dashboard or app. Everything is there, organized, and ready to be run.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Docker images&lt;&#x2F;strong&gt;: a container is run based on images layered on top of each other. One image is the OS (e.g. Windows), another Python, another the packages (e.g. Streamlit, sklearn).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Docker files&lt;&#x2F;strong&gt;: A blueprint for the images. The files that specify how the images must be layered and build. For example, you want the OS to be built before the image of Python, and Python to be built before installing sklearn.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;docker-progression.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In summary, you use dockerfiles to build images, that are the parts or the final container that can be deployed.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;practial-example-dockerizing-a-python-script&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#practial-example-dockerizing-a-python-script&quot; aria-label=&quot;Anchor link for: practial-example-dockerizing-a-python-script&quot;&gt;Practial example dockerizing a Python script&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Dockerizing a script allows to deploy it. Docker handles the packing, deploying, and running of applications.&lt;&#x2F;p&gt;
&lt;p&gt;This is an example on how to dockerize a basic python script that show the first 5 rows of the iris dataset. The output is basic, but it can be easily a Dashboard, a statistical analysis that needs to be replicable for auditing, or a trained pickle output of a model + API. The important here is not what to deploy but how to do it.&lt;&#x2F;p&gt;
&lt;p&gt;First, make sure that you have installed correctly installed Docker for your machine following the &lt;a href=&quot;https:&#x2F;&#x2F;docs.docker.com&#x2F;desktop&#x2F;install&#x2F;&quot;&gt;Docker documentation&lt;&#x2F;a&gt; .&lt;&#x2F;p&gt;
&lt;p&gt;The instructions of the container must be written in the dockerfile. Here we build a basic container by writing the following instructions or Dockerfile:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;dockerfile.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Below I&#x27;ll explain the meaning of the different commands.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Every new space in between commands (&lt;code&gt;FROM&lt;&#x2F;code&gt;, &lt;code&gt;COPY&lt;&#x2F;code&gt;, &lt;code&gt;RUN&lt;&#x2F;code&gt;...) is an instruction to create a new image on top of the starting image of python 3.8.&lt;&#x2F;li&gt;
&lt;li&gt;In &lt;code&gt;FROM python:3.8&lt;&#x2F;code&gt; we say to Docker to build an staring image with python 3.8.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This is one of the pre-installed images that Docker has. There are more pre-installed images with everything you might need, from slim operating systems to full-packed data science environments. For us, we only need a basic python 3.8 environment image.&lt;&#x2F;p&gt;
&lt;p&gt;In &lt;code&gt;COPY requirements.txt .&#x2F;&lt;&#x2F;code&gt; we tell Docker to copy the requirements. This requirements are the list of packages that we need for the script. In this case only &lt;code&gt;seaborn&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;requirements.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;We install them running &lt;code&gt;RUN pip install --no-cache-dir -r requirements.txt&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;COPY python_script.py &#x2F;&lt;&#x2F;code&gt; copy the script the code that we want to run into the docker container. The script is just displaying the first 5 rows of the iris dataset:&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;python_script.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The last line of the Dockerfile, &lt;code&gt;ENTRYPOINT&lt;&#x2F;code&gt; is the final image layered that say to Docker what to do when someone access this Docker image: run python, and the python script.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Done! We have al the instructions in our &lt;code&gt;Dockfile&lt;&#x2F;code&gt; to be able to build &lt;code&gt;python_script.py&lt;&#x2F;code&gt; into a container.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;local-deployment&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#local-deployment&quot; aria-label=&quot;Anchor link for: local-deployment&quot;&gt;Local deployment&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Let&#x27;s build a docker and call or tag it &lt;code&gt;simple_python_app&lt;&#x2F;code&gt; by a command line:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;docker build -t simple_python_app .&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;build.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;It should create the docker quite fast and without errors if Docker is installed correctly in your machine.&lt;&#x2F;p&gt;
&lt;p&gt;Checking if the image is created can be done by running: &lt;code&gt;docker image list&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;list.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Once we know is there, we can run the Docker image by &lt;code&gt;docker run&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;posts&#x2F;dockerizing-python-application&#x2F;.&#x2F;images&#x2F;iris_head.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Et voilà, the docker run the code, compartmentalized.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;uses&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#uses&quot; aria-label=&quot;Anchor link for: uses&quot;&gt;Uses&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Why should I care or how can I use this in DS?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Bundling a DS app into production&lt;&#x2F;strong&gt; (e.g. Streamlit&#x2F;Shiny Dashboard). The script, requirements and dockerfile can be send to the engineering team in your organization and can it be hosted in the company server.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;&#x2F;strong&gt;. Let&#x27;s say that your code is a gigantic neural network that your local computers cannot handle. You can train the NN in a virtual machine with an expensive GPU, instead of having to buy one.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Parallelize&lt;&#x2F;strong&gt;. You can build an API that receives a lot of requests (an predictive ML system for example) so you can create many many many containers with the same image, and users can access different containers.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>One week electricity forecast in California</title>
        <published>2023-01-09T00:00:00+00:00</published>
        <updated>2023-01-09T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/projects/ml-weather-project/"/>
        <id>https://pipegalera.github.io/projects/ml-weather-project/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/projects/ml-weather-project/">&lt;h1 id=&quot;one-week-electricity-forecast-in-california&quot;&gt;One week electricity forecast in California&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;img src=&quot;&#x2F;projects&#x2F;weather_forecast_cover.png&quot; alt=&quot;cover image&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;This dashboard shows the latest data on electricity demand for the main 4 primary electric utility companies in California:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Pacific Gas and Electric (PGAE)&lt;&#x2F;li&gt;
&lt;li&gt;Southern California Edison (SCE)&lt;&#x2F;li&gt;
&lt;li&gt;San Diego Gas and Electric (SDGE)&lt;&#x2F;li&gt;
&lt;li&gt;Valley Electric Association (VEA)&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;energy_forecasting&#x2F;&quot;&gt;Live Dashboard&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;pipegalera&#x2F;energy_forecasting&quot;&gt;Github repository&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;code&gt;data&lt;&#x2F;code&gt; (US hourly demand for electricity) comes from EIA API. The predictions are made with &lt;code&gt;XGBoost&lt;&#x2F;code&gt; trained via &lt;code&gt;Optuna&lt;&#x2F;code&gt; for hypertunning. I tracked experiments and select the best models via &lt;code&gt;MLflow&lt;&#x2F;code&gt;.
The visualization is made with &lt;code&gt;plotly&lt;&#x2F;code&gt; package.&lt;&#x2F;p&gt;
&lt;p&gt;The data, forecasting, and visualization is refreshed daily using a &lt;code&gt;Docker&lt;&#x2F;code&gt; image run via &lt;code&gt;Github Actions&lt;&#x2F;code&gt; and deployed in a &lt;code&gt;Github page&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Tidy and Declutter Jupyter Notebooks</title>
        <published>2022-01-15T00:00:00+00:00</published>
        <updated>2022-01-15T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/posts/tidy-and-declutter-jupyter-notebooks/"/>
        <id>https://pipegalera.github.io/posts/tidy-and-declutter-jupyter-notebooks/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/posts/tidy-and-declutter-jupyter-notebooks/">&lt;p&gt;As a general rule, I create a new conda&#x2F;mamba environment for every new data science project. For simplicity and to avoid compatibility issues. Every 3 months I used to end up with 12 different environments and ipykernels.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.stack.imgur.com&#x2F;99fyH.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This post contains how to create, remove, and remove environments and kernels for tidiness.&lt;&#x2F;p&gt;
&lt;p&gt;A first step to declutter is taking a look of what you already have first.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-to-see-all-the-environments-and-kernels-installed&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-see-all-the-environments-and-kernels-installed&quot; aria-label=&quot;Anchor link for: how-to-see-all-the-environments-and-kernels-installed&quot;&gt;How to see all the environments and kernels installed?&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Environments:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; env list
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Also locally at:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Miniconda: C:&#x2F;Users&#x2F;{&lt;em&gt;windows_username_here&lt;&#x2F;em&gt;}&#x2F;miniconda3&#x2F;envs&lt;&#x2F;li&gt;
&lt;li&gt;Anaconda: C:&#x2F;Users&#x2F;{&lt;em&gt;windows_username_here&lt;&#x2F;em&gt;}&#x2F;.conda&#x2F;envs&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Kernels:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;jupyter&lt;&#x2F;span&gt;&lt;span&gt; kernelspec list
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Also locally at:&lt;&#x2F;p&gt;
&lt;p&gt;C:&#x2F;Users&#x2F;{&lt;em&gt;windows_username_here&lt;&#x2F;em&gt;}&#x2F;AppData&#x2F;Roaming&#x2F;jupyter&#x2F;kernels&#x2F;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-do-i-create-jupyter-project-from-zero&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-do-i-create-jupyter-project-from-zero&quot; aria-label=&quot;Anchor link for: how-do-i-create-jupyter-project-from-zero&quot;&gt;How do I create jupyter project from zero?&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Environment:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; create&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;enviroment_name_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span&gt; python=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;X.X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Activate it
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; activate &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;enviroment_name_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;em&gt;Optional&lt;&#x2F;em&gt; -&amp;gt; consider creating a -f &lt;code&gt;requirements.txt&lt;&#x2F;code&gt; file for best practices.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Kernel:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install ipykernel jupyter
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;python&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -m&lt;&#x2F;span&gt;&lt;span&gt; ipykernel install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; --user --name &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;kernel_name_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Your IDE will say something similar to:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;Installed kernelspec {kernel_name_here} in
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;C:&#x2F;Users&#x2F;{windows_username_here&#x2F;AppData&#x2F;Roaming&#x2F;jupyter&#x2F;kernels&#x2F;{kernel_name_here}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;how-to-remove-them&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-remove-them&quot; aria-label=&quot;Anchor link for: how-to-remove-them&quot;&gt;How to remove them?&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Environments:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; env remove&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;environment_name_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Kernels:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;jupyter&lt;&#x2F;span&gt;&lt;span&gt; kernelspec uninstall &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;kernel_name_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;how-to-rename-them&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#how-to-rename-them&quot; aria-label=&quot;Anchor link for: how-to-rename-them&quot;&gt;How to rename them?&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Environments:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;You can&#x27;t. One workaround is to create clone a new environment and then remove the original one (&lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;42231764&#x2F;how-can-i-rename-a-conda-environment&quot;&gt;source&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; env create&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n&lt;&#x2F;span&gt;&lt;span&gt; new_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; --copy --clone &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;old_nama_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; remove&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;old_nama_here&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Kernels:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The display name for a kernel is found in the &lt;code&gt;kernel.json&lt;&#x2F;code&gt; file in the corresponding directory for the kernel. Edit the &lt;code&gt;display_name&lt;&#x2F;code&gt; property in the kernel.json file and it will change the display name next time you start Jupyter (&lt;a href=&quot;https:&#x2F;&#x2F;stackoverflow.com&#x2F;questions&#x2F;45085233&#x2F;jupyter-kernel-is-there-a-way-to-rename-them&quot;&gt;source&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.imgur.com&#x2F;U8arU29.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Fastai - Deep Learning for Coders, by Jeremy Howard</title>
        <published>2022-01-01T00:00:00+00:00</published>
        <updated>2022-01-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-fastai/"/>
        <id>https://pipegalera.github.io/mostly_books/book-fastai/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-fastai/">&lt;p&gt;My personal notes of the Fastbook&#x2F;Fastai course &lt;a href=&quot;https:&#x2F;&#x2F;fastai.github.io&#x2F;fastbook2e&#x2F;&quot;&gt;Practical Deep Learning for Coders&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;516YvsJCS9L._SY445_SX342_.jpg&quot; alt=&quot;book cover&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;chapters-1-to-4-introduction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#chapters-1-to-4-introduction&quot; aria-label=&quot;Anchor link for: chapters-1-to-4-introduction&quot;&gt;Chapters 1 to 4 - Introduction&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Fastbook is a book is focused on the practical side of deep learning. It starts with the big picture, such as definitions and general applications of deep learning, and progressively digs beneath the surface into concrete examples.&lt;&#x2F;p&gt;
&lt;p&gt;The book is based on &lt;em&gt;fastai&lt;&#x2F;em&gt; API, an API on top of Pytorch that makes it easier to use state-of-the-art methods in deep learning. It doesn&#x27;t need you to understand models such as Convolutional Neural Networks and how they work, but it definitely have helped me following the book.&lt;&#x2F;p&gt;
&lt;p&gt;The fastbook package includes fastai and several easy-access datasets to test the models.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-1-install-fastai-api&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-1-install-fastai-api&quot; aria-label=&quot;Anchor link for: 1-1-install-fastai-api&quot;&gt;1.1 Install fastai API&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Installing fastai in Segamaker Studio Lab:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Initiate a Terminal and create a conda environment:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; create&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n&lt;&#x2F;span&gt;&lt;span&gt; fastai python=3.8
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; activate fastai
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Install Pytorch and Fastai:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Pytorch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install pytorch torchvision torchaudio
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Fastai
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -c&lt;&#x2F;span&gt;&lt;span&gt; fastchan fastai
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Import fastai:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;fastbook
&lt;&#x2F;span&gt;&lt;span&gt;fastbook.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;setup_book&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.vision.all &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.vision &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Installing fastai locally&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Please notice that unless you have a really powerful GPU (e.g. Nvidia 3080+) you won&#x27;t get the same training times than training the models in Google Colab or Amazon Segamaker.&lt;&#x2F;p&gt;
&lt;p&gt;The instructions are very similar, you only have to take care of the CUDA Toolkit first.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;developer.nvidia.com&#x2F;cuda-11.3.0-download-archive?target_os=Windows&amp;amp;target_arch=x86_64&amp;amp;target_version=10&quot;&gt;Install CUDA Toolkit 11.3&lt;&#x2F;a&gt; . Follow the link and install.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Create a new clean Python environment using &lt;a href=&quot;https:&#x2F;&#x2F;docs.conda.io&#x2F;en&#x2F;latest&#x2F;miniconda.html&quot;&gt;miniconda&lt;&#x2F;a&gt; :&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; create&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -n&lt;&#x2F;span&gt;&lt;span&gt; fastai python=3.8
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; activate fastai
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Install Pytorch and Fastai:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Pytorch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install pytorch torchvision torchaudio cudatoolkit=11.3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -c&lt;&#x2F;span&gt;&lt;span&gt; pytorch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Fastai
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;conda&lt;&#x2F;span&gt;&lt;span&gt; install&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt; -c&lt;&#x2F;span&gt;&lt;span&gt; fastchan fastai
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Test Pytorch and Fastai:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;If Pytorch was successfully installed you should see you GPU name by running:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;torch
&lt;&#x2F;span&gt;&lt;span&gt;x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;torch.cuda.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_device_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;torch.cuda.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;is_available&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;None
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;If Fastai was successfully installed you should load fastai without any error:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;fastbook
&lt;&#x2F;span&gt;&lt;span&gt;fastbook.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;setup_book&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.vision.all &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.vision &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;1-2-machine-learning-intro&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-2-machine-learning-intro&quot; aria-label=&quot;Anchor link for: 1-2-machine-learning-intro&quot;&gt;1.2 Machine Learning Intro&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Machine Learning: The training of programs developed by allowing a computer to learn from its experience, rather than through manually coding the individual steps.&lt;&#x2F;p&gt;
&lt;p&gt;Deep Learning is a branch of Machine Learning focus in Neural Networks. Visually, this is how they work:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Neural Networks, in theory, can solve any problem to any level of accuracy based on the parametrization of the weights - &lt;em&gt;Universal approximation theorem&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-3-weights&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-3-weights&quot; aria-label=&quot;Anchor link for: 1-3-weights&quot;&gt;1.3 Weights&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The key for the parametrization to be correct is updating the weight. The weights are &quot;responsible&quot; of finding the right solution to the problem at hand. For example, weighting correctly the pixels in a picture to solve the question &quot;Is a Dog or a Cat picture?&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;The weight updating is made by Stochastic gradient descent (SGD).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-4-terminology&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-4-terminology&quot; aria-label=&quot;Anchor link for: 1-4-terminology&quot;&gt;1.4 Terminology&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The terminology has changed. Here is the modern deep learning terminology for all the pieces we have discussed:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The functional form of the &lt;em&gt;model&lt;&#x2F;em&gt; is called its &lt;strong&gt;architecture&lt;&#x2F;strong&gt; (but be careful—sometimes people use &lt;em&gt;model&lt;&#x2F;em&gt; as a synonym of &lt;em&gt;architecture&lt;&#x2F;em&gt;, so this can get confusing).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;weights&lt;&#x2F;em&gt; are called &lt;strong&gt;parameters&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;strong&gt;predictions&lt;&#x2F;strong&gt; are calculated from the &lt;em&gt;independent variable&lt;&#x2F;em&gt;, which is the &lt;em&gt;data&lt;&#x2F;em&gt; not including the &lt;em&gt;labels&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;em&gt;results&lt;&#x2F;em&gt; of the model are called &lt;em&gt;predictions&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The measure of &lt;em&gt;performance&lt;&#x2F;em&gt; is called the &lt;strong&gt;loss&lt;&#x2F;strong&gt;. This measure of performance is only relevant &lt;strong&gt;for the computer&lt;&#x2F;strong&gt; to see if the model is doing better or worse in order &lt;strong&gt;to update the parameters&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The loss depends not only on the predictions, but also the correct &lt;em&gt;labels&lt;&#x2F;em&gt; (also known as &lt;em&gt;targets&lt;&#x2F;em&gt; or the &lt;em&gt;dependent variable&lt;&#x2F;em&gt;); e.g., &quot;dog&quot; or &quot;cat.&quot;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Metric&lt;&#x2F;strong&gt; is a function that measures quality of the model prediction &lt;strong&gt;for you&lt;&#x2F;strong&gt;. For example, the % of true labels predicted accurately. It can be the case that the loss change but identify the same number of true labels.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Clarification: In the course they use &quot;regression&quot; not as a linear regression but as any prediction model in which the result is a continuous variable.&lt;&#x2F;p&gt;
&lt;p&gt;After making these changes, our diagram looks like:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;More important terminology:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Data Ethics: Positive feedback loop&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Positive feedback loop is the effect of a small increase in the values of one part of a system that increases other values in the remaining system. Given that the definition is kinda technical, let&#x27;s use the case of a predictive policing model.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s say that a predictive policing model is created based on where arrests have been made in the past. In practice, this is not actually predicting crime, but rather predicting arrests, and is therefore partially simply reflecting biases in existing policing processes. Law enforcement officers then might use that model to decide where to focus their police activity, resulting in creased arrests in those areas.&lt;&#x2F;p&gt;
&lt;p&gt;These additional arrests would then feed back to re-training future versions of the model. The more the model is used, the more biased the data becomes, making the model even more biased, and so forth.&lt;&#x2F;p&gt;
&lt;p&gt;This is an example of a Positive feedback loop, where the system is this predictive policing model and the values are arrests.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;You cannot avoid positive feedback loop, use human interaction to notice the weird stuff that your algorithm might create.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Proxy bias&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Taking the previous example - If the proxy for the variable that you are interested (arrests as proxied for crime) is bias, the variable that you are predicting too.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Transfer learning&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Using a pretrained model for a task different to what it was originally trained for. It is key to use models with less data. Basically, instead of the model starting with random weights, it is already trained by someone else and parametrized.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Fine tuning&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;A transfer learning technique where the parameters of a pretrained model are updated by training for additional epochs using a different task to that used for pretaining.&lt;&#x2F;p&gt;
&lt;p&gt;An epoch is how many times the model looks at the data.&lt;&#x2F;p&gt;
&lt;p&gt;A more extended dictionary:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;4.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-5-p-values-principles&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-5-p-values-principles&quot; aria-label=&quot;Anchor link for: 1-5-p-values-principles&quot;&gt;1.5 P-values principles&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The practical importance of a model is not given by the p-values but by the results and implications. &lt;strong&gt;It only says that the confidence of the event happening by chance.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;P-values can indicate how incompatible the data are with a specified statistical model.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;P-values do not measure the probability that the studied hypothesis is true, or the probability that the data were produced by random chance alone.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Scientific conclusions and business or policy decisions should not be based only on whether a P-value passes a specific threshold.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Proper inference requires full reporting and transparency.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;A P-value, or statistical significance, does not measure the size of an effect or the importance of a result.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The threshold of statistical significance that is commonly used is a P-value of 0.05. This is conventional and arbitrary.&lt;&#x2F;p&gt;
&lt;ol start=&quot;6&quot;&gt;
&lt;li&gt;By itself, a P-value does not provide a good measure of evidence regarding a model or hypothesis.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;1-6-starting-a-machine-learning-project-defining-the-problem&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-6-starting-a-machine-learning-project-defining-the-problem&quot; aria-label=&quot;Anchor link for: 1-6-starting-a-machine-learning-project-defining-the-problem&quot;&gt;1.6 Starting a Machine Learning Project: Defining the problem&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;First, define the problem that you want to solve and the levers or variables that you can pull to change the outcome. &lt;strong&gt;What its the point of predicting an outcome if you cannot do anything about it?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;chapter-5-image-classification&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#chapter-5-image-classification&quot; aria-label=&quot;Anchor link for: chapter-5-image-classification&quot;&gt;Chapter 5 - Image Classification&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This chapter focused on building an image classification model.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-1-oxford-iiit-pet-dataset&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-1-oxford-iiit-pet-dataset&quot; aria-label=&quot;Anchor link for: 5-1-oxford-iiit-pet-dataset&quot;&gt;5.1 Oxford-IIIT Pet Dataset&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We will use a images dataset with 37 pet breeds classes and roughly 200 images for each class. The images have large variations in scale, pose, and lighting (here the &lt;a href=&quot;https:&#x2F;&#x2F;www.robots.ox.ac.uk&#x2F;~vgg&#x2F;data&#x2F;pets&#x2F;&quot;&gt;original source&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Downloading the Oxford-IIIT Pet Dataset
&lt;&#x2F;span&gt;&lt;span&gt;path &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;untar_data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;URLs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;PETS&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can use the &lt;code&gt;ls&lt;&#x2F;code&gt; method from fastai to see what is in our dataset and folders&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;Path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;BASE_PATH &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;path
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ls&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ls&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;annotations&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;#7393) [Path(&amp;#39;images&#x2F;newfoundland_31.jpg&amp;#39;),Path(&amp;#39;images&#x2F;Ragdoll_79.jpg&amp;#39;),Path(&amp;#39;images&#x2F;yorkshire_terrier_31.jpg&amp;#39;),Path(&amp;#39;images&#x2F;havanese_172.jpg&amp;#39;),Path(&amp;#39;images&#x2F;newfoundland_61.jpg&amp;#39;),Path(&amp;#39;images&#x2F;Abyssinian_175.jpg&amp;#39;),Path(&amp;#39;images&#x2F;leonberger_164.jpg&amp;#39;),Path(&amp;#39;images&#x2F;saint_bernard_86.jpg&amp;#39;),Path(&amp;#39;images&#x2F;boxer_108.jpg&amp;#39;),Path(&amp;#39;images&#x2F;scottish_terrier_195.jpg&amp;#39;)...]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;5-2-datablocks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-2-datablocks&quot; aria-label=&quot;Anchor link for: 5-2-datablocks&quot;&gt;5.2 DataBlocks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Fastai uses DataBlocks to load the data. Here we load the images of the folder into this &lt;code&gt;DataBlock&lt;&#x2F;code&gt;. Most of the arguments of the functions are quite intuitive to guess, but they are explained below in any case.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;DataBlock&lt;&#x2F;code&gt; is the envelope of the structure of the data. Here you tell fastai API how you organized the data.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;blocks&lt;&#x2F;code&gt; is how you tell fastai what inputs are images (&lt;code&gt;ImageBlock&lt;&#x2F;code&gt;) and what are the targets for the categories (&lt;code&gt;CategoryBlock&lt;&#x2F;code&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;get_items&lt;&#x2F;code&gt; is how you tell fastai to assemble our items inside the &lt;code&gt;DataBlock&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;splitter&lt;&#x2F;code&gt; is used to divide the images in training and validation set randomly.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;get_y&lt;&#x2F;code&gt; is used to create target values. The images are not labeled, they are just 7393 jpgs. We extract the target label (y) from the name of the file using regex expressions &lt;code&gt;RegexLabeller&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;pets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (&lt;&#x2F;span&gt;&lt;span&gt;ImageBlock, CategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_items&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;get_image_files,
&lt;&#x2F;span&gt;&lt;span&gt;                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;splitter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RandomSplitter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;seed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;42&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;using_attr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RegexLabeller&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;)_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;\d&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;jpg&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;$&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;name&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;460&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;aug_transforms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.75&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Tell DataBlock where the &amp;quot;source&amp;quot; is
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can take the first image and print the path using &lt;code&gt;.ls()&lt;&#x2F;code&gt; method as well.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;fname &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ls&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;fname
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;root&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;.fastai&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;oxford&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;iiit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;pet&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;images&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;newfoundland_31.jpg
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Using regex we can extract the target from the jpg name&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;re.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;findall&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;)_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;\d&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;+&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;jpg&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;$&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, fname.name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;newfoundland&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The last 2 methods are about data augmentation strategy, what fastai call &lt;em&gt;presizing&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Presizing&lt;&#x2F;strong&gt; is a particular way to do image augmentation that is designed to speed up computation and improve model accuracy.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;item_tfms&lt;&#x2F;code&gt; resize so all the images have the same dimension (In this case 460x460). It is needed so they can collate into tensors to be passed to the GPU. By default, it crops the image (not squish like when you set the background of your computer screen). On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;batch_tfms&lt;&#x2F;code&gt; randomly random crops and augment parts of the images. It&#x27;s only done once, in one batch. On the validation set, it only resizes to the final size needed for the model. On the training set, it first random crops and performs any augmentations, and then it resizes.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;aug_transforms&lt;&#x2F;code&gt; function can be used to create a list of images flipped, rotated, zoomed, wrapped, or with changed lighting. It helps the training process and avoids overfitting. &lt;code&gt;min_scale&lt;&#x2F;code&gt; determines how much of the image to select at minimum each time (More &lt;a href=&quot;https:&#x2F;&#x2F;docs.fast.ai&#x2F;vision.augment.html#aug_transforms&quot;&gt;here&lt;&#x2F;a&gt;)&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;fastai&#x2F;fastbook&#x2F;780b76bef3127ce5b64f8230fce60e915a7e0735&#x2F;images&#x2F;att_00060.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-3-other-resizing-methods&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-3-other-resizing-methods&quot; aria-label=&quot;Anchor link for: 5-3-other-resizing-methods&quot;&gt;5.3 Other resizing methods&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;With &lt;code&gt;show_batch&lt;&#x2F;code&gt; we can print a batch of images of the training set.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Show some images
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_12_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can squish the images, or add padding to the sides or crop it by copying the model with &lt;code&gt;.new&lt;&#x2F;code&gt; method and modifying the part of the model that you want to change.&lt;&#x2F;p&gt;
&lt;p&gt;Here we squish the images:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;pets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;256&lt;&#x2F;span&gt;&lt;span&gt;, ResizeMethod.Squish&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_14_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Here we add padding to the images:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;pets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;256&lt;&#x2F;span&gt;&lt;span&gt;, ResizeMethod.Pad, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;pad_mode&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;zeros&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_15_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Remember that by cropping the images we removed some of the features that allow us to perform recognition.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Instead, what we normally do in practice is to randomly select part of the image and crop it. On each epoch (which is one complete pass through all of our images in the dataset) we randomly select a different crop of each image. We can use &lt;code&gt;RandomResizedCrop&lt;&#x2F;code&gt; for that.&lt;&#x2F;p&gt;
&lt;p&gt;This means that our model can learn to focus on, and recognize, different features in our images at different epochs. It also reflects how images work in the real world as different photos of the same thing may be framed in slightly different ways.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;pets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RandomResizedCrop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;128&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Unique=True to have the same image repeated with different versions of this RandomResizedCrop transform
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;unique&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_17_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can alwasy use &lt;code&gt;new&lt;&#x2F;code&gt; method to get back to the first resizing method chosen (&lt;code&gt;aug_transforms&lt;&#x2F;code&gt;):&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;pets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;new&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;460&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;aug_transforms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.75&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pets.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;images&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;unique &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_19_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-4-creating-a-baseline-model&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-4-creating-a-baseline-model&quot; aria-label=&quot;Anchor link for: 5-4-creating-a-baseline-model&quot;&gt;5.4 Creating a baseline model&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We can see the shape of the data by printing one batch. Here we printed the labels &lt;code&gt;y&lt;&#x2F;code&gt;. There are 64 listed numbers printed as the batch size is 64. The range of the numbers goes from 0 to 36 as it represents the labels for the 37 pet breeds.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;x, y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;one_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;y
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TensorCategory&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;22&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;35&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;27&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;28&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;31&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;13&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;36&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;13&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;11&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;33&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;29&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;27&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;13&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;30&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;24&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;20&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;32&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;35&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;23&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;11&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;24&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;21&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;22&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;36&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;33&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;21&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    , &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;device&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;cuda:0&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Training a powerful baseline model requires 2 lines of code:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet34, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;error_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fine_tune&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;dls&lt;&#x2F;code&gt; is our data.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;restnet34&lt;&#x2F;code&gt; is a certain pre-trained CNN architecture.&lt;&#x2F;li&gt;
&lt;li&gt;The &lt;code&gt;metric&lt;&#x2F;code&gt; requested is &lt;code&gt;error_rate&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;By default, fast ai chooses the loss function that best fit our kind of data. With image data and a categorical outcome, fastai will default to using &lt;code&gt;cross-entropy loss&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;fine_tune(2)&lt;&#x2F;code&gt; indicates the number of epochs with the default model configuration.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This is the magic and simplicity of fastai. Once you have the data correctly loaded, the modeling with pre-trained models cannot be easier. Fastai automatically download the pre-trained architecture, choses an appropriate loss function and prints the metric results:&lt;&#x2F;p&gt;
&lt;table style=&quot;center&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1.532038&lt;&#x2F;td&gt;
      &lt;td&gt;0.331124&lt;&#x2F;td&gt;
      &lt;td&gt;0.112991&lt;&#x2F;td&gt;
      &lt;td&gt;01:07&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center; overflow-x:auto;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.514930&lt;&#x2F;td&gt;
      &lt;td&gt;0.295484&lt;&#x2F;td&gt;
      &lt;td&gt;0.094046&lt;&#x2F;td&gt;
      &lt;td&gt;01:12&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.330700&lt;&#x2F;td&gt;
      &lt;td&gt;0.223524&lt;&#x2F;td&gt;
      &lt;td&gt;0.071042&lt;&#x2F;td&gt;
      &lt;td&gt;01:12&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;The second column show the cross-entropy loss in the training and validation set. The fourth column show less than 1% of error classifying the images.&lt;&#x2F;p&gt;
&lt;p&gt;It even includes handy shortcuts like &lt;code&gt;show_results&lt;&#x2F;code&gt; to print the real and predicted labels for a quick check test of labels and predictions:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_results&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_26_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-5-model-interpretation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-5-model-interpretation&quot; aria-label=&quot;Anchor link for: 5-5-model-interpretation&quot;&gt;5.5 Model interpretation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;After building a model, you don&#x27;t want to know only how many targets got right. You might want to know which targets are harder to predict or which images got wrong to train it better. fastai includes a &lt;code&gt;ClassificationInterpretation&lt;&#x2F;code&gt; class from which you can call &lt;code&gt;plot_confusion_matrix&lt;&#x2F;code&gt;, &lt;code&gt;most_confused&lt;&#x2F;code&gt; or &lt;code&gt;plot_top_losses&lt;&#x2F;code&gt; methods to extract this information easily.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;interp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;ClassificationInterpretation.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;from_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;learn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;interp.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_confusion_matrix&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;figsize &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dpi&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;60&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_28_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can see which are the labels that the model more struggles to differentiate:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;interp.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;most_confused&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_val &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;american_pit_bull_terrier&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;american_bulldog&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;British_Shorthair&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Russian_Blue&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Ragdoll&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Birman&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Siamese&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Birman&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;american_pit_bull_terrier&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;staffordshire_bull_terrier&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;chihuahua&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;miniature_pinscher&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;staffordshire_bull_terrier&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;american_pit_bull_terrier&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And the the most &quot;wrong&quot; predictions:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;interp.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_top_losses&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;nrows &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_31_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-6-exporting-and-importing-a-model&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-6-exporting-and-importing-a-model&quot; aria-label=&quot;Anchor link for: 5-6-exporting-and-importing-a-model&quot;&gt;5.6 Exporting and importing a model&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Models with multiple layers, epochs, and parameters can take hours to train. Instead of starting over every time you run the notebook, the model can be saved and loaded again.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Saving&#x2F;Exporting a model&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;export&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;os.path.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;abspath&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;.&#x2F;my_export.pkl&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To check that the model is saved, you can either navigate the folder and see the &lt;code&gt;.pkl&lt;&#x2F;code&gt;, or also you can call the &lt;code&gt;path.ls()&lt;&#x2F;code&gt; method and see the file printed.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Loading&#x2F;Importing a model&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn_inf &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;load_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;my_export.pkl&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;5-7-testing-the-model-outside-the-fastai-environment&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-7-testing-the-model-outside-the-fastai-environment&quot; aria-label=&quot;Anchor link for: 5-7-testing-the-model-outside-the-fastai-environment&quot;&gt;5.7 Testing the model outside the fastai environment&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To see if the model would work outside the dataloader environment, I googled &quot;Bengal cat&quot; in google images and drag a random image into the Google Colab folder. I consider the image as tricky, as it contains a human holding a Bengal cat:&lt;&#x2F;p&gt;
&lt;img src=&quot;.&#x2F;images&#x2F;test_image.jpg&quot; alt=&quot;Random image finded in the internet&quot; width=&quot;300&quot; height=&quot;400&quot;&gt;
&lt;p&gt;I simply called the &lt;code&gt;predict&lt;&#x2F;code&gt; method of the model trained before to see if it is as easy at it looks to use fastai.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn_inf.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;test_image.jpg&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;(&amp;#39;Bengal&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt; tensor(1),
&lt;&#x2F;span&gt;&lt;span&gt; tensor([9.6365e-07, 9.9558e-01, 5.0118e-09, 2.5665e-08, 5.0663e-08, 4.2385e-03, 1.6677e-04, 1.0780e-08, 3.7194e-08, 1.1227e-07, 7.4500e-09, 3.3078e-06, 4.6680e-08, 8.1986e-07, 1.0533e-07, 8.3078e-08,
&lt;&#x2F;span&gt;&lt;span&gt;         9.4154e-08, 2.7704e-08, 2.7787e-07, 2.6699e-06, 2.5465e-06, 7.7660e-09, 8.5412e-09, 1.5087e-07, 3.9640e-08, 3.1239e-08, 9.4404e-07, 3.2094e-08, 5.2541e-08, 7.1558e-09, 4.6352e-09, 1.7388e-08,
&lt;&#x2F;span&gt;&lt;span&gt;         6.1503e-08, 6.6123e-08, 7.2059e-09, 9.4673e-08, 5.6627e-07]))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Surprisingly, it got the label of the image right. Loading the training data was less than 10 lines of code and the model itself is 1 line. It could handle random animal images and classify them regardless of the input image size, image format, or anything else.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-8-improving-our-model&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-8-improving-our-model&quot; aria-label=&quot;Anchor link for: 5-8-improving-our-model&quot;&gt;5.8 Improving Our Model&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The one-line-of-code model is great, but we might want to tweak the model and compare the results to increase the accuracy. We will explore 4 techniques or tools that can improve the model:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Learning rate finder&lt;&#x2F;li&gt;
&lt;li&gt;Transfer Learning&lt;&#x2F;li&gt;
&lt;li&gt;Discriminative Learning rates&lt;&#x2F;li&gt;
&lt;li&gt;Selecting the right number of epochs&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h4 id=&quot;5-8-1-learning-rate-finder&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-8-1-learning-rate-finder&quot; aria-label=&quot;Anchor link for: 5-8-1-learning-rate-finder&quot;&gt;5.8.1 Learning rate finder&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The general idea of a &lt;em&gt;learning rate finder&lt;&#x2F;em&gt; is to start with a very very small learning rates, watch the loss function, and iterating with bigger and bigger learning rates.&lt;&#x2F;p&gt;
&lt;p&gt;We start with some number so small that we would never expect it to be too big to handle, like .00000007. We use that for one mini-batch, track the loss, and double the learning rate. We keep doing it until the loss gets worse. Once it started to get worse and worse, we should select a learning rate a bit lower than that point.&lt;&#x2F;p&gt;
&lt;p&gt;fastai method &lt;code&gt;lr_find()&lt;&#x2F;code&gt; does all this loop for us:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet34, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;error_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;lr_find&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;SuggestedLRs(valley=tensor(0.0025))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_40_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Let&amp;#39;s call it the &amp;quot;leslie_smith_lr&amp;quot; in honor to the author of the orignal paper
&lt;&#x2F;span&gt;&lt;span&gt;leslie_smith_lr &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.0025
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet34, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;error_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fine_tune&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;base_lr &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;leslie_smith_lr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1.261431&lt;&#x2F;td&gt;
      &lt;td&gt;0.310061&lt;&#x2F;td&gt;
      &lt;td&gt;0.102842&lt;&#x2F;td&gt;
      &lt;td&gt;01:10&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.547525&lt;&#x2F;td&gt;
      &lt;td&gt;0.373586&lt;&#x2F;td&gt;
      &lt;td&gt;0.115020&lt;&#x2F;td&gt;
      &lt;td&gt;01:14&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.348811&lt;&#x2F;td&gt;
      &lt;td&gt;0.226233&lt;&#x2F;td&gt;
      &lt;td&gt;0.068336&lt;&#x2F;td&gt;
      &lt;td&gt;01:14&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;Compared with the baseline model we reduced slightly the error_rate. In the next tables, I will keep track of the improvements and the comparison of the methods.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;5-8-2-transfer-learning-and-freezing&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-8-2-transfer-learning-and-freezing&quot; aria-label=&quot;Anchor link for: 5-8-2-transfer-learning-and-freezing&quot;&gt;5.8.2 Transfer Learning and Freezing&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;&lt;strong&gt;Transfer learning&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The last layer in a CNN is the classification task. This pet breed classification task is a layer with 37 neurons with a softmax function that gives the probability of the image for each of the 37 classes.&lt;&#x2F;p&gt;
&lt;p&gt;But how can we use all this hard-consuming weighting parametrization in another image classification task?&lt;&#x2F;p&gt;
&lt;p&gt;We can take the model, ditch the last layer and substitute it for our new classification task. That&#x27;s transfer learning - using the knowledge learned from a task and re-using it for another different.&lt;&#x2F;p&gt;
&lt;p&gt;In practical terms,we take the parameters&#x2F;weights of the model and we substitute the last layer for the new task without starting the weighting from scratch. It saves time and also produces better results. &lt;code&gt;restnet34&lt;&#x2F;code&gt; is an example of this, as it is a pre-trained model with its custom parametrization.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Freezing&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Transfer learning can be applied by a technique called freezing. By freezing you tell the model not to touch certain layers. They are &quot;frozen&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Why you would want to freeze layers?&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;To focus on the layer that matters. As I said, &lt;code&gt;restnet34&lt;&#x2F;code&gt; is already trained beforehand. We can tell the model to focus more on the last layer, our classification task, and keep the former ones untouched. Freeze and unfreeze effectively allow you to decide which specific layers of your model you want to train at a given time.&lt;&#x2F;p&gt;
&lt;p&gt;Freezing is especially handy when you want to focus not only on the weighting but also on some parameters like the learning rate.&lt;&#x2F;p&gt;
&lt;p&gt;To allow transfer learning we can use &lt;code&gt;fit_one_cycle&lt;&#x2F;code&gt; method, instead of &lt;code&gt;fine_tune&lt;&#x2F;code&gt;. Here we load the model with our data and train it for 3 epochs:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet34, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;error_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;, leslie_smith_lr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1.220214&lt;&#x2F;td&gt;
      &lt;td&gt;0.328361&lt;&#x2F;td&gt;
      &lt;td&gt;0.103518&lt;&#x2F;td&gt;
      &lt;td&gt;01:13&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.559653&lt;&#x2F;td&gt;
      &lt;td&gt;0.242575&lt;&#x2F;td&gt;
      &lt;td&gt;0.080514&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.340312&lt;&#x2F;td&gt;
      &lt;td&gt;0.220747&lt;&#x2F;td&gt;
      &lt;td&gt;0.069689&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;Consider this model parametrization &quot;froze&quot;. Using &lt;code&gt;unfreeze()&lt;&#x2F;code&gt; method allows the model to start over from the already weighting from &lt;code&gt;fit_one_cyle&lt;&#x2F;code&gt;, so it doesn&#x27;t start from random weighting but the &quot;frozen&quot; parameters from the 3 first epochs of &lt;code&gt;fit_one_cyle&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;unfreeze&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It is is easier for the model to start from a pre-trained weighting than with random weighting. To illustrate this point let&#x27;s try to search for an optimal learning rate again:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;lr_find&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;SuggestedLRs(valley=4.365158383734524e-05)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_49_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In this graph, the Loss axis is way smaller than the previous one. The model is already trained beforehand and therefore trying mini-batches of different learning rates and iterating is easier.&lt;&#x2F;p&gt;
&lt;p&gt;To apply &quot;transfer learning&quot; we train the model with another 6 epochs that will start from the previous parametrization. We use the new learning rate as well.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;leslie_smith_lr_2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.365158383734524e-05
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span&gt;, leslie_smith_lr_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Instead of printing the epoch results, from here on I&#x27;ll show the results of the last epoch and the comparison with the other models:&lt;&#x2F;p&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;Model&lt;&#x2F;th&gt;
      &lt;th&gt;Train Loss&lt;&#x2F;th&gt;
      &lt;th&gt;Validation Loss&lt;&#x2F;th&gt;
      &lt;th&gt;Error rate&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 Baseline (2 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.330700&lt;&#x2F;td&gt;
      &lt;td&gt;0.223524&lt;&#x2F;td&gt;
      &lt;td&gt;0.071042&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 with Learning rate finder (2 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.348811&lt;&#x2F;td&gt;
      &lt;td&gt;0.226233&lt;&#x2F;td&gt;
      &lt;td&gt;0.068336&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;b&gt;ResNet-34 with Transfer Learning (6 epochs)&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.534172&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.261891&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.083897&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;h4 id=&quot;5-8-3-discriminative-learning-rates&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-8-3-discriminative-learning-rates&quot; aria-label=&quot;Anchor link for: 5-8-3-discriminative-learning-rates&quot;&gt;5.8.3 Discriminative learning rates&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Like many good ideas in deep learning, the idea of &lt;strong&gt;&lt;em&gt;Discriminative learning rates&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; is extremely simple: use a lower learning rate for the early layers of the neural network,
and a higher learning rate for the later layers&lt;&#x2F;p&gt;
&lt;p&gt;The first layer learns very simple foundations, like image edges and gradient detectors; these are likely to be just as useful for nearly any task. The later layers learn much more complex concepts, like the concept of “eye” and “sunset,” which might not be useful in your task at all - maybe you’re classifying car models, for instance. So it makes sense to let the later layers fine-tune more quickly than earlier layers.&lt;&#x2F;p&gt;
&lt;p&gt;By default, fastai &lt;code&gt;cnn_learner&lt;&#x2F;code&gt; uses discriminative learning rates.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s use this approach to replicate the previous training, but this time using Discriminative learning rates using a slice range in the learning rate parameter: &lt;code&gt;lr_max=slice(4e-6,4e-4)&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The first value (&lt;code&gt;4e-6&lt;&#x2F;code&gt;) is the learning rate in the earliest layer of the neural network.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The second value (&lt;code&gt;4e-4&lt;&#x2F;code&gt;) is the learning rate of the final layer.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The layers in between will have learning rates that scale up equidistantly throughout that range - from the first until they reach the second value.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Model
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet34, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;error_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Pre-train the model
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;, leslie_smith_lr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;unfreeze&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Train the model with a learning rate range
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;lr_max&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;slice&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4e-6&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4e-4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;Model&lt;&#x2F;th&gt;
      &lt;th&gt;Train Loss&lt;&#x2F;th&gt;
      &lt;th&gt;Validation Loss&lt;&#x2F;th&gt;
      &lt;th&gt;Error rate&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 Baseline (2 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.330700&lt;&#x2F;td&gt;
      &lt;td&gt;0.223524&lt;&#x2F;td&gt;
      &lt;td&gt;0.071042&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 with Learning rate Finder (2 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.348811&lt;&#x2F;td&gt;
      &lt;td&gt;0.226233&lt;&#x2F;td&gt;
      &lt;td&gt;0.068336&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 with Transfer Learning (6 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.534172&lt;&#x2F;td&gt;
      &lt;td&gt;0.261891&lt;&#x2F;td&gt;
      &lt;td&gt;0.083897&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;&lt;b&gt;ResNet-34 with Discriminative learning rates (12 epochs)&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.049675&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.181254&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.048714&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;h4 id=&quot;5-8-4-selecting-the-number-of-epochs&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-8-4-selecting-the-number-of-epochs&quot; aria-label=&quot;Anchor link for: 5-8-4-selecting-the-number-of-epochs&quot;&gt;5.8.4 Selecting the Number of Epochs&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The more epochs, the more time and tries the model has to learn the trained data. Your first approach to training should be to simply pick a specific number of epochs that you are happy to wait for, and look at the training and validation loss plots.&lt;&#x2F;p&gt;
&lt;p&gt;Using &lt;code&gt;.plot_loss()&lt;&#x2F;code&gt; you can see if the validation loss keeps getting better with more epochs. If not, it is a waste of time to use more than the necessary epochs.&lt;&#x2F;p&gt;
&lt;p&gt;For some machine learning problems is worth keep training the model for a day to earn 1% more accuracy, such as programming competitions, but in most cases choosing the right model or better parametrization is going to be more important than squishing the last marginal accuracy point with 300 more epochs.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.recorder.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_loss&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;Fastbook_Chapter_5_Image_Classification_60_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-9-deeper-architectures&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-9-deeper-architectures&quot; aria-label=&quot;Anchor link for: 5-9-deeper-architectures&quot;&gt;5.9 Deeper Architectures&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In general, a model with more parameters can describe your data more accurately. A larger version of a ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, basically because it has more parameters to suffer from overfitting.&lt;&#x2F;p&gt;
&lt;p&gt;Another downside of deeper architectures is that they take quite a bit longer to
train. One technique that can speed things up a lot is &lt;strong&gt;mixed-precision training&lt;&#x2F;strong&gt;. This
refers to using less-precise numbers (half-precision floating point, also called fp16)
where possible during training.&lt;&#x2F;p&gt;
&lt;p&gt;Instead of using &lt;code&gt;.fit_one_cycle()&lt;&#x2F;code&gt; and then &lt;code&gt;unfreeze()&lt;&#x2F;code&gt; methods we tell fastai how many epochs to freeze with &lt;code&gt;freeze_epochs&lt;&#x2F;code&gt; since we are not changing the learning rates from one step to the other like in Discriminative learning rates.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.callback.fp16 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet50, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;error_rate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to_fp16&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fine_tune&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;freeze_epochs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1.260760&lt;&#x2F;td&gt;
      &lt;td&gt;0.327534&lt;&#x2F;td&gt;
      &lt;td&gt;0.095399&lt;&#x2F;td&gt;
      &lt;td&gt;01:07&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.595598&lt;&#x2F;td&gt;
      &lt;td&gt;0.297897&lt;&#x2F;td&gt;
      &lt;td&gt;0.089310&lt;&#x2F;td&gt;
      &lt;td&gt;01:07&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.431712&lt;&#x2F;td&gt;
      &lt;td&gt;0.256303&lt;&#x2F;td&gt;
      &lt;td&gt;0.089986&lt;&#x2F;td&gt;
      &lt;td&gt;01:07&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;error_rate&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.286988&lt;&#x2F;td&gt;
      &lt;td&gt;0.246470&lt;&#x2F;td&gt;
      &lt;td&gt;0.079161&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.323408&lt;&#x2F;td&gt;
      &lt;td&gt;0.258964&lt;&#x2F;td&gt;
      &lt;td&gt;0.091340&lt;&#x2F;td&gt;
      &lt;td&gt;01:08&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.262799&lt;&#x2F;td&gt;
      &lt;td&gt;0.315306&lt;&#x2F;td&gt;
      &lt;td&gt;0.083221&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.167648&lt;&#x2F;td&gt;
      &lt;td&gt;0.242762&lt;&#x2F;td&gt;
      &lt;td&gt;0.073072&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.090543&lt;&#x2F;td&gt;
      &lt;td&gt;0.180670&lt;&#x2F;td&gt;
      &lt;td&gt;0.056834&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;0.060775&lt;&#x2F;td&gt;
      &lt;td&gt;0.174947&lt;&#x2F;td&gt;
      &lt;td&gt;0.050068&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;h3 id=&quot;5-10-final-model-results-comparison&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#5-10-final-model-results-comparison&quot; aria-label=&quot;Anchor link for: 5-10-final-model-results-comparison&quot;&gt;5.10 Final model results comparison&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Based on the validation loss and the error rate, a deeper and more complex architecture(RestNet50) and the model with discriminative learning rates hold the best results.&lt;&#x2F;p&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: center;&quot;&gt;
      &lt;th&gt;Model&lt;&#x2F;th&gt;
      &lt;th&gt;Train Loss&lt;&#x2F;th&gt;
      &lt;th&gt;Validation Loss&lt;&#x2F;th&gt;
      &lt;th&gt;Error rate&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 Baseline (2 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.330700&lt;&#x2F;td&gt;
      &lt;td&gt;0.223524&lt;&#x2F;td&gt;
      &lt;td&gt;0.071042&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 with Learning rate Finder (2 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.348811&lt;&#x2F;td&gt;
      &lt;td&gt;0.226233&lt;&#x2F;td&gt;
      &lt;td&gt;0.068336&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 with Transfer Learning (6 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.534172&lt;&#x2F;td&gt;
      &lt;td&gt;0.261891&lt;&#x2F;td&gt;
      &lt;td&gt;0.083897&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;ResNet-34 with Discriminative learning rates (12 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.049675&lt;&#x2F;td&gt;
      &lt;td&gt;0.181254&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.048714&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Mixed-Precision ResNet-50 (6 epochs)&lt;&#x2F;td&gt;
      &lt;td&gt;0.060775&lt;&#x2F;td&gt;
      &lt;td&gt;&lt;b&gt;0.174947&lt;&#x2F;b&gt;&lt;&#x2F;td&gt;
      &lt;td&gt;0.050068&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;In any case, these techniques should be tried and evaluated for every image classification problem, as the results depend on the specific data. This is just an example of the applications and could easily improve any initial model baseline.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;chapter-6-other-computer-vision-problems&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#chapter-6-other-computer-vision-problems&quot; aria-label=&quot;Anchor link for: chapter-6-other-computer-vision-problems&quot;&gt;Chapter 6 - Other Computer Vision Problems&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Multi-label classification&lt;&#x2F;li&gt;
&lt;li&gt;Regression.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I will use Google Colab to run the code, as in Chapter 5 notes.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;!pip install &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Uqq fastbook
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt;████████████████████████████████&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt; 727kB &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;29.0&lt;&#x2F;span&gt;&lt;span&gt;MB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;s
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt;████████████████████████████████&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;| &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.2&lt;&#x2F;span&gt;&lt;span&gt;MB &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;45.6&lt;&#x2F;span&gt;&lt;span&gt;MB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;s
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt;████████████████████████████████&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt; 194kB &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;47.3&lt;&#x2F;span&gt;&lt;span&gt;MB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;s
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt;████████████████████████████████&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt; 51kB &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.9&lt;&#x2F;span&gt;&lt;span&gt;MB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;s
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt;████████████████████████████████&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt; 61kB &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.2&lt;&#x2F;span&gt;&lt;span&gt;MB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;s
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt;████████████████████████████████&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;|&lt;&#x2F;span&gt;&lt;span&gt; 61kB &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.0&lt;&#x2F;span&gt;&lt;span&gt;MB&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;s
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;fastbook
&lt;&#x2F;span&gt;&lt;span&gt;fastbook.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;setup_book&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.vision.all &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Mounted at &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;content&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;gdrive
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;6-1-multi-label-classification&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-1-multi-label-classification&quot; aria-label=&quot;Anchor link for: 6-1-multi-label-classification&quot;&gt;6.1 Multi-label classification&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Multi-label classification&lt;&#x2F;strong&gt; is when you want to predict more than one label per image (or sometimes none at all). In practice, it is probably more common to have some images with zero matches or more than one match, we should probably expect in practice that multi-label classifiers are more widely applicable than single-label classifiers.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;PASCAL Visual Object Classes Challenge 2007 Dataset&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;path &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;untar_data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;URLs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;PASCAL_2007&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;read_csv&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;train.csv&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;head&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;fname&lt;&#x2F;th&gt;
      &lt;th&gt;labels&lt;&#x2F;th&gt;
      &lt;th&gt;is_valid&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;000005.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;chair&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000007.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;car&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000009.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;horse person&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000012.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;car&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000016.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;bicycle&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;&lt;strong&gt;Building the DataBlock&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The data is not preprocessed, so we will need to shape it correctly to use Fastai.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Get the input path and the target variable&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The original dataset is a collection that returns a tuple of your independent and dependent variable for a single item. To use the &lt;code&gt;DataLoader&lt;&#x2F;code&gt; of Fastai we will need to format and preprocess the data. In a &lt;code&gt;DataLoader&lt;&#x2F;code&gt;, each mini-batch contains a batch of independent variables and a batch of dependent variables.&lt;&#x2F;p&gt;
&lt;p&gt;We can see the current shape of the data by calling &lt;code&gt;DataBlock.datasets&lt;&#x2F;code&gt; to create a Datasets object from the source.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;dsets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;datasets&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dsets.train&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;(fname       002815.jpg
&lt;&#x2F;span&gt;&lt;span&gt; labels          person
&lt;&#x2F;span&gt;&lt;span&gt; is_valid          True
&lt;&#x2F;span&gt;&lt;span&gt; Name: 1414, dtype: object,
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt; fname       002815.jpg
&lt;&#x2F;span&gt;&lt;span&gt; labels          person
&lt;&#x2F;span&gt;&lt;span&gt; is_valid          True
&lt;&#x2F;span&gt;&lt;span&gt; Name: 1414, dtype: object)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dsets.valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;(fname             000892.jpg
&lt;&#x2F;span&gt;&lt;span&gt; labels      person motorbike
&lt;&#x2F;span&gt;&lt;span&gt; is_valid               False
&lt;&#x2F;span&gt;&lt;span&gt; Name: 443, dtype: object,
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt; fname             000892.jpg
&lt;&#x2F;span&gt;&lt;span&gt; labels      person motorbike
&lt;&#x2F;span&gt;&lt;span&gt; is_valid               False
&lt;&#x2F;span&gt;&lt;span&gt; Name: 443, dtype: object)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The data is in the wrong format. Instead of a path to the images and the corresponding label, it simply returns a row of the DataFrame, twice. This is because by default, the &lt;strong&gt;&lt;code&gt;DataBlock&lt;&#x2F;code&gt; assumes we have two things: input and target&lt;&#x2F;strong&gt;. Here we don&#x27;t have a path or the target specified, so it returns the input twice.&lt;&#x2F;p&gt;
&lt;p&gt;We are going to need to grab the appropriate fields from the DataFrame, which we can do by passing &lt;code&gt;get_x&lt;&#x2F;code&gt; and &lt;code&gt;get_y&lt;&#x2F;code&gt; functions.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;get_x&lt;&#x2F;code&gt;: to create a function that points out the path of the files (in the &lt;em&gt;fname&lt;&#x2F;em&gt; column).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_images_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;train&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fname&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;get_y&lt;&#x2F;code&gt;: to create a function that takes the targets from the labels column and splits on the space character as there are several labels.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_target_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;r&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;labels&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;split&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We will try &lt;code&gt;DataBlock.datasets&lt;&#x2F;code&gt; again, now with the data formatted using the functions:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# We add the data format to the DataBlock
&lt;&#x2F;span&gt;&lt;span&gt;dblock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_images_name,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_target_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# We update de dataset feeded to the DataBlock
&lt;&#x2F;span&gt;&lt;span&gt;dsets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;datasets&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dsets.train&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;34&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;pascal_2007&#x2F;train&#x2F;002359.jpg&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;dog&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now it returns correctly the datablock format: input (the &lt;em&gt;jpg&lt;&#x2F;em&gt;), and the target (the image label).&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Transform the data into tensors&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;We can use the parameter &lt;code&gt;ImageBlock&lt;&#x2F;code&gt; to transform these inputs and targets into tensors. It is a good practice to specify the &lt;code&gt;MultiCategoryBlock&lt;&#x2F;code&gt; method so fastai knows that is a multiclassification type of problem.&lt;&#x2F;p&gt;
&lt;p&gt;In any case, fastai would know that is this type of problem because of the multiple labeling.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span&gt;ImageBlock, MultiCategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_images_name,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_target_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;dsets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;datasets&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dsets.train&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;PILImage mode=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RGB &lt;&#x2F;span&gt;&lt;span&gt;size=500x336,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TensorMultiCategory&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;By adding &lt;code&gt;ImageBlock&lt;&#x2F;code&gt;, each element is transformed into a tensor with a 1 representing the label of the image. The categories are &lt;strong&gt;hot-encoded&lt;&#x2F;strong&gt;. A vector of 0s and 1s in each location is represented in the data, to encode a list of integers. There are 20 categories, so the length of this list of 0s and 1 equals 20.&lt;&#x2F;p&gt;
&lt;p&gt;The reason we can&#x27;t just use a list of category indices is that each list would be a different length. For example, an image with 2 labels would have 2 elements in a list and a length of 2. An image with 1 label would be a list of length 1. Pytorch&#x2F;fastai require tensors where targets have to have the same length and that&#x27;s why we use hot-encoding.&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Create a training and validation data split&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;For now, the dataset is not divided correctly into train and validation dataset. If we take a look at the dataset, it contains a column called &lt;code&gt;is_valid&lt;&#x2F;code&gt; that we have been ignoring. This column is a boolean that signals that the data belongs to the train set or the validation set.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;head&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;fname&lt;&#x2F;th&gt;
      &lt;th&gt;labels&lt;&#x2F;th&gt;
      &lt;th&gt;is_valid&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;000005.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;chair&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000007.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;car&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000009.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;horse person&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000012.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;car&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;000016.jpg&lt;&#x2F;td&gt;
      &lt;td&gt;bicycle&lt;&#x2F;td&gt;
      &lt;td&gt;True&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;&lt;code&gt;DataBlock&lt;&#x2F;code&gt; has been using a random split of the data by default. However, we can create a simple splitter function that takes the values in which &lt;code&gt;is_valid&lt;&#x2F;code&gt; is &lt;code&gt;False&lt;&#x2F;code&gt; and stored them in a variable called &lt;code&gt;train&lt;&#x2F;code&gt;, and if &lt;code&gt;True&lt;&#x2F;code&gt; stored them in a variable called &lt;code&gt;valid&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;splitter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;  train &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df.index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;~&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tolist&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;  valid &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df.index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tolist&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;train,valid
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This function separates train and validation datasets to make the split. As long as it returns these 2 elements (train and validation), the &lt;code&gt;splitter&lt;&#x2F;code&gt; method of &lt;code&gt;DataBlock&lt;&#x2F;code&gt; can take it.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span&gt;ImageBlock, MultiCategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;splitter &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;splitter,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_images_name,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_target_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dsets &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;datasets&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dsets.train&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;PILImage mode=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RGB &lt;&#x2F;span&gt;&lt;span&gt;size=500x333,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TensorMultiCategory&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now, the split of train and validation has the correct labeling.&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Input resizing&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Lastly, for the &lt;code&gt;DataBlock&lt;&#x2F;code&gt; to be converted into a &lt;code&gt;DataLoader&lt;&#x2F;code&gt; it needs that every item is of the same size. To do this, we can use &lt;code&gt;RandomResizedCrop&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To prove that, we can try the previous &lt;code&gt;DataBlock&lt;&#x2F;code&gt; without resizing:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;nrows&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ncols&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;---------------------------------------------------------------------------
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RuntimeError                              Traceback &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;most recent call last&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;ipython&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;input&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;127&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;98aca4c77278&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;module&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;()
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;----&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2 &lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;nrows&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ncols&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;core.py &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;, b, max_n, ctxs, show, unique, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;**&lt;&#x2F;span&gt;&lt;span&gt;kwargs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;98             &lt;&#x2F;span&gt;&lt;span&gt;old_get_idxs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.get_idxs
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;99             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.get_idxs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;lambda&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;Inf.zeros
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;--&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;is &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;None&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;b &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;one_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;101         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if not &lt;&#x2F;span&gt;&lt;span&gt;show&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;_pre_show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;b, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;102         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#d33682;&quot;&gt;self&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;_pre_show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;b, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ctxs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;ctxs, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;max_n, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;**&lt;&#x2F;span&gt;&lt;span&gt;kwargs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RuntimeError&lt;&#x2F;span&gt;&lt;span&gt;: stack expects each tensor to be equal size,
&lt;&#x2F;span&gt;&lt;span&gt;    but got &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;500&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;441&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span&gt;at entry &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;and &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;333&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;500&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span&gt;at entry &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;By including resizing, the &lt;code&gt;DataBlock&lt;&#x2F;code&gt; is correctly loaded and transformed into a &lt;code&gt;DataLoader&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span&gt;ImageBlock, MultiCategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;splitter &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;splitter,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_images_name,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;get_target_name,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RandomResizedCrop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;128&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.35&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;nrows&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ncols&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_40_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Fastai includes a method called &lt;code&gt;summary&lt;&#x2F;code&gt; to check if anything goes wrong when you create your dataset. Besides the previous printing of the batch, we can call it to see errors, if any.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock.summary
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;bound method DataBlock.summary of
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;fastai.data.block.DataBlock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;object &lt;&#x2F;span&gt;&lt;span&gt;at &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0x7f01e42df2d0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&amp;gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Binary Cross Entropy and Categorical Cross Entropy&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Now that we have our &lt;code&gt;DataLoaders&lt;&#x2F;code&gt; object we can move to define the loss function that we will use: &lt;strong&gt;&lt;em&gt;Binary Cross Entropy&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; (BCE).&lt;&#x2F;p&gt;
&lt;p&gt;BCE is a kind of loss function for &lt;strong&gt;multiple-labels&lt;&#x2F;strong&gt; classification problem. It is slightly different from &lt;strong&gt;&lt;em&gt;Categorical Cross Entropy&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;, the default loss function of &lt;strong&gt;single-label&lt;&#x2F;strong&gt; classification problem.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;Categorical Cross Entropy&lt;&#x2F;strong&gt;, all the nodes of the final layer of the neural network go through a &lt;code&gt;softmax&lt;&#x2F;code&gt; transformation function that takes the most positive as the label predicted. The biggest positive value is transformed to 1 and the rest of the label values to 0.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;In &lt;strong&gt;Binary Cross Entropy&lt;&#x2F;strong&gt;, all the nodes of the final layer pass through a &lt;code&gt;sigmoid&lt;&#x2F;code&gt; function that transforms all the positive values above a threshold to 1, and the rest to 0. Several values can be above the threshold, as multiple labels could be present in the image.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The &lt;em&gt;&quot;Binary&quot;&lt;&#x2F;em&gt; comes from having a prediction &lt;strong&gt;for every category&lt;&#x2F;strong&gt;. Every label is either 0 or 1, depending on if the label is present in the image.&lt;&#x2F;p&gt;
&lt;p class=&quot;mark&quot;&gt;Why do we use sigmoid instead of softmax in multi-labeling?&lt;&#x2F;p&gt;
&lt;p&gt;Well, the image &lt;strong&gt;in single-label classification cannot be 2 things at the same time&lt;&#x2F;strong&gt;. An image is either labeled as &quot;dog&quot; or &quot;cat&quot;, but cannot be both. Makes sense to use softmax and use the maximum value for the most probable predicted label - That would be a 1, and the rest 0s.&lt;&#x2F;p&gt;
&lt;p&gt;The problem in multi-labeling is different. In &lt;strong&gt;multicalss classification an image can contain several labels that are independent&lt;&#x2F;strong&gt;. For example a dog, a cat, and a person in the same photo. Therefore, the probability of the label &quot;dog&quot; should not depend on the probability of the label &quot;person&quot;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Sigmoid transformation in practice&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;To illustrate how sigmoid and the BCE loss function works we will build a simple model using the data that we formated before.&lt;&#x2F;p&gt;
&lt;p&gt;We will use Restnet18 and pass a small batch to explore the outputs.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Model
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet18&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Making sure that both the model and the data are processed in the GPU
&lt;&#x2F;span&gt;&lt;span&gt;learn.model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cuda&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;learn.dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;cuda&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Passing one batch
&lt;&#x2F;span&gt;&lt;span&gt;X,y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dls.train.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;one_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Exploring the outputs of the last layer of the model
&lt;&#x2F;span&gt;&lt;span&gt;outputs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;outputs.shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;outputs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;20&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.0786&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6746&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.7760&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.8992&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9360&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.1045&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.5859&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3760&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6101&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6136&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.0267&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5890&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2249&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5697&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4767&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2276&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2324&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.0516&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.7298&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1993&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;device&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;cuda:0&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;grad_fn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;SelectBackward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p class=&quot;mark&quot;&gt; What are these tensor values?&lt;&#x2F;p&gt;
&lt;p&gt;These are values corresponding to the nodes of the last layer. Note that these values haven&#x27;t gone yet through the transformation function (sigmoid&#x2F;softmax&#x2F;others) that gets you the final label prediction. &lt;strong&gt;After&lt;&#x2F;strong&gt; the transformation function, these outputs will be either 0s (not that label) or 1s (label identified).&lt;&#x2F;p&gt;
&lt;p class=&quot;mark&quot;&gt; What represents the &quot;64&quot; and &quot;20&quot; in torch.Size([64, 20])? &lt;&#x2F;p&gt;
&lt;p&gt;64 Refers to the number of images in the batch. Every batch is made of 64 images. Trying to select the 65th image (&lt;code&gt;outputs[64]&lt;&#x2F;code&gt;) will show an out-of-range error because a batch contains only 64 images.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;outputs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;---------------------------------------------------------------------------
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;IndexError                                Traceback &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;most recent call last&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;ipython&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;input&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;134&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;2751f6a48786&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;module&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;()
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;----&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1 &lt;&#x2F;span&gt;&lt;span&gt;outputs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;IndexError&lt;&#x2F;span&gt;&lt;span&gt;: index &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;is &lt;&#x2F;span&gt;&lt;span&gt;out of bounds &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;dimension 0 &lt;&#x2F;span&gt;&lt;span style=&quot;background-color:#6e2e32;color:#839496;&quot;&gt;with&lt;&#x2F;span&gt;&lt;span&gt; size 64
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The &quot;20&quot; are the number of categories or labels. It represents the last layer in the neural network. It has 20 nodes corresponding to the 20 different categories&#x2F;labels.&lt;&#x2F;p&gt;
&lt;p&gt;Now that we know the output of the model, we can apply to them a sigmoid transformation and the Binary Cross Entropy loss. We will take the first image of the batch &lt;code&gt;output[0]&lt;&#x2F;code&gt; and can call the &lt;code&gt;sigmoid()&lt;&#x2F;code&gt; method on it to see the difference in the results:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;outputs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.0786&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6746&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.7760&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.8992&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9360&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.1045&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.5859&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3760&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6101&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6136&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.0267&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5890&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2249&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5697&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4767&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2276&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2324&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.0516&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.7298&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1993&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;device&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;cuda:0&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;grad_fn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;SelectBackward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;outputs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sigmoid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5196&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6625&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.1448&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9478&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.7183&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.4739&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.0700&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.4071&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3520&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3512&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9538&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3569&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.4440&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.3613&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.1859&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5566&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5578&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.1139&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6748&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2316&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;device&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;cuda:0&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;grad_fn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&amp;lt;&lt;&#x2F;span&gt;&lt;span&gt;SelectBackward&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Notice that the sigmoid function transforms all the predictions of the model (outputs) into a &lt;strong&gt;range 0 to 1&lt;&#x2F;strong&gt;. This is very useful for Binary Cross Entropy loss as it requires every label to be either a 1 or a 0.&lt;&#x2F;p&gt;
&lt;p&gt;Remember that each of the 20 values of the &lt;code&gt;tensor&lt;&#x2F;code&gt; represents a label, and the number resulting from this transformation represents the probability of this label.&lt;&#x2F;p&gt;
&lt;p class=&quot;mark&quot;&gt; How do we select which predictions are 1s and which ones 0s? &lt;&#x2F;p&gt;
&lt;p&gt;The easiest solution is &lt;strong&gt;setting a threshold&lt;&#x2F;strong&gt;, a value, positive enough that we consider that the label is predicted. All the values more than this threshold are transformed to 1, or labels predicted.&lt;&#x2F;p&gt;
&lt;p&gt;For example, let&#x27;s take the last outputs in &lt;code&gt;outputs.sigmoid()[0]&lt;&#x2F;code&gt; above and set a threshold of 0.7. The label associated with the node with the value &lt;code&gt;0.9478&lt;&#x2F;code&gt; and &lt;code&gt;0.7183&lt;&#x2F;code&gt; are the predicted labels, for the 18 other labels are not activated as they are below the threshold.&lt;&#x2F;p&gt;
&lt;p&gt;Here we have shown the transformation for the first image - index 0 (&lt;code&gt;[0]&lt;&#x2F;code&gt;). In practice, we apply sigmoid for every batch of the model and select the values for Binary Cross Entropy into the same step as we see next.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Sigmoid Threshold Optimiazion&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The default threshold used for the Sigmoid transformation is 0.5. However, it can be other values as we saw in the last section setting 0.7. There is &lt;strong&gt;no way to see if the default value is a good threshold before you try with several&lt;&#x2F;strong&gt; thresholds.&lt;&#x2F;p&gt;
&lt;p&gt;To test this, we will build the model with different thresholds and give them some epochs to see if the accuracy changes.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Deeper Model with more batches
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet50,
&lt;&#x2F;span&gt;&lt;span&gt;                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;partial&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;accuracy_multi, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;thresh &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Optimize the learning rate
&lt;&#x2F;span&gt;&lt;span&gt;lr_suggested &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;lr_find&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Freeze the first 5 epochs and run 5 epochs
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fine_tune&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;base_lr &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;lr_suggested, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;freeze_epochs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy_multi&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.988882&lt;&#x2F;td&gt;
      &lt;td&gt;0.733648&lt;&#x2F;td&gt;
      &lt;td&gt;0.200498&lt;&#x2F;td&gt;
      &lt;td&gt;00:40&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.897558&lt;&#x2F;td&gt;
      &lt;td&gt;0.651835&lt;&#x2F;td&gt;
      &lt;td&gt;0.226036&lt;&#x2F;td&gt;
      &lt;td&gt;00:40&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.797924&lt;&#x2F;td&gt;
      &lt;td&gt;0.555892&lt;&#x2F;td&gt;
      &lt;td&gt;0.264064&lt;&#x2F;td&gt;
      &lt;td&gt;00:40&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.654679&lt;&#x2F;td&gt;
      &lt;td&gt;0.331369&lt;&#x2F;td&gt;
      &lt;td&gt;0.504701&lt;&#x2F;td&gt;
      &lt;td&gt;00:40&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.454360&lt;&#x2F;td&gt;
      &lt;td&gt;0.168649&lt;&#x2F;td&gt;
      &lt;td&gt;0.888008&lt;&#x2F;td&gt;
      &lt;td&gt;00:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy_multi&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.192024&lt;&#x2F;td&gt;
      &lt;td&gt;0.137152&lt;&#x2F;td&gt;
      &lt;td&gt;0.931693&lt;&#x2F;td&gt;
      &lt;td&gt;00:45&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.164923&lt;&#x2F;td&gt;
      &lt;td&gt;0.118155&lt;&#x2F;td&gt;
      &lt;td&gt;0.942410&lt;&#x2F;td&gt;
      &lt;td&gt;00:46&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.139310&lt;&#x2F;td&gt;
      &lt;td&gt;0.108408&lt;&#x2F;td&gt;
      &lt;td&gt;0.952570&lt;&#x2F;td&gt;
      &lt;td&gt;00:46&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.118630&lt;&#x2F;td&gt;
      &lt;td&gt;0.106424&lt;&#x2F;td&gt;
      &lt;td&gt;0.950259&lt;&#x2F;td&gt;
      &lt;td&gt;00:45&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.104928&lt;&#x2F;td&gt;
      &lt;td&gt;0.105443&lt;&#x2F;td&gt;
      &lt;td&gt;0.952151&lt;&#x2F;td&gt;
      &lt;td&gt;00:46&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;Please note that instead of changing the entire model you can use &lt;code&gt;metrics&lt;&#x2F;code&gt; and &lt;code&gt;partial&lt;&#x2F;code&gt;. The sigmoid threshold only applies to the last layer of the neural network.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;partial&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;accuracy_multi, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;thresh &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;validate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;#2) [0.10544303804636002,0.9638046622276306]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Using &lt;code&gt;validate()&lt;&#x2F;code&gt; returns the validation loss (&lt;code&gt;valid_loss&lt;&#x2F;code&gt;) and the metrics loss (&lt;code&gt;accuracy_multi&lt;&#x2F;code&gt; in this case). A threshold of 0.5 produces a slightly better accuracy loss (0.964 vs previous 0.952)&lt;&#x2F;p&gt;
&lt;p&gt;As you can imagine, there must be a way to loop over several thresholds instead of trying all possible thresholds by hand.&lt;&#x2F;p&gt;
&lt;p&gt;To loop over different values we can make a batch of predictions using &lt;code&gt;get_preds&lt;&#x2F;code&gt; and use this batch of predictions to loop a range of possible thresholds and compare accuracy.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Batch of predictions
&lt;&#x2F;span&gt;&lt;span&gt;train, targs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_preds&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Possible sigmoid thresholds, from 0.05 to 0.95
&lt;&#x2F;span&gt;&lt;span&gt;thr &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;linspace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.05&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.95&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;29&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Accuracy loop
&lt;&#x2F;span&gt;&lt;span&gt;accs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;accuracy_multi&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;train, targs, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;thresh&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;i, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;sigmoid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;thr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# plot them
&lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;xs,accs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_69_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The x-axis denotes the threshold values, and the y-axis the accuracy values. We can see that a sigmoid threshold between 0.45 and 0.7 gives us around 0.96 accuracies in the validation set.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;get_preds()&lt;&#x2F;code&gt; apply by default sigmoid, so you will have to set &lt;code&gt;accuracy_multi(sigmoid=False)&lt;&#x2F;code&gt; in the model to not pass the transformation twice.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-2-regression&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-2-regression&quot; aria-label=&quot;Anchor link for: 6-2-regression&quot;&gt;6.2 Regression&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Regression&lt;&#x2F;strong&gt; is when your labels are one or several numbers - a quantity instead of a category.&lt;&#x2F;p&gt;
&lt;p&gt;Image regression refers to learning from a dataset in which the independent variable is an image or element, and &lt;strong&gt;the dependent variable is one or more floats&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Perhaps we have an independent variable that’s an image, and a dependent that’s text (e.g. generating a caption from an image); or perhaps we have an independent variable
that’s text and a dependent that’s an image (e.g. generating an image from a caption).&lt;&#x2F;p&gt;
&lt;p&gt;To illustrate this kind of model we’re going to do a &lt;strong&gt;key point model&lt;&#x2F;strong&gt;. A key point refers to a specific location represented in an image. So the input is face images, and the output should be a float with the coordinates of the center of the face.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Head Pose Dataset&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The data needs a little preprocessing and formating. The idea is the same as before, creating a function that points to the path of the data and create targets.&lt;&#x2F;p&gt;
&lt;p&gt;The path of the images is inside objects formatted as &lt;code&gt;obj&lt;&#x2F;code&gt;. The targets will be created with a function that calculates the center of the image. The model will try to predict the coordinates of the center of the image.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Load data
&lt;&#x2F;span&gt;&lt;span&gt;path &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;untar_data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;URLs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;BIWI_HEAD_POSE&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;div&gt;
    &lt;style&gt;
        &#x2F;* Turns off some styling *&#x2F;
        progress {
            &#x2F;* gets rid of default border in Firefox and Opera. *&#x2F;
            border: none;
            &#x2F;* Needs to be in here for Safari polyfill so background images work as expected. *&#x2F;
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    &lt;&#x2F;style&gt;
  &lt;progress value=&#x27;452321280&#x27; class=&#x27;&#x27; max=&#x27;452316199&#x27; style=&#x27;width:300px; height:20px; vertical-align: middle;&#x27;&gt;&lt;&#x2F;progress&gt;
  100.00% [452321280&#x2F;452316199 00:09&lt;00:00]
&lt;&#x2F;div&gt;
&lt;p&gt;The data is inside this objects &lt;code&gt;obj&lt;&#x2F;code&gt; and there are 24 objects.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;path.ls()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    (#50) [Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;14.obj&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;           Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;18&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;           Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;06.obj&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;           Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;io_sample.cpp&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;           ...]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Every object has 1000 images and labeled poses.&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;(path&#x2F;&amp;#39;01&amp;#39;).ls()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    (#1000) [Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;01&#x2F;frame_00307_pose.txt&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;             Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;01&#x2F;frame_00159_pose.txt&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;             Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;01&#x2F;frame_00363_pose.txt&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;             Path(&amp;#39;&#x2F;root&#x2F;.fastai&#x2F;data&#x2F;biwi_head_pose&#x2F;01&#x2F;frame_00434_pose.txt&amp;#39;),
&lt;&#x2F;span&gt;&lt;span&gt;             ...]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We will create the function &lt;code&gt;img2pose&lt;&#x2F;code&gt; to extract the pose path.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;img_files &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_image_files&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# write a function that converts an image filename
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;img2pose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;): &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;str&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;pose.txt&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now that we have the pose and the image path, we should have the images in &lt;em&gt;jpg&lt;&#x2F;em&gt; and the labels in &lt;em&gt;txt&lt;&#x2F;em&gt; format under the same identifier.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;img_files&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;img2pose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;img_files&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;root&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;.fastai&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;biwi_head_pose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;frame_00518_rgb.jpg
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;root&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;.fastai&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;biwi_head_pose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;frame_00518_pose.txt
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s take a look at an image.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;im &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;PILImage.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;create&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;img_files&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;im.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to_thumb&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_83_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We extract the center of the image creating a function that returns the coordinates as a tensor of two items. However, the details of the function are not important. Every dataset will require a different cleaning a formatting process.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;cal &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;genfromtxt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;01&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;rgb.cal&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;skip_footer&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_ctr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;  ctr &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;genfromtxt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;img2pose&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;f&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;skip_header&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;  c1 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;ctr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span&gt;cal&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;][&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;ctr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span&gt;cal&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;][&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;  c2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;ctr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;* &lt;&#x2F;span&gt;&lt;span&gt;cal&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;][&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&#x2F;&lt;&#x2F;span&gt;&lt;span&gt;ctr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span&gt;cal&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;][&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span&gt;c1,c2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# The center of the image is the label that we are trying to predict
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_ctr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;img_files&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;344.3451&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;330.0573&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;Building the DataBlock&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;biwi_data &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span&gt;ImageBlock, PointBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_items&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;get_image_files,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;get_ctr,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Splitter function that returns True for just one person,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# as we dont want to train with the same person all over and over.
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;splitter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;FuncSplitter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;lambda &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;o&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;: &lt;&#x2F;span&gt;&lt;span&gt;o.parent.name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;==&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;13&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Data augmentation and normalization
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;aug_transforms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;240&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;320&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                Normalize.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;from_stats&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(*&lt;&#x2F;span&gt;&lt;span&gt;imagenet_stats&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)]
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;biwi_data.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;dls.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_batch&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_89_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The input is the image, and the target is the red dots. The batch of data looks correct.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Modeling&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cnn_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, resnet18, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y_range&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;When coordinates are used as the dependent variable, most of the time we’re likely to be trying to predict something as close as possible, so we would like to use the MSE loss function. We can check the default loss function using &lt;code&gt;loss_func&lt;&#x2F;code&gt;:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.loss_func
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    FlattenedLoss of &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;MSELoss&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Fastai applied the loss function correctly. Let&#x27;s find a good learning rate and fit the model. You can use &lt;code&gt;one_cyle_fit&lt;&#x2F;code&gt; instead of &lt;code&gt;fine_tune&lt;&#x2F;code&gt; to save time using large learning rates (more &lt;a href=&quot;https:&#x2F;&#x2F;fastai1.fast.ai&#x2F;callbacks.one_cycle.html&quot;&gt;here&lt;&#x2F;a&gt; and &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1708.07120&quot;&gt;here&lt;&#x2F;a&gt;).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;lr_finder &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;lr_find&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fine_tune&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7&lt;&#x2F;span&gt;&lt;span&gt;, lr_finder&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.111715&lt;&#x2F;td&gt;
      &lt;td&gt;0.004949&lt;&#x2F;td&gt;
      &lt;td&gt;03:32&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;table border=&quot;0&quot; class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.009237&lt;&#x2F;td&gt;
      &lt;td&gt;0.001873&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.003953&lt;&#x2F;td&gt;
      &lt;td&gt;0.000574&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.002914&lt;&#x2F;td&gt;
      &lt;td&gt;0.000619&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.002445&lt;&#x2F;td&gt;
      &lt;td&gt;0.000372&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.001847&lt;&#x2F;td&gt;
      &lt;td&gt;0.000476&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;0.001449&lt;&#x2F;td&gt;
      &lt;td&gt;0.000187&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0.001440&lt;&#x2F;td&gt;
      &lt;td&gt;0.000143&lt;&#x2F;td&gt;
      &lt;td&gt;04:41&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;The predicted center points are quite close to the real center of the faces!&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show_results&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ds_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_n &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_100_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In problems that are at first glance completely different (single-label classification,
multi-label classification, and regression), we end up using the same model with just
different numbers of outputs. The loss function is the one thing that changes, which
is why it’s important to double-check that you are using the right loss function for
your problem using &lt;code&gt;loss_func&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;chapter-7-training-a-state-of-the-art-model&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#chapter-7-training-a-state-of-the-art-model&quot; aria-label=&quot;Anchor link for: chapter-7-training-a-state-of-the-art-model&quot;&gt;Chapter 7 - Training a State-of-the-Art Model&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;7-1-imagenette-dataset&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-1-imagenette-dataset&quot; aria-label=&quot;Anchor link for: 7-1-imagenette-dataset&quot;&gt;7.1 Imagenette Dataset&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Imagenette is a lighter version of the dataset ImageNet.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ImageNet&lt;&#x2F;strong&gt;: 1.3 million images of various sizes, around 500 pixels across, in 1,000 categories.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Imagenette&lt;&#x2F;strong&gt;: Smaller version of ImageNet that takes only 10 classes that looks very different from one another.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Trayining models using ImageNet took several hours so fastai created this lighter version. The philosophy behind is that you should aim to have an iteration speed of no more than a couple of minutes - that is, when you come up with a new idea you want to try out, you should be able to train a model and see how it goes within a couple of minutes.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Imagenette
&lt;&#x2F;span&gt;&lt;span&gt;path &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;untar_data&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;URLs.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;IMAGENETTE&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ImageBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_items&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;get_image_files,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;parent_label,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;460&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;aug_transforms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.75&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# bs indicates how many samples per batch to load
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;bs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;7-2-normalization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-2-normalization&quot; aria-label=&quot;Anchor link for: 7-2-normalization&quot;&gt;7.2 Normalization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;When training a model, it helps if your input data is normalized — that is, has a mean
of 0 and a standard deviation of 1. But most images and computer vision libraries use
values between 0 and 255 for pixels, or between 0 and 1; in either case, your data is
not going to have a mean of 0 and a standard deviation of 1.&lt;&#x2F;p&gt;
&lt;p&gt;To normalize the dat, you can add &lt;code&gt;batch_tfms&lt;&#x2F;code&gt; to the datablock to transform the mean andstandard deviation that you want to use.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dblock_norm &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ImageBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_items&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;get_image_files,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;parent_label,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;460&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;aug_transforms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.75&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Normalization
&lt;&#x2F;span&gt;&lt;span&gt;                                Normalize.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;from_stats&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(*&lt;&#x2F;span&gt;&lt;span&gt;imagenet_stats&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)])
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;dls_norm &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;dblock_norm.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;bs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s compare two models, one with normalized data and one without normalization. The baseline model is &lt;code&gt;xResNet50&lt;&#x2F;code&gt;. To keep it short, &lt;code&gt;xResNet50&lt;&#x2F;code&gt; is a twist of &lt;code&gt;ResNet50&lt;&#x2F;code&gt; that have shown favourable results when compared to other RestNets &lt;strong&gt;when training from scratch&lt;&#x2F;strong&gt;. For testing use &lt;code&gt;fit_one_cycle()&lt;&#x2F;code&gt; and not&lt;code&gt;fine_tune()&lt;&#x2F;code&gt;, as it faster.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Non-normalzied xRestNet50&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;xresnet50&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, model, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;loss_func &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CrossEntropyLossFlat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;accuracy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3e-3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table  class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1.639044&lt;&#x2F;td&gt;
      &lt;td&gt;7.565507&lt;&#x2F;td&gt;
      &lt;td&gt;0.211725&lt;&#x2F;td&gt;
      &lt;td&gt;02:20&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;1.264875&lt;&#x2F;td&gt;
      &lt;td&gt;1.688994&lt;&#x2F;td&gt;
      &lt;td&gt;0.523152&lt;&#x2F;td&gt;
      &lt;td&gt;02:16&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.961111&lt;&#x2F;td&gt;
      &lt;td&gt;1.115392&lt;&#x2F;td&gt;
      &lt;td&gt;0.664302&lt;&#x2F;td&gt;
      &lt;td&gt;02:17&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.717251&lt;&#x2F;td&gt;
      &lt;td&gt;0.651410&lt;&#x2F;td&gt;
      &lt;td&gt;0.789768&lt;&#x2F;td&gt;
      &lt;td&gt;02:22&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.589625&lt;&#x2F;td&gt;
      &lt;td&gt;0.550697&lt;&#x2F;td&gt;
      &lt;td&gt;0.825243&lt;&#x2F;td&gt;
      &lt;td&gt;02:16&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Normalized xRestNet50&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Normalized data
&lt;&#x2F;span&gt;&lt;span&gt;learn_norm &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls_norm, model, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;loss_func &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CrossEntropyLossFlat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;accuracy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn_norm.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3e-3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.817426&lt;&#x2F;td&gt;
      &lt;td&gt;1.625511&lt;&#x2F;td&gt;
      &lt;td&gt;0.572069&lt;&#x2F;td&gt;
      &lt;td&gt;02:17&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.790636&lt;&#x2F;td&gt;
      &lt;td&gt;1.329097&lt;&#x2F;td&gt;
      &lt;td&gt;0.592233&lt;&#x2F;td&gt;
      &lt;td&gt;02:15&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.671544&lt;&#x2F;td&gt;
      &lt;td&gt;0.681273&lt;&#x2F;td&gt;
      &lt;td&gt;0.781553&lt;&#x2F;td&gt;
      &lt;td&gt;02:17&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.501642&lt;&#x2F;td&gt;
      &lt;td&gt;0.431404&lt;&#x2F;td&gt;
      &lt;td&gt;0.864078&lt;&#x2F;td&gt;
      &lt;td&gt;02:15&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.395240&lt;&#x2F;td&gt;
      &lt;td&gt;0.387665&lt;&#x2F;td&gt;
      &lt;td&gt;0.875280&lt;&#x2F;td&gt;
      &lt;td&gt;02:17&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;&lt;strong&gt;Normalizing the data helped achive 4% to 5% more accuracy!&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Normalization is specially important in pre-trained models. If the model was trained with normalized data (pixels with mean 1 and standard deviation 1), then it will perform better if your data is also normalized. Matching the statistics is very important for transfer learning to work well.&lt;&#x2F;p&gt;
&lt;p&gt;The default behaviour in fastai &lt;code&gt;cnn_learner&lt;&#x2F;code&gt; is adding the proper &lt;code&gt;Normalize&lt;&#x2F;code&gt; function automatically, but you will have to add it manually when training models from scratch.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-3-progressive-resizing&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-3-progressive-resizing&quot; aria-label=&quot;Anchor link for: 7-3-progressive-resizing&quot;&gt;7.3 Progressive Resizing&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Progressive resizing is gradually using larger and larger images as you train the model.&lt;&#x2F;p&gt;
&lt;p&gt;Benefits:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Training complete much faster, as most of the epochs are used training small images.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;You will have better generalization of your models, as progressive resizing is just a method of data augmentation and therefore tend to improve external validity.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;How it works?&lt;&#x2F;p&gt;
&lt;p&gt;First, we create a &lt;code&gt;get_dls&lt;&#x2F;code&gt; function that calls the exactly same datablock that we made before, &lt;strong&gt;but with arguments for the size of the images and the size of the batch&lt;&#x2F;strong&gt; - so we can test different batch sizes.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_dls&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_size&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;image_size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;  dblock_norm &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;blocks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ImageBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CategoryBlock&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_items&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;get_image_files,
&lt;&#x2F;span&gt;&lt;span&gt;                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;get_y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;parent_label,
&lt;&#x2F;span&gt;&lt;span&gt;                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;item_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Resize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;460&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;batch_tfms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;*&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;aug_transforms&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;image_size, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.75&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                                  Normalize.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;from_stats&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(*&lt;&#x2F;span&gt;&lt;span&gt;imagenet_stats&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)])
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;dblock_norm.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;path, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;bs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;batch_size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s start with 128 batch of images of 128 pixels each:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_dls&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;128&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;128&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;xresnet50&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;loss_func&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CrossEntropyLossFlat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;accuracy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3e-3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1.859451&lt;&#x2F;td&gt;
      &lt;td&gt;2.136631&lt;&#x2F;td&gt;
      &lt;td&gt;0.392084&lt;&#x2F;td&gt;
      &lt;td&gt;01:14&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;1.297873&lt;&#x2F;td&gt;
      &lt;td&gt;1.321736&lt;&#x2F;td&gt;
      &lt;td&gt;0.585138&lt;&#x2F;td&gt;
      &lt;td&gt;01:12&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.979822&lt;&#x2F;td&gt;
      &lt;td&gt;0.863942&lt;&#x2F;td&gt;
      &lt;td&gt;0.723674&lt;&#x2F;td&gt;
      &lt;td&gt;01:12&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.761521&lt;&#x2F;td&gt;
      &lt;td&gt;0.687464&lt;&#x2F;td&gt;
      &lt;td&gt;0.781927&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;As with transfered learning, we take the model and we train it 5 more batches with 64 more images but this time with a larger size of 224 pixels:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_dls&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;224&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fine_tune&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1e-3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.863330&lt;&#x2F;td&gt;
      &lt;td&gt;1.115129&lt;&#x2F;td&gt;
      &lt;td&gt;0.645631&lt;&#x2F;td&gt;
      &lt;td&gt;02:16&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.677025&lt;&#x2F;td&gt;
      &lt;td&gt;0.756777&lt;&#x2F;td&gt;
      &lt;td&gt;0.762136&lt;&#x2F;td&gt;
      &lt;td&gt;02:15&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.659812&lt;&#x2F;td&gt;
      &lt;td&gt;0.931320&lt;&#x2F;td&gt;
      &lt;td&gt;0.712099&lt;&#x2F;td&gt;
      &lt;td&gt;02:15&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.592581&lt;&#x2F;td&gt;
      &lt;td&gt;0.682786&lt;&#x2F;td&gt;
      &lt;td&gt;0.775579&lt;&#x2F;td&gt;
      &lt;td&gt;02:15&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.481050&lt;&#x2F;td&gt;
      &lt;td&gt;0.454066&lt;&#x2F;td&gt;
      &lt;td&gt;0.855863&lt;&#x2F;td&gt;
      &lt;td&gt;02:17&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.427033&lt;&#x2F;td&gt;
      &lt;td&gt;0.425391&lt;&#x2F;td&gt;
      &lt;td&gt;0.868185&lt;&#x2F;td&gt;
      &lt;td&gt;02:23&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;Pregressive resizing can be done at more epochs and for as big an image as you wish, but notice that you will not get any benefit by using an image size larger that the size of the images.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-4-test-time-augmentation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-4-test-time-augmentation&quot; aria-label=&quot;Anchor link for: 7-4-test-time-augmentation&quot;&gt;7.4 Test Time augmentation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We have been using random cropping as a way to get some useful data augmentation,
which leads to better generalization, and results in a need for less training data. When
we use random cropping, fastai will automatically use center-cropping for the validation
set — that is, it will select the largest square area it can in the center of the image,
without going past the image’s edges.&lt;&#x2F;p&gt;
&lt;p&gt;This can often be problematic. For instance, in a multi-label dataset, sometimes there
are small objects toward the edges of an image; these could be entirely cropped out by
center cropping.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;Squishing&lt;&#x2F;em&gt; could be a solution but also can make the image recognition
more difficult for our model. It has to learn how to recognize squished and
squeezed images, rather than just correctly proportioned images.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Test Time Augmentation (TTA)&lt;&#x2F;strong&gt; is a method that instead of centering or squishing, takes a number of
areas to crop from the original rectangular image, pass each of them through our
model, and take the maximum or average of the predictions.&lt;&#x2F;p&gt;
&lt;p&gt;It does not change the time required to train at all, but will
increase the amount of time required for validation or inference by the number of
test-time-augmented images requested. By default, fastai will use the unaugmented
center crop image plus four randomly augmented images&lt;&#x2F;p&gt;
&lt;p&gt;To use it, pass the DataLoader to fastai’s &lt;code&gt;tta&lt;&#x2F;code&gt; method; by default, it will crop your validation set - you just have to store the &quot;new validation set&quot; in a variable.&lt;&#x2F;p&gt;
&lt;p&gt;Run it to observe the output shape:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tta&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TensorBase&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.3654e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1131e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.8078e-05&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.0065e-09&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.8123e-08&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.7091e-08&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.8131e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.0205e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.8520e-03&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0132e-11&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.4396e-12&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.2754e-11&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.4551e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.6013e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.6602e-03&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.2817e-09&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.7115e-09&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.0039e-09&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.5209e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.8668e-01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.5150e-07&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.3289e-11&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.2414e-11&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.5075e-12&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.9031e-01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.3725e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.4502e-04&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.1489e-11&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.6372e-11&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.8058e-11&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1344e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.2957e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.8214e-01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0300e-11&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.2358e-11&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.7416e-11&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]])&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TensorCategory&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The outputs are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The validation set (after this &quot;random average cropping&quot; technique), and&lt;&#x2F;li&gt;
&lt;li&gt;The real labels&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Notice that the model do not have to be retrained because &lt;strong&gt;we don&#x27;t use the validation set in the training phase&lt;&#x2F;strong&gt;. We only take cropping averages of the images in the validation set, so the model doesn&#x27;t change.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;preds, targs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tta&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;accuracy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;preds, targs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;item&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.869305431842804
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;TTA gives a little boost in performance (~1%) - taking into account that it doesn&#x27;t require additional model training.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;However, it does make inference slower. For example, if you’re averaging five images for TTA inference will be five times slower.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-5-mixup&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-5-mixup&quot; aria-label=&quot;Anchor link for: 7-5-mixup&quot;&gt;7.5 Mixup&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Mixup is a powerful data augmentation technique that &lt;strong&gt;can provide
dramatically higher accuracy, especially when you don’t have much data&lt;&#x2F;strong&gt; and don’t
have a pretrained model that was trained on data similar to your dataset&lt;&#x2F;p&gt;
&lt;p&gt;Mixup is a technique that uses the weighted average of random images to improve the accuracy of the model. It iterates through the images in the dataset to combine:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;The pixel and label values of each image with;&lt;&#x2F;li&gt;
&lt;li&gt;The pixel and label values of a random image.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;For example, the following image is a mixup of a church with a gas station image:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;i.postimg.cc&#x2F;kG6kLbJ5&#x2F;fig1.png&quot; alt=&quot;Mixing a church and a gas station&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The constructed image is a &lt;strong&gt;linear combination of the first and the second images&lt;&#x2F;strong&gt; - like a linear regresion in which the dependent variable is the mixup image and the dependent variables the 2 images. It is built by adding 0.3 times the first one and 0.7 times the second.&lt;&#x2F;p&gt;
&lt;p&gt;In this example, should the model predict “church” or “gas station”?&lt;&#x2F;p&gt;
&lt;p&gt;The right answer is 30% church and 70% gas station, since that’s what we’ll get if we take the linear combination
of the one-hot-encoded targets.&lt;&#x2F;p&gt;
&lt;p&gt;For instance, suppose we have 10 classes,
and “church” is represented by the index 2 and “gas station” by the index 7. The onehot-
encoded representations are as follows:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;[0, 0, 1, 0, 0, 0, 0, 0, 0, 0] and [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;So here is our final target:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;[0, 0, 0.3, 0, 0, 0, 0, 0.7, 0, 0]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;div class=&quot;alert alert-block alert-info&quot;&gt; Notice that for Mixup to work, our targets need to be one-hot encoded. &lt;&#x2F;div&gt;
&lt;p&gt;Here is how we train a model with Mixup:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;xresnet50&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, model, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;loss_func&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CrossEntropyLossFlat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;accuracy,
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Mixup!
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;cbs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;MixUp&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;46&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3e-3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;2.328936&lt;&#x2F;td&gt;
      &lt;td&gt;1.526767&lt;&#x2F;td&gt;
      &lt;td&gt;0.511576&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;1.774001&lt;&#x2F;td&gt;
      &lt;td&gt;1.380210&lt;&#x2F;td&gt;
      &lt;td&gt;0.552651&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;1.623476&lt;&#x2F;td&gt;
      &lt;td&gt;1.196524&lt;&#x2F;td&gt;
      &lt;td&gt;0.612397&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1.564727&lt;&#x2F;td&gt;
      &lt;td&gt;1.234234&lt;&#x2F;td&gt;
      &lt;td&gt;0.609783&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1.564727&lt;&#x2F;td&gt;
      &lt;td&gt;1.234234&lt;&#x2F;td&gt;
      &lt;td&gt;0.609783&lt;&#x2F;td&gt;
      &lt;td&gt;01:11&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;29&lt;&#x2F;td&gt;
      &lt;td&gt;0.862966&lt;&#x2F;td&gt;
      &lt;td&gt;0.427176&lt;&#x2F;td&gt;
      &lt;td&gt;0.874160&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30&lt;&#x2F;td&gt;
      &lt;td&gt;0.856436&lt;&#x2F;td&gt;
      &lt;td&gt;0.375472&lt;&#x2F;td&gt;
      &lt;td&gt;0.889096&lt;&#x2F;td&gt;
      &lt;td&gt;01:09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
      &lt;td&gt;[...]&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
      &lt;td&gt;46&lt;&#x2F;td&gt;
      &lt;td&gt;0.714792&lt;&#x2F;td&gt;
      &lt;td&gt;0.288479&lt;&#x2F;td&gt;
      &lt;td&gt;0.922704&lt;&#x2F;td&gt;
      &lt;td&gt;01:08&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;&lt;strong&gt;Mixup requires far more epochs to train to get better accuracy&lt;&#x2F;strong&gt;, compared with other models.&lt;&#x2F;p&gt;
&lt;p&gt;With normalization, we reached 87% accuracy after 5 epochs, while by using mixup we needed 29.&lt;&#x2F;p&gt;
&lt;p&gt;The model is harder to train, because it’s harder to see what’s in each image. And the
model has to predict two labels per image, rather than just one, as well as figuring out
how much each one is weighted.&lt;&#x2F;p&gt;
&lt;p&gt;Overfitting seems less likely to be a problem, however,
because we’re not showing the same image in each epoch, but are instead showing
a random combination of two images.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-6-label-smoothing&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-6-label-smoothing&quot; aria-label=&quot;Anchor link for: 7-6-label-smoothing&quot;&gt;7.6 Label Smoothing&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;ML models optimize for the metric that you select. If the metric is accuracy, the model search for the maximum accuracy - minimazing the loss function by SGD.&lt;&#x2F;p&gt;
&lt;p&gt;The optimization process, in practice, tells the model to return 0 for all categories but one, for which it is trained to return 1. Even 0.999 is not “good enough”; the model will get gradients and learn to predict activations with even higher confidence. This can become very harmful if your data is not perfectly labeled, and it never is in real life scenarios.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Label smoothing&lt;&#x2F;strong&gt; replace all the 1 with a number a bit less than 1, and the 0s with a number a bit more than 0. When you train the model, the model doesn&#x27;t have to be 100% sure that it found the correct label - with 99% is good enough.&lt;&#x2F;p&gt;
&lt;p&gt;For example, for a 10 class classification problem (Imagenette) with the correct label in the index 3:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;[0.01, 0.01, 0.01, 0.91, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Label smoothing can be incorporated in the &lt;code&gt;loss_func&lt;&#x2F;code&gt; argument: &lt;code&gt;loss_func=LabelSmoothingCrossEntropy()&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;xresnet50&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, model, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;loss_func&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;LabelSmoothingCrossEntropy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;metrics&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;accuracy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3e-3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;accuracy&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;2.512356&lt;&#x2F;td&gt;
      &lt;td&gt;2.483313&lt;&#x2F;td&gt;
      &lt;td&gt;0.449216&lt;&#x2F;td&gt;
      &lt;td&gt;02:24&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;2.120067&lt;&#x2F;td&gt;
      &lt;td&gt;2.909898&lt;&#x2F;td&gt;
      &lt;td&gt;0.462659&lt;&#x2F;td&gt;
      &lt;td&gt;02:24&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;1.868167&lt;&#x2F;td&gt;
      &lt;td&gt;1.840382&lt;&#x2F;td&gt;
      &lt;td&gt;0.730769&lt;&#x2F;td&gt;
      &lt;td&gt;02:28&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1.704343&lt;&#x2F;td&gt;
      &lt;td&gt;1.646435&lt;&#x2F;td&gt;
      &lt;td&gt;0.801344&lt;&#x2F;td&gt;
      &lt;td&gt;02:28&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;1.598507&lt;&#x2F;td&gt;
      &lt;td&gt;1.552380&lt;&#x2F;td&gt;
      &lt;td&gt;0.827110&lt;&#x2F;td&gt;
      &lt;td&gt;02:28&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;As with Mixup, you won’t generally see significant improvements from label smoothing
until you train more epochs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;chapter-9-tabular-modeling-deep-dive&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#chapter-9-tabular-modeling-deep-dive&quot; aria-label=&quot;Anchor link for: chapter-9-tabular-modeling-deep-dive&quot;&gt;Chapter 9 - Tabular Modeling Deep Dive&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;For this Chapter we will use more than the &lt;code&gt;fastai&lt;&#x2F;code&gt; package so I let below the necessary imports:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;torch
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;pandas &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;pd
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;numpy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;np
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;matplotlib.pyplot &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;plt
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;seaborn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;sns
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;IPython
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;graphviz
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;dtreeviz.trees &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;scipy.cluster &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;hierarchy &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;hc
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn.model_selection &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;train_test_split,
&lt;&#x2F;span&gt;&lt;span&gt;                                    cross_val_score
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn.tree &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;DecisionTreeRegressor,
&lt;&#x2F;span&gt;&lt;span&gt;                         DecisionTreeClassifier,
&lt;&#x2F;span&gt;&lt;span&gt;                         export_graphviz
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn.ensemble &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;BaggingClassifier,
&lt;&#x2F;span&gt;&lt;span&gt;                             RandomForestClassifier,
&lt;&#x2F;span&gt;&lt;span&gt;                             BaggingRegressor,
&lt;&#x2F;span&gt;&lt;span&gt;                             RandomForestRegressor,
&lt;&#x2F;span&gt;&lt;span&gt;                             GradientBoostingRegressor
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn.metrics &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;mean_squared_error,
&lt;&#x2F;span&gt;&lt;span&gt;                            confusion_matrix,
&lt;&#x2F;span&gt;&lt;span&gt;                            classification_report
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;fastai.tabular.all &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;*
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;plt.style.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;use&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;seaborn-white&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;warnings
&lt;&#x2F;span&gt;&lt;span&gt;warnings.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;filterwarnings&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ignore&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;torch.cuda.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_device_name&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;if &lt;&#x2F;span&gt;&lt;span&gt;torch.cuda.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;is_available&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;() &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;else &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;None
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;x&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;      Tesla T4
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Tabular modeling takes data in the form of a table (like a spreadsheet or a CSV). The
objective is to predict the value in one column based on the values in the other columns.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-1-beyond-deep-learning&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-1-beyond-deep-learning&quot; aria-label=&quot;Anchor link for: 9-1-beyond-deep-learning&quot;&gt;9.1 Beyond Deep Learning&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;So far, the solution to all of our modeling problems has been to train a deep learning model. And indeed, that is a pretty good rule of thumb for complex unstructured data like images, sounds, natural language text, and so forth.&lt;&#x2F;p&gt;
&lt;p&gt;Deep learning also works very well for collaborative filtering. But it is not always the best starting point for analyzing tabular data.&lt;&#x2F;p&gt;
&lt;p&gt;Although deep learning is nearly always clearly superior for unstructured data, Ensembles of decision trees tend to give &lt;strong&gt;quite similar results for many kinds of structured data&lt;&#x2F;strong&gt;. Also, they train faster, are often easier to interpret, do not require special GPU hardware, and require less hyperparameter tuning.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-2-the-dataset&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-2-the-dataset&quot; aria-label=&quot;Anchor link for: 9-2-the-dataset&quot;&gt;9.2 The Dataset&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The dataset we use in this chapter is from the Blue Book for Bulldozers Kaggle competition, which has the following description:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&quot;The goal of the contest is to predict the sale price of a particular piece of heavy equipment at auction based on its usage, equipment type, and configuration.&quot;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;read_csv&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;&#x2F;home&#x2F;studio-lab-user&#x2F;sagemaker-studiolab-notebooks&#x2F;TrainAndValid.csv&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;low_memory&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;head&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr &gt;
      &lt;th&gt;&lt;&#x2F;th&gt;
      &lt;th&gt;SalesID&lt;&#x2F;th&gt;
      &lt;th&gt;SalePrice&lt;&#x2F;th&gt;
      &lt;th&gt;MachineID&lt;&#x2F;th&gt;
      &lt;th&gt;ModelID&lt;&#x2F;th&gt;
      &lt;th&gt;datasource&lt;&#x2F;th&gt;
      &lt;th&gt;auctioneerID&lt;&#x2F;th&gt;
      &lt;th&gt;YearMade&lt;&#x2F;th&gt;
      &lt;th&gt;MachineHoursCurrentMeter&lt;&#x2F;th&gt;
      &lt;th&gt;UsageBand&lt;&#x2F;th&gt;
      &lt;th&gt;saledate&lt;&#x2F;th&gt;
      &lt;th&gt;...&lt;&#x2F;th&gt;
      &lt;th&gt;Undercarriage_Pad_Width&lt;&#x2F;th&gt;
      &lt;th&gt;Stick_Length&lt;&#x2F;th&gt;
      &lt;th&gt;Thumb&lt;&#x2F;th&gt;
      &lt;th&gt;Pattern_Changer&lt;&#x2F;th&gt;
      &lt;th&gt;Grouser_Type&lt;&#x2F;th&gt;
      &lt;th&gt;Backhoe_Mounting&lt;&#x2F;th&gt;
      &lt;th&gt;Blade_Type&lt;&#x2F;th&gt;
      &lt;th&gt;Travel_Controls&lt;&#x2F;th&gt;
      &lt;th&gt;Differential_Type&lt;&#x2F;th&gt;
      &lt;th&gt;Steering_Controls&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;&#x2F;th&gt;
      &lt;td&gt;1139246&lt;&#x2F;td&gt;
      &lt;td&gt;66000.0&lt;&#x2F;td&gt;
      &lt;td&gt;999089&lt;&#x2F;td&gt;
      &lt;td&gt;3157&lt;&#x2F;td&gt;
      &lt;td&gt;121&lt;&#x2F;td&gt;
      &lt;td&gt;3.0&lt;&#x2F;td&gt;
      &lt;td&gt;2004&lt;&#x2F;td&gt;
      &lt;td&gt;68.0&lt;&#x2F;td&gt;
      &lt;td&gt;Low&lt;&#x2F;td&gt;
      &lt;td&gt;11&#x2F;16&#x2F;2006 0:00&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;Standard&lt;&#x2F;td&gt;
      &lt;td&gt;Conventional&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;&#x2F;th&gt;
      &lt;td&gt;1139248&lt;&#x2F;td&gt;
      &lt;td&gt;57000.0&lt;&#x2F;td&gt;
      &lt;td&gt;117657&lt;&#x2F;td&gt;
      &lt;td&gt;77&lt;&#x2F;td&gt;
      &lt;td&gt;121&lt;&#x2F;td&gt;
      &lt;td&gt;3.0&lt;&#x2F;td&gt;
      &lt;td&gt;1996&lt;&#x2F;td&gt;
      &lt;td&gt;4640.0&lt;&#x2F;td&gt;
      &lt;td&gt;Low&lt;&#x2F;td&gt;
      &lt;td&gt;3&#x2F;26&#x2F;2004 0:00&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;Standard&lt;&#x2F;td&gt;
      &lt;td&gt;Conventional&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;&#x2F;th&gt;
      &lt;td&gt;1139249&lt;&#x2F;td&gt;
      &lt;td&gt;10000.0&lt;&#x2F;td&gt;
      &lt;td&gt;434808&lt;&#x2F;td&gt;
      &lt;td&gt;7009&lt;&#x2F;td&gt;
      &lt;td&gt;121&lt;&#x2F;td&gt;
      &lt;td&gt;3.0&lt;&#x2F;td&gt;
      &lt;td&gt;2001&lt;&#x2F;td&gt;
      &lt;td&gt;2838.0&lt;&#x2F;td&gt;
      &lt;td&gt;High&lt;&#x2F;td&gt;
      &lt;td&gt;2&#x2F;26&#x2F;2004 0:00&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;&#x2F;th&gt;
      &lt;td&gt;1139251&lt;&#x2F;td&gt;
      &lt;td&gt;38500.0&lt;&#x2F;td&gt;
      &lt;td&gt;1026470&lt;&#x2F;td&gt;
      &lt;td&gt;332&lt;&#x2F;td&gt;
      &lt;td&gt;121&lt;&#x2F;td&gt;
      &lt;td&gt;3.0&lt;&#x2F;td&gt;
      &lt;td&gt;2001&lt;&#x2F;td&gt;
      &lt;td&gt;3486.0&lt;&#x2F;td&gt;
      &lt;td&gt;High&lt;&#x2F;td&gt;
      &lt;td&gt;5&#x2F;19&#x2F;2011 0:00&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;&#x2F;th&gt;
      &lt;td&gt;1139253&lt;&#x2F;td&gt;
      &lt;td&gt;11000.0&lt;&#x2F;td&gt;
      &lt;td&gt;1057373&lt;&#x2F;td&gt;
      &lt;td&gt;17311&lt;&#x2F;td&gt;
      &lt;td&gt;121&lt;&#x2F;td&gt;
      &lt;td&gt;3.0&lt;&#x2F;td&gt;
      &lt;td&gt;2007&lt;&#x2F;td&gt;
      &lt;td&gt;722.0&lt;&#x2F;td&gt;
      &lt;td&gt;Medium&lt;&#x2F;td&gt;
      &lt;td&gt;7&#x2F;23&#x2F;2009 0:00&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;5 rows × 53 columns&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;metric&lt;&#x2F;strong&gt; selected to evaluate the model is the &lt;strong&gt;root mean squared log error (RMLSE)&lt;&#x2F;strong&gt; between the actual and predicted auction prices. We are going to transform the sales price column into a logarithm, so when we apply the RMSE, it is already taking the logarithm into account.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;log&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;9-3-categorical-embeddings&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-3-categorical-embeddings&quot; aria-label=&quot;Anchor link for: 9-3-categorical-embeddings&quot;&gt;9.3 Categorical Embeddings&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Categorical embeddings transforms the categorical variables into inputs that are both continuous and meaningful. Clustering or ordening different categories is important because models are better at understanding continuous variables.&lt;&#x2F;p&gt;
&lt;p&gt;This is unsurprising considering models are built of many continuous parameter weights and continuous activation values, which are updated via gradient descent.&lt;&#x2F;p&gt;
&lt;p&gt;Categorical embedding also:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Reduces memory usage and speeds up neural networks compared with one-hot encoding.&lt;&#x2F;li&gt;
&lt;li&gt;Reveals the intrinsic properties of the categorical variables - increasing their predictive power.&lt;&#x2F;li&gt;
&lt;li&gt;It can be used for visualizing categorical data and for data clustering. The model learns an embedding for these entities that defines a continuous
notion of distance between them.&lt;&#x2F;li&gt;
&lt;li&gt;Avoid overfitting. It is especially useful for datasets with lots of high cardinality features, where other methods tend to overfit.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;We will start by embedding the &quot;Product Size&quot; variable, giving it it&#x27;s natural order:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;unique&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span&gt;nan, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Small&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large &#x2F; Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Mini&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;           &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Compact&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.dtype
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;O&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Order
&lt;&#x2F;span&gt;&lt;span&gt;sizes &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large &#x2F; Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Small&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Mini&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Compact&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;astype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;category&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.cat.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;set_categories&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;sizes, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ordered&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.dtype
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;CategoricalDtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;categories&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large &#x2F; Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Small&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Mini&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;                      &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Compact&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    , &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ordered&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;It is not needed to do hot-encoding&lt;&#x2F;strong&gt;. For binary classification
and regression, it was shown that ordering the predictor categories in each
split leads to exactly the same splits as the standard approach. This reduces computational
complexity because only k − 1 splits have to be considered for a nominal predictor
with k categories&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-4-feature-engineering-dates&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-4-feature-engineering-dates&quot; aria-label=&quot;Anchor link for: 9-4-feature-engineering-dates&quot;&gt;9.4 Feature Engineering: Dates&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The fundamental basis of the decision tree is &lt;strong&gt;bisection&lt;&#x2F;strong&gt; — dividing a group into two.&lt;&#x2F;p&gt;
&lt;p&gt;We look at the ordinal variables and divide the dataset based on whether the variable’s value is greater (or lower) than a threshold, and we look at the categorical variables and divide the dataset based on whether the variable’s level is a particular level. So this algorithm has a way of dividing the dataset based on both ordinal and categorical data.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;But how does this apply to a common data type, the date?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We might want our model to make decisions based on that date’s day of the week, on whether a day is a holiday, on what month it is in, and so forth. fastai comes with a function that will do this for us: &lt;code&gt;add_datepart&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;add_datepart&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saledate&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Last 15 columns, now we added more feature columns based on the day
&lt;&#x2F;span&gt;&lt;span&gt;df.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sample&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.iloc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;:,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr &gt;
      &lt;th&gt;&lt;&#x2F;th&gt;
      &lt;th&gt;Differential_Type&lt;&#x2F;th&gt;
      &lt;th&gt;Steering_Controls&lt;&#x2F;th&gt;
      &lt;th&gt;saleYear&lt;&#x2F;th&gt;
      &lt;th&gt;saleMonth&lt;&#x2F;th&gt;
      &lt;th&gt;saleWeek&lt;&#x2F;th&gt;
      &lt;th&gt;saleDay&lt;&#x2F;th&gt;
      &lt;th&gt;saleDayofweek&lt;&#x2F;th&gt;
      &lt;th&gt;saleDayofyear&lt;&#x2F;th&gt;
      &lt;th&gt;saleIs_month_end&lt;&#x2F;th&gt;
      &lt;th&gt;saleIs_month_start&lt;&#x2F;th&gt;
      &lt;th&gt;saleIs_quarter_end&lt;&#x2F;th&gt;
      &lt;th&gt;saleIs_quarter_start&lt;&#x2F;th&gt;
      &lt;th&gt;saleIs_year_end&lt;&#x2F;th&gt;
      &lt;th&gt;saleIs_year_start&lt;&#x2F;th&gt;
      &lt;th&gt;saleElapsed&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;295937&lt;&#x2F;th&gt;
      &lt;td&gt;Standard&lt;&#x2F;td&gt;
      &lt;td&gt;Conventional&lt;&#x2F;td&gt;
      &lt;td&gt;2007&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;16&lt;&#x2F;td&gt;
      &lt;td&gt;18&lt;&#x2F;td&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;1.176854e+09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;177280&lt;&#x2F;th&gt;
      &lt;td&gt;Standard&lt;&#x2F;td&gt;
      &lt;td&gt;Conventional&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;12&lt;&#x2F;td&gt;
      &lt;td&gt;21&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;80&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;1.111363e+09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;198868&lt;&#x2F;th&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;2007&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;27&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;86&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;1.174954e+09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;55758&lt;&#x2F;th&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;1991&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;21&lt;&#x2F;td&gt;
      &lt;td&gt;21&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;141&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;6.747840e+08&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;154301&lt;&#x2F;th&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;8&lt;&#x2F;td&gt;
      &lt;td&gt;23&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;54&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;False&lt;&#x2F;td&gt;
      &lt;td&gt;1.140653e+09&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;h3 id=&quot;9-5-using-tabularpandas-and-tabularproc&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-5-using-tabularpandas-and-tabularproc&quot; aria-label=&quot;Anchor link for: 9-5-using-tabularpandas-and-tabularproc&quot;&gt;9.5 Using TabularPandas and TabularProc&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;A second piece of preparatory processing is to be sure we can handle strings and missing data. fastai includes &lt;code&gt;Categorify&lt;&#x2F;code&gt; for the fists and &lt;code&gt;FillMissing&lt;&#x2F;code&gt; for the second.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Categorify&lt;&#x2F;code&gt; is a TabularProc that replaces a column value with a numeric categorical transformation levels chosen consecutively as they are seen in a column.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;FillMissing&lt;&#x2F;code&gt; is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;procs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span&gt;Categorify, FillMissing&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The Kaggle training data ends in April 2012, so we will define a narrower
training dataset that consists only of the Kaggle training data from before November
2011, and we’ll define a validation set consisting of data from after November 2011.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;cond &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (&lt;&#x2F;span&gt;&lt;span&gt;df.saleYear &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2011&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) | (&lt;&#x2F;span&gt;&lt;span&gt;df.saleMonth&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;train_idx &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;where&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;cond&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;valid_idx &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;where&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(~&lt;&#x2F;span&gt;&lt;span&gt;cond&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;splits &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;list&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;train_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;list&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;valid_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;TabularPandas needs to be told which columns are continuous and which are categorical&lt;&#x2F;strong&gt;. We can handle that automatically using the helper function cont_cat_split:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;cont, cat &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cont_cat_split&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dep_var&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;to &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TabularPandas&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;procs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;procs,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;cat_names&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;cat,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;cont_names&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;cont,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y_names&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;                   &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;splits&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;splits&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;to.train&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;to.valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;404710&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7988&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Fastai &lt;code&gt;TabularPandas&lt;&#x2F;code&gt; helps pre-processing the data. The following table is the first items of the orginal dataset:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;head&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;UsageBand&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiBaseModel&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiSecondaryDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelSeries&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr &gt;
      &lt;th&gt;&lt;&#x2F;th&gt;
      &lt;th&gt;UsageBand&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiBaseModel&lt;&#x2F;th&gt;
      &lt;th&gt;fiSecondaryDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelSeries&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;&#x2F;th&gt;
      &lt;td&gt;Low&lt;&#x2F;td&gt;
      &lt;td&gt;521D&lt;&#x2F;td&gt;
      &lt;td&gt;521&lt;&#x2F;td&gt;
      &lt;td&gt;D&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;&#x2F;th&gt;
      &lt;td&gt;Low&lt;&#x2F;td&gt;
      &lt;td&gt;950FII&lt;&#x2F;td&gt;
      &lt;td&gt;950&lt;&#x2F;td&gt;
      &lt;td&gt;F&lt;&#x2F;td&gt;
      &lt;td&gt;II&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;&#x2F;th&gt;
      &lt;td&gt;High&lt;&#x2F;td&gt;
      &lt;td&gt;226&lt;&#x2F;td&gt;
      &lt;td&gt;226&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;&#x2F;th&gt;
      &lt;td&gt;High&lt;&#x2F;td&gt;
      &lt;td&gt;PC120-6E&lt;&#x2F;td&gt;
      &lt;td&gt;PC120&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;-6E&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;&#x2F;th&gt;
      &lt;td&gt;Medium&lt;&#x2F;td&gt;
      &lt;td&gt;S175&lt;&#x2F;td&gt;
      &lt;td&gt;S175&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
      &lt;td&gt;NaN&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;And this is how &lt;code&gt;to&lt;&#x2F;code&gt; dataframe looks afert the transformation:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Numerical version of the columns
&lt;&#x2F;span&gt;&lt;span&gt;to.items.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;head&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;UsageBand&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiBaseModel&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiSecondaryDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelSeries&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr &gt;
      &lt;th&gt;&lt;&#x2F;th&gt;
      &lt;th&gt;UsageBand&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiBaseModel&lt;&#x2F;th&gt;
      &lt;th&gt;fiSecondaryDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelSeries&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;&#x2F;th&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;963&lt;&#x2F;td&gt;
      &lt;td&gt;298&lt;&#x2F;td&gt;
      &lt;td&gt;43&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;&#x2F;th&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;1745&lt;&#x2F;td&gt;
      &lt;td&gt;529&lt;&#x2F;td&gt;
      &lt;td&gt;57&lt;&#x2F;td&gt;
      &lt;td&gt;98&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;&#x2F;th&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;336&lt;&#x2F;td&gt;
      &lt;td&gt;111&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;&#x2F;th&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;3716&lt;&#x2F;td&gt;
      &lt;td&gt;1381&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;45&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;&#x2F;th&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;4261&lt;&#x2F;td&gt;
      &lt;td&gt;1538&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;The conversion of categorical columns to numbers is done by simply replacing each unique level with a number. &lt;strong&gt;The numbers associated with the levels are chosen consecutively as they are seen in a column&lt;&#x2F;strong&gt;, so there’s no particular meaning to the numbers in categorical columns after conversion.&lt;&#x2F;p&gt;
&lt;p&gt;The exception is if you first convert a column to a Pandas ordered category (as we did for ProductSize earlier), in which case the ordering you chose is used. We can see the mapping by looking at the classes attribute:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;unique&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;NaN, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Small&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large &#x2F; Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Mini&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Compact&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Categories &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large &#x2F; Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Small&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Mini&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt; &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Compact&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;to&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;unique&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;int8&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;to.classes&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;#na#&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Large &#x2F; Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Medium&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Small&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Mini&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Compact&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Save the progress
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;save_pickle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;to.pkl&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,to&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# To load progress:
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;#to = load_pickle(&amp;#39;to.pkl&amp;#39;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;9-7-decision-trees-avoiding-overfitting&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-7-decision-trees-avoiding-overfitting&quot; aria-label=&quot;Anchor link for: 9-7-decision-trees-avoiding-overfitting&quot;&gt;9.7 Decision Trees: Avoiding Overfitting&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To begin, we define our independent and dependent variables. The &lt;code&gt;TabularPandas&lt;&#x2F;code&gt; dataframe knows that the dependent variable is the sale price, because we specify it at &lt;code&gt;y_names=&#x27;SalePrice&#x27;&lt;&#x2F;code&gt; inside the transformation. It is also stored which rows are from the test and which rows are from the validation dataset as we set it by the &lt;code&gt;splits=splits&lt;&#x2F;code&gt; in which we splitted the data based on the condition &lt;code&gt;cond = (df.saleYear &amp;lt; 2011) | (df.saleMonth&amp;lt; 10)&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The arguments &lt;code&gt;xs&lt;&#x2F;code&gt;, &lt;code&gt;y&lt;&#x2F;code&gt;, and &lt;code&gt;train&lt;&#x2F;code&gt;, &lt;code&gt;valid&lt;&#x2F;code&gt; can be used to split the data accordingly - and very fast!&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# X train and y train
&lt;&#x2F;span&gt;&lt;span&gt;X, y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;to.train.xs, to.train.y
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# X valid and y valid
&lt;&#x2F;span&gt;&lt;span&gt;X_valid, y_valid &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;to.valid.xs, to.valid.y
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now that our data is all numeric, and there are no missing values, we can create a decision tree:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tree_model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DecisionTreeRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_leaf_nodes&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;tree_model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To keep it simple, we’ve told sklearn to create just four leaf nodes. To see what it’s
learned, we can display the tree:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;draw_tree&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;t&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;df&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ratio&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;precision&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;**&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kwargs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;export_graphviz&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;t, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;out_file&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;None&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;feature_names&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;df.columns, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;filled&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;rounded&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;special_characters&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;rotate&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;precision&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;precision, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;**&lt;&#x2F;span&gt;&lt;span&gt;kwargs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;graphviz.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Source&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;re.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sub&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Tree {&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;f&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Tree &lt;&#x2F;span&gt;&lt;span style=&quot;color:#dc322f;&quot;&gt;{{&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt; size=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;; ratio=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span&gt;ratio&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, s&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;draw_tree&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;tree_model, X, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;leaves_parallel&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;precision&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_47_0.svg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We see the importance of bisection: only dividing the dataset based on the value of &lt;code&gt;Copler_System&lt;&#x2F;code&gt; predicts an average value of 9.21 versus 10.1. The deeper the model, the more questions it will be able to ask separating high-value from low-value auction results.&lt;&#x2F;p&gt;
&lt;p&gt;We will use the package &lt;code&gt;dtreeviz&lt;&#x2F;code&gt; to see the distribution of the tree leafs, and catch possible data quality issues.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Random sample of the data
&lt;&#x2F;span&gt;&lt;span&gt;samp_idx &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.random.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;permutation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;500&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Representation for decision tree visualization and model interpretation
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dtreeviz&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;tree_model,
&lt;&#x2F;span&gt;&lt;span&gt;         X.iloc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;samp_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;         y.iloc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;samp_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;         X.columns,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;fontname&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;DejaVu Sans&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;label_fontsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;orientation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;LR&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_49_0.svg&quot; alt=&quot;svg&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can clearly see that there’s a problem with our YearMade data: there are bulldozers made in the year 1000. Let’s replace it with 1950:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X.loc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1900&lt;&#x2F;span&gt;&lt;span&gt;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1950
&lt;&#x2F;span&gt;&lt;span&gt;X_valid.loc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;lt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1900&lt;&#x2F;span&gt;&lt;span&gt;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1950
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;That change makes the split much clearer in the tree visualization, even although it
doesn’t change the result of the model in any significant way. This is a great example
of how resilient decision trees are to data issues.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tree_model_2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DecisionTreeRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_leaf_nodes&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;tree_model_2.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dtreeviz&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;tree_model_2,
&lt;&#x2F;span&gt;&lt;span&gt;         X.iloc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;samp_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;         y.iloc&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;samp_idx&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;         X.columns,
&lt;&#x2F;span&gt;&lt;span&gt;         &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;         &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;fontname&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;DejaVu Sans&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;scale&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.6&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;label_fontsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;orientation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;LR&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_53_0.svg&quot; alt=&quot;svg&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We’ll create a little function to check the root mean squared error of our model
(m_rmse), since that’s how the competition was judged:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;r_mse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;a&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Formula: Root mean squared error between 2 values: a and b
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;math.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sqrt&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(((&lt;&#x2F;span&gt;&lt;span&gt;a&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;b&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)**&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;mean&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;m_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Model application: RMSE between the predictions of the model and the y
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;r_mse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Training RMSE: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;m_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model, X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Validation RMSE: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;m_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model, X_valid, y_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;To ilustrate overfitting, let the model create a tree model without a limit of leafs:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tree_model_3 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DecisionTreeRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;tree_model_3.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;tree_model_3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre style=&quot;background-color:#002b36;color:#839496;&quot;&gt;&lt;code&gt;&lt;span&gt;Training RMSE: 0.0
&lt;&#x2F;span&gt;&lt;span&gt;Validation RMSE: 0.332212
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The model perfectly predicts the price of the auctions on the training, but checking it on the validation sets it seems to be overfitting indeed.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tree_model_3.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_n_leaves&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;324565
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The model uses around 325k leafs for 400k datapoints - of course it is overfitting, we have nearly as many leaf nodes as data points.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s try a new model with at least 25 autions per leaf.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tree_model_4 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DecisionTreeRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_samples_leaf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;25&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;tree_model_4.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;tree_model_4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.211706
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.268875
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tree_model_4.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_n_leaves&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12400
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;9-8-random-forests&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-8-random-forests&quot; aria-label=&quot;Anchor link for: 9-8-random-forests&quot;&gt;9.8 Random Forests&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Random Forests are based on a process called &lt;em&gt;bagging&lt;&#x2F;em&gt;:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Randomly choose a subset of the rows of your data.&lt;&#x2F;li&gt;
&lt;li&gt;Train a model using this subset.&lt;&#x2F;li&gt;
&lt;li&gt;Save that model, and then return to step 1 a few times.&lt;&#x2F;li&gt;
&lt;li&gt;This will give you multiple trained models. To make a prediction, predict using all of the models, and then take the average of each of those model’s predictions.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Although each of the models trained on a subset of data will make more errors than a model trained on the full dataset, &lt;strong&gt;those errors will not be correlated with each other&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Different models will make different errors. The average of those errors, therefore, is
zero! So if we take the average of all of the models’ predictions, we should end up
with a prediction that gets closer and closer to the correct answer, the more models
we have.&lt;&#x2F;p&gt;
&lt;p&gt;In the following function:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;n_estimators&lt;&#x2F;code&gt; defines the number of trees we want.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;max_samples&lt;&#x2F;code&gt; defines how many rows to sample for training each tree.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;max_features&lt;&#x2F;code&gt; defines how many columns to sample at each split point (where 0.5 means “take half the total number of columns”).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;min_samples_leaf&lt;&#x2F;code&gt; specify when to stop splitting the tree nodes, effectively limiting the depth of the tree.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;n_jobs=-1&lt;&#x2F;code&gt; tells sklearn to use all our CPUs to build the trees in parallel.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;n_estimators&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_samples&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;200_000&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_features&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_samples_leaf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;**&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kwargs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RandomForestRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;n_jobs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;n_estimators&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;n_estimators,
&lt;&#x2F;span&gt;&lt;span&gt;                                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_samples&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;max_samples,
&lt;&#x2F;span&gt;&lt;span&gt;                                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_features&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;max_features,
&lt;&#x2F;span&gt;&lt;span&gt;                                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_samples_leaf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;min_samples_leaf,
&lt;&#x2F;span&gt;&lt;span&gt;                                 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;oob_score&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Our validation RMSE is now much improved over our last result produced by the &lt;code&gt;DecisionTreeRegressor&lt;&#x2F;code&gt;, which made just one tree using all the available data:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.169543
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.231052
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;div class=&quot;alert alert-block alert-info&quot;&gt; You can set &lt;b&gt;n_estimators&lt;&#x2F;b&gt; to as high a number as you have time to train — the more trees you have, the more accurate the model will be. &lt;&#x2F;div&gt;
&lt;h3 id=&quot;9-10-out-of-bag-error-and-prediction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-10-out-of-bag-error-and-prediction&quot; aria-label=&quot;Anchor link for: 9-10-out-of-bag-error-and-prediction&quot;&gt;9.10 Out-of-Bag Error and Prediction&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The OOB error is a way of measuring prediction error in the training dataset by including in the calculation of a row’s error trees only where that row was &lt;em&gt;not&lt;&#x2F;em&gt; included in training.&lt;&#x2F;p&gt;
&lt;p&gt;Imagining that every tree it has also has its own validation set. That validation set is simply the rows that were not selected for that tree’s training.&lt;&#x2F;p&gt;
&lt;p&gt;The OOB predictions are available in the &lt;code&gt;oob_prediction_&lt;&#x2F;code&gt; attribute.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model.oob_prediction_
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.96384715&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.89122526&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.39799785&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.30305792&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.46965767&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.5851676 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;r_mse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model.oob_prediction_, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.20854
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;sklearn&lt;&#x2F;code&gt; also have a a &lt;code&gt;oob_score_&lt;&#x2F;code&gt; attribute that calculates the number of correctly predicted rows from the out of bag sample.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model.oob_score_
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.909784962573175
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can include them in the definition above to having a full picture of the RMSE loss:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X_valid&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Training RMSE: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;m_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model, X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Validation RMSE: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;m_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model, X_valid, y_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Out-of-Bag RMSE: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;r_mse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model.oob_prediction_, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Out-of-Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;model.oob_score_.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;9-11-model-simplification-and-improvements&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-model-simplification-and-improvements&quot; aria-label=&quot;Anchor link for: 9-11-model-simplification-and-improvements&quot;&gt;9.11 Model Simplification and Improvements&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For tabular data, model interpretation is particularly important. For a given model,
we are most likely to be interested in are the following:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;How confident are we in our predictions using a particular row of data?&lt;&#x2F;li&gt;
&lt;li&gt;For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?&lt;&#x2F;li&gt;
&lt;li&gt;Which columns are the strongest predictors, which can we ignore?&lt;&#x2F;li&gt;
&lt;li&gt;Which columns are effectively redundant with each other, for purposes of prediction?&lt;&#x2F;li&gt;
&lt;li&gt;How do predictions vary as we vary these columns?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This section covers the first questions above, not necessarery improving the accuracy of the model but simplyfing the variables to focus to and identifying the part of the data that the model have more problems with.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;9-11-1-tree-variance-for-prediction-confidence&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-1-tree-variance-for-prediction-confidence&quot; aria-label=&quot;Anchor link for: 9-11-1-tree-variance-for-prediction-confidence&quot;&gt;9.11.1 Tree Variance for Prediction Confidence&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;How can we know the conficdence of the estimate? One simple way is to use the standard deviation of predictions across the tree, instead of just the mean. This tells us the relative confidence of predictions.&lt;&#x2F;p&gt;
&lt;p&gt;Therefore, the task is taking all the trees in the model, stack the different predictions and call the standard deviation between them - insteal of the mean.&lt;&#x2F;p&gt;
&lt;p&gt;The different trees from a &lt;code&gt;RandomForestRegressor&lt;&#x2F;code&gt; model can be called as elements of a list:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DecisionTreeRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_features&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;min_samples_leaf&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;random_state&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1600246232&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;predict()&lt;&#x2F;code&gt; can be called on individual trees - for every one of the 7988 auction predictions.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.97461469&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.10483758&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.32772859&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.38366519&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37713079&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37713079&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7988
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;All the trees are under the &lt;code&gt;m.estimators_&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model.estimators_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Let&#x27;s stack all the predictions. 7988 predictions, in each of 100 trees.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;preds_stacked &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;stack&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;i.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;rf_model.estimators_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;preds_stacked
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.97461469&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.10483758&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.32772859&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.38366519&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37713079&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37713079&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.02496635&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.99724274&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.23241147&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.3199054 &lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.38743793&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.38743793&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.93373553&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.96011698&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.01997169&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.1301562 &lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.22006596&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.22006596&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.84292495&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.95399866&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.43168683&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.41749875&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.11293326&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.11293326&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.91806875&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.12426186&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37828723&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.47753124&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.22080501&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.22080501&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.29240811&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.00102539&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.40523815&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.34376642&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.50345051&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.50345051&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;preds_stacked.shape
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;100&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7988&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;preds_stacked&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.97461469&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.10483758&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.32772859&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.38366519&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37713079&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.37713079&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Lastly, we use &lt;code&gt;std&lt;&#x2F;code&gt; to calculate the standard deviation for every auction.&lt;&#x2F;p&gt;
&lt;p&gt;We are setting the axis to 0 calculate the standard deviation at a column level - it takes the 100 tree prediction of the 1st auction and compares the results, takes 100 predictions of the 2nd auction and compares the results, and so forth.&lt;&#x2F;p&gt;
&lt;p&gt;Giving 7988 standard deviations - one for every auction. The ones with a high standard deviation means that the trees dissagree more. If every tree gives the same prediction, the standard deviation would be 0.&lt;&#x2F;p&gt;
&lt;p&gt;Wrapping everything into a function:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tree_variance&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Stack the estimations for every tree
&lt;&#x2F;span&gt;&lt;span&gt;    preds_stacked &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;stack&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;i.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;model.estimators_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Calculate the standard deviation
&lt;&#x2F;span&gt;&lt;span&gt;    pres_std &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;preds_stacked.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;std&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Discrepancies
&lt;&#x2F;span&gt;&lt;span&gt;    max_std &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pres_std.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;max&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    max_row &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;where&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;pres_std &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span&gt;pres_std.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;max&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;astype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.int&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    min_std &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pres_std.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    min_row &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;where&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;pres_std &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;== &lt;&#x2F;span&gt;&lt;span&gt;pres_std.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;astype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;np.int&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Checking differences
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;The row &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt; have the MAX standard deviation between trees (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;max_row, max_std&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;The row &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt; have the MIN standard deviation between trees (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;min_row, min_std&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tree_variance&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    The row &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7083 7084&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span&gt;have the &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;MAX &lt;&#x2F;span&gt;&lt;span&gt;standard deviation between &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;trees &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.625&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    The row &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5364&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span&gt;have the &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;MIN &lt;&#x2F;span&gt;&lt;span&gt;standard deviation between &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;trees &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.058&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As you can see, the confidence in the predictions varies widely. For the auction in the index position 6722th, the trees disagree &quot;a lot&quot;. For the the auction 5364th, the trees predictions varely differ.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;9-11-2-feature-importance&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-2-feature-importance&quot; aria-label=&quot;Anchor link for: 9-11-2-feature-importance&quot;&gt;9.11.2 Feature Importance&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;We also want to know how the model its making predictions. The feature importances give us
this insight.&lt;&#x2F;p&gt;
&lt;p&gt;The attribute &lt;code&gt;feature_importances_&lt;&#x2F;code&gt; gives the list of importance of every feature the model is using to create the splits.&lt;&#x2F;p&gt;
&lt;p&gt;The feature importance algorithm loops through each tree, and then recursively explores each branch. At each branch, it looks to see what feature was used for that split, and how much the model improves as a result of that split.&lt;&#x2F;p&gt;
&lt;p&gt;The improvement (weighted by the number of rows in that group) is added to the importance score for that feature. This is summed across all branches of all trees, and finally the scores are normalized such that they add to 1.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model.feature_importances_
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.60696687e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.31017616e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.25121316e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.93692577e-02&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.80074883e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.32673274e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.17510826e-01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.59427101e-02&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.71958629e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.34531419e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.60803859e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.05581866e-03&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.71543507e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.76569695e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.19521447e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.22032682e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.19242811e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.59219664e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.07939210e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.75216860e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.05861815e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.83936750e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.14647214e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.98867415e-03&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.09370102e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.32816635e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.42691323e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.85881769e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.29902352e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.87666527e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.26387160e-01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.53398306e-02&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.44002655e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.95951710e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.28346841e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.29951677e-03&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.37011193e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.16194890e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.62936850e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.99996690e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.58723627e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.19530794e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.28332319e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.65172973e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.80335713e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.72524636e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.06478776e-05&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.18794614e-06&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.00000000e+00&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.00000000e+00&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.16771593e-04&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.35195462e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.45309332e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.70504793e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.21241853e-02&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.65017421e-04&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.74614566e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.76327989e-01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.73509851e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.98023956e-02&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.60375078e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.41878336e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.36842581e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.15207722e-03&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.90070273e-03&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.05610397e-02&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Sadly, &lt;code&gt;.feature_importances_&lt;&#x2F;code&gt; doesn&#x27;t provide a visual way to present the data - so we will create a function to translate the array output into a visual representation.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_importance&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;model&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;features&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    df_importance &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;DataFrame&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;({&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Feature&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;: features.columns, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Importance&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;: model.feature_importances_&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;})
&lt;&#x2F;span&gt;&lt;span&gt;    df_importance_storted &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df_importance.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;sort_values&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Importance&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ascending &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;reset_index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;drop &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    df_importance_top &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df_importance_storted.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;head&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    fig, ax &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;subplots&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;,n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    sns.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;barplot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;x &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Importance&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Feature&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;data &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df_importance_top,
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;palette &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Blues_r&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;xticks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;yticks&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;ylabel&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    sns.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;despine&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;left&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_importance&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model, X, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_102_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h4 id=&quot;9-11-3-removing-low-importance-variables&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-3-removing-low-importance-variables&quot; aria-label=&quot;Anchor link for: 9-11-3-removing-low-importance-variables&quot;&gt;9.11.3 Removing Low-Importance Variables&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;We have 66 features in the initial mode, let&#x27;s try keeping just those with a feature importance greater than 0.005:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;columns_keep &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X.columns&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;rf_model.feature_importances_ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.005&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;columns_keep
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiBaseModel&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiSecondaryDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDescriptor&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;           &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiProductClassDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductGroup&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductGroupDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;           &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Drive_System&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Enclosure&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Hydraulics&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Tire_Size&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;           &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Coupler_System&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Grouser_Tracks&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Hydraulics_Flow&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalesID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;           &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MachineID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ModelID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleElapsed&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;          &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;object&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We should trim the column both in the train and in the validation set - and then we test the model again:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Only keep important features based on `feature_importances_` attribute
&lt;&#x2F;span&gt;&lt;span&gt;X_imp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;columns_keep&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_imp &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;columns_keep&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Retrain the model with less features
&lt;&#x2F;span&gt;&lt;span&gt;rf_model_2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_imp, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# New model with less features
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_2, X_imp, X_valid_imp&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.180246
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.229436
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.212309
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Previous model
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model, X, X_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.169543
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.231052
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.20854
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.91
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Our validation accuracy is about the same than before (&lt;code&gt;rf_model&lt;&#x2F;code&gt;), even a little bit better, and we have 45(!!) fewer columns to study:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X.columns&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) - &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;columns_keep&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;45
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;9-11-4-removing-redundant-features&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-4-removing-redundant-features&quot; aria-label=&quot;Anchor link for: 9-11-4-removing-redundant-features&quot;&gt;9.11.4 Removing Redundant Features&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;We will create a function using Spearman or rank correlation between the variables.&lt;&#x2F;p&gt;
&lt;p&gt;Intuitively, the &lt;strong&gt;Spearman correlation&lt;&#x2F;strong&gt; between variables will be high when observations have a similar rank, and low when observations have a dissimilar (or fully opposed for a correlation of −1) rank between the two variables.&lt;&#x2F;p&gt;
&lt;p&gt;We use rank correlation because not all the variables follow the same normal distribution and range of values (a.k.a &lt;em&gt;distribution-free&#x2F;nonparametric&lt;&#x2F;em&gt;). For example, the distribution and range of values of the &lt;code&gt;YearID&lt;&#x2F;code&gt; and the &lt;code&gt;Tire_Size&lt;&#x2F;code&gt; of the auctions are widely different.&lt;&#x2F;p&gt;
&lt;p&gt;The only requirement to Spearman correlation is that the variables follow a given order (a.k.a &lt;em&gt;monotonic&lt;&#x2F;em&gt;).&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cluster_columns&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;df&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;font_size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    corr &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;scipy.stats.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;spearmanr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.correlation, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    corr_condensed &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;hc.distance.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;squareform&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;corr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    z &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;hc.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;linkage&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;corr_condensed, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;method&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;average&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    fig &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;figure&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    hc.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dendrogram&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;z, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;labels&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;df.columns, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;orientation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;left&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;leaf_font_size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;font_size&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;    plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;show&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cluster_columns&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_imp&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_115_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The more correlated the features, the early the group at the right of the rank.&lt;&#x2F;p&gt;
&lt;p&gt;Out of the 21 variables &lt;code&gt;saleElapsed&lt;&#x2F;code&gt; and &lt;code&gt;saleYear&lt;&#x2F;code&gt; seems to be closelly correlated. Same goes for:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Hydraulics_Flow&lt;&#x2F;code&gt;, &lt;code&gt;Grouser_Tracks&lt;&#x2F;code&gt;, and &lt;code&gt;Coupler_System&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;ProductGroupDesc&lt;&#x2F;code&gt; and &lt;code&gt;ProductGroup&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;fiBaseModel&lt;&#x2F;code&gt; and &lt;code&gt;fiModelDesc&lt;&#x2F;code&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Let’s try removing some of these closely related features to see if the model can be simplified. We will use OOB acurracy to see the effect of removing the variables one by one.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;var_redundant &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleElapsed&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Hydraulics_Flow&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Grouser_Tracks&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Coupler_System&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductGroupDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductGroup&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;                 &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiBaseModel&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest_redundancy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;X&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;redundant_variables&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;):
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Baseline Model with the &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;{}&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt; most important variables&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;format&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;len&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X.columns&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.oob_score_.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;{&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;print&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Model Accuracy without&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, i, &amp;quot;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;:&lt;&#x2F;span&gt;&lt;span&gt;&amp;quot;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;i, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;.oob_score_.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;round&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)) &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;for &lt;&#x2F;span&gt;&lt;span&gt;i &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;in &lt;&#x2F;span&gt;&lt;span&gt;redundant_variables&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;}
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest_redundancy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_imp, var_redundant&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Baseline Model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;with &lt;&#x2F;span&gt;&lt;span&gt;the &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;21 &lt;&#x2F;span&gt;&lt;span&gt;most important variables &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without saleElapsed : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.901
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without saleYear : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without Hydraulics_Flow : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.907
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without Grouser_Tracks : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without Coupler_System : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.907
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without ProductGroupDesc : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without ProductGroup : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.907
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without fiBaseModel : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without fiModelDesc : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.906
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;As we see, removing redundant variables doesn&#x27;t seem to affect the accuracy.&lt;&#x2F;p&gt;
&lt;p&gt;We can try to keep 4 and remove 5 that seems redundant, and see the accuracy impact - e.g. from &lt;code&gt;[&#x27;Hydraulics_Flow&#x27;, &#x27;Grouser_Tracks&#x27;, &#x27;Coupler_System&#x27;]&lt;&#x2F;code&gt; only keeping &lt;code&gt;Grouser_Tracks&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;var_drop &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleElapsed&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Hydraulics_Flow&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Grouser_Tracks&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductGroupDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiBaseModel&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# We remove the redundant variables
&lt;&#x2F;span&gt;&lt;span&gt;X_final &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_imp.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;var_drop, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_valid_imp.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;var_drop, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Fit the model with the reduced features dataset
&lt;&#x2F;span&gt;&lt;span&gt;rf_model_3 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_final, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_3, X_final, X_valid_final&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.188442
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.230612
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.219032
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;The validation RMSE and Out-of-Bag RMSE is the metrics that we most care about, as they rely on data that the model hasn&#x27;t seen before. And they are looking good!&lt;&#x2F;p&gt;
&lt;p&gt;We made a model with 17 features that achieve almost the same loss as the model using 53 features.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X_final.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to_csv&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;data&#x2F;X_final.csv&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to_csv&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;data&#x2F;X_valid_final.csv&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;index&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;9-11-5-partial-dependence&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-5-partial-dependence&quot; aria-label=&quot;Anchor link for: 9-11-5-partial-dependence&quot;&gt;9.11.5 Partial Dependence&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable?&lt;&#x2F;p&gt;
&lt;p&gt;As we’ve seen, the two most important predictors are &lt;code&gt;ProductSize&lt;&#x2F;code&gt; and &lt;code&gt;YearMade&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;What we do is replace every single value in the &lt;code&gt;YearMade&lt;&#x2F;code&gt; column with 1950, and then calculate the predicted sale price for every auction, and take the average over all auctions. Then we do the same for 1951, 1952, and so forth until our final year of 2011. This isolates the effect of only YearMade&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;sklearn.inspection &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;plot_partial_dependence
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;fig,ax &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;subplots&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_partial_dependence&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_3, X_valid_final, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ax&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;ax&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_129_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;YearMade&lt;&#x2F;code&gt; partial plot show a nearly linear relationship between &lt;code&gt;YearMade&lt;&#x2F;code&gt; and &lt;code&gt;Salesprice&lt;&#x2F;code&gt; after year 1970&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The &lt;code&gt;ProductSize&lt;&#x2F;code&gt; partial shows that for 5 and 6 classes the auctions have the lowest &lt;code&gt;Salesprice&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This kind of insights can give an extra advantage to squish a bit of accuracy in, for example, Kaggle competitions.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;9-11-6-data-leakage&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-11-6-data-leakage&quot; aria-label=&quot;Anchor link for: 9-11-6-data-leakage&quot;&gt;9.11.6 Data Leakage&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;Data leakage is another way to get an advantage in programming competitions.&lt;&#x2F;p&gt;
&lt;p&gt;Tips to identify data leakages:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Check whether the accuracy of the model is &lt;strong&gt;too good to be true&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Look for &lt;strong&gt;important predictors that don’t make sense in practice&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Look for &lt;strong&gt;partial dependence plot&lt;&#x2F;strong&gt; results that don’t make sense in practice.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The only question that remains is:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction?&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-12-tree-models-and-extrapolation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-12-tree-models-and-extrapolation&quot; aria-label=&quot;Anchor link for: 9-12-tree-models-and-extrapolation&quot;&gt;9.12 Tree-models and Extrapolation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;A problem with random forests, like all machine learning or deep learning algorithms, is that they don’t always generalize well to new data.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s consider the simple task of creating a &lt;code&gt;RandomForestRegressor()&lt;&#x2F;code&gt; that learns from the first 30 points and try to predict the next 10. The &quot;features&quot; is a one-dimensional tensor and the &quot;target&quot; is the same one-dimensional tensor plus some noise mady by adding random numbers from a normal distribution.&lt;&#x2F;p&gt;
&lt;p&gt;Therefore, the relation between the features and the target is almost linear plus noise - they are almost the same number. Any human could see the corralation between the &lt;code&gt;X&lt;&#x2F;code&gt; numbers and the &lt;code&gt;y&lt;&#x2F;code&gt; numbers in the tensors below:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;linspace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;20&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;steps&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;40&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;target &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;+ &lt;&#x2F;span&gt;&lt;span&gt;torch.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;randn_like&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.0000&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5128&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0256&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.5385&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.0513&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.5641&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.0769&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.5897&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.1026&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.6154&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.1282&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.6410&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.1538&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.6667&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.1795&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.6923&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.2051&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.7179&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.2308&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.7436&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.2564&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.7692&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;11.2821&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;11.7949&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12.3077&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12.8205&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;13.3333&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;13.8462&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14.3590&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;14.8718&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15.3846&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15.8974&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;16.4103&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;16.9231&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17.4359&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17.9487&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18.4615&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;18.9744&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;19.4872&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;20.0000&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;target
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([-&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.8952e-01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1971e-02&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.6504e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0710e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3.3717e+00&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.5045e-01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.3698e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.2385e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.2067e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.5659e+00&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.5455e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4.8772e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7.8788e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5.7786e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.2888e+00&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6.7935e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.7160e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.1112e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.8788e+00&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0618e+01&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0592e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.2324e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1950e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1621e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1374e+01&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.1379e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4004e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4633e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4821e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.5068e+01&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4898e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.4943e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.6194e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.6307e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.8478e+01&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.7215e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.9295e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.9452e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2.0081e+01&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.9914e+01&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;fig, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;ax1, ax2, ax3&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) = &lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;subplots&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;figsize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;15&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))
&lt;&#x2F;span&gt;&lt;span&gt;ax1.title.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;set_text&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Generated &amp;quot;features&amp;quot;&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;ax2.title.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;set_text&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Generated target&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;ax3.title.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;set_text&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Relationship&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;ax1.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;ax2.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;target&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;ax3.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;scatter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, target&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_139_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The linear relationship is really straightforward, as we can see in the &quot;Relationship&quot; plot - X and y values are very correlated.&lt;&#x2F;p&gt;
&lt;p&gt;It should be easy for the model to take the relationship between the first 30 points and extrapolate it to predict the next 10 right? Let&#x27;s try!&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;unsqueeze&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Fitting the first 30 datapoints
&lt;&#x2F;span&gt;&lt;span&gt;tree_model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;RandomForestRegressor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;30&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, target&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;:&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;30&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Predictions
&lt;&#x2F;span&gt;&lt;span&gt;y_preds &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;tree_model.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Real values in blue
&lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;scatter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, target&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Predictions in red
&lt;&#x2F;span&gt;&lt;span&gt;plt.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;scatter&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, y_preds, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;color&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;red&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;alpha&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_142_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The random forest is not able to see the &quot;clear&quot; linear relationship between our linear points!&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Remember that a random forest just averages the predictions of a number of trees. And a tree simply predicts the average value of the rows in a leaf. Therefore, &lt;strong&gt;a tree and a random forest can never predict values outside the range of the training data&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;This is particularly problematic for data indicating a trend over time, such as inflation, and you wish to make predictions for a future time. Your predictions will be systematically too low.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Random forests are not able to extrapolate outside the types of data they have seen, in a more general sense. That’s why we need to make sure our validation set does not contain out-of-domain data.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-13-finding-out-of-domain-data&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-13-finding-out-of-domain-data&quot; aria-label=&quot;Anchor link for: 9-13-finding-out-of-domain-data&quot;&gt;9.13 Finding Out-of-Domain Data&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The main problem above is that test set is distributed in a different way than the training data. &lt;strong&gt;If the tree model hasn&#x27;t seen a value more than 16, it will never predict more than 16&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Sometimes it is hard to know whether your test set is distributed in the same way as your training data or, if it is different, which columns reflect that difference. There’s
an easy way to figure this out, which is ironically using a random forest!&lt;&#x2F;p&gt;
&lt;p&gt;But in this case, we don’t use the random forest to predict our actual dependent variable. Instead, &lt;strong&gt;we try to predict whether a row is in the validation set or the training set&lt;&#x2F;strong&gt;. To see this in action, let’s combine our training and validation sets, create a dependent variable that represents which dataset each row comes from, build a random forest using that data, and get its feature importance&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Create a column with the target
&lt;&#x2F;span&gt;&lt;span&gt;X_final&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Concat the dfs and create variables
&lt;&#x2F;span&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;concat&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span&gt;X_final, X_valid_final&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;span&gt;is_valid &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;copy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Drop the new variable from the features dataset
&lt;&#x2F;span&gt;&lt;span&gt;X &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;X_final &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_final.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_valid_final.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;is_valid&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Create a model with the target being `is_valid`
&lt;&#x2F;span&gt;&lt;span&gt;rf_model_ODD &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X, is_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;plot_importance&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_ODD, X, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_147_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The difference in &lt;code&gt;SalesID&lt;&#x2F;code&gt; suggests that identifiers for auction sales might increment over time, we&#x27;ll find bigger &lt;code&gt;SalesID&lt;&#x2F;code&gt; values in the validation set.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;saleYear&lt;&#x2F;code&gt; suggest that the latest auctions are in the validation set.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;MachineID&lt;&#x2F;code&gt; suggests something similar might be happening for individual items sold in those auctions, we&#x27;ll find bigger &lt;code&gt;MachineID&lt;&#x2F;code&gt; values in the validation set.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;code&gt;YearMade&lt;&#x2F;code&gt;, same same.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;All these features that are different in the training and validation set have something in common: &lt;strong&gt;they encode the date of the auction&lt;&#x2F;strong&gt;. This is an issue because we are training the past datapoints to predict future datapoints, and as we have seen in the &lt;em&gt;Tree-models and Extrapolation&lt;&#x2F;em&gt; section this works badly.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s try to remove the &quot;date variables&quot; to see if we lose accuracy removing the variables 1 by 1:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest_redundancy&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_final, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalesID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MachineID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Baseline Model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;with &lt;&#x2F;span&gt;&lt;span&gt;the &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;16 &lt;&#x2F;span&gt;&lt;span&gt;most important variables &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without SalesID : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.899
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without saleYear : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.842
&lt;&#x2F;span&gt;&lt;span&gt;    Model Accuracy without MachineID : &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.901
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We should not remove &lt;code&gt;saleYear&lt;&#x2F;code&gt;, as we see a drop in the accuracy.But we can remove &lt;code&gt;SalesID&lt;&#x2F;code&gt; and &lt;code&gt;MachineID&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;We should look as well at the RMSE loss, not only the accuracy:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Reduced datasets
&lt;&#x2F;span&gt;&lt;span&gt;X_final_2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_final.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalesID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MachineID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final_2 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_valid_final.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;drop&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalesID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;MachineID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;axis &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Re-train the model
&lt;&#x2F;span&gt;&lt;span&gt;rf_model_4 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_final_2, y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# New model
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_4, X_final_2, X_valid_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.200645
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.227668
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.219294
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We have improved a bit the model wrt the previous one:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Training RMSE: 0.188482&lt;&#x2F;li&gt;
&lt;li&gt;Validation RMSE: 0.230356&lt;&#x2F;li&gt;
&lt;li&gt;Out-of-Bag RMSE: 0.219128&lt;&#x2F;li&gt;
&lt;li&gt;Out-of-Bag Accuracy: 0.9&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;What we do with &lt;code&gt;salesYear&lt;&#x2F;code&gt;. The distrubtion of this variable is different in the training and in the validation set, but remocing it reduces the accuracy. However... can we trim it?&lt;&#x2F;p&gt;
&lt;p&gt;One thing that might help in our case is to simply avoid using old data. Often, old
data shows relationships that just aren’t valid anymore. Let’s try just using the most
recent few years of the data:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;hist&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_154_0.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2004&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr &gt;
      &lt;th&gt;&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiSecondaryDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelDescriptor&lt;&#x2F;th&gt;
      &lt;th&gt;ProductSize&lt;&#x2F;th&gt;
      &lt;th&gt;fiProductClassDesc&lt;&#x2F;th&gt;
      &lt;th&gt;ProductGroup&lt;&#x2F;th&gt;
      &lt;th&gt;Drive_System&lt;&#x2F;th&gt;
      &lt;th&gt;Enclosure&lt;&#x2F;th&gt;
      &lt;th&gt;Hydraulics&lt;&#x2F;th&gt;
      &lt;th&gt;Tire_Size&lt;&#x2F;th&gt;
      &lt;th&gt;Coupler_System&lt;&#x2F;th&gt;
      &lt;th&gt;ModelID&lt;&#x2F;th&gt;
      &lt;th&gt;YearMade&lt;&#x2F;th&gt;
      &lt;th&gt;saleYear&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;&#x2F;th&gt;
      &lt;td&gt;963&lt;&#x2F;td&gt;
      &lt;td&gt;43&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;59&lt;&#x2F;td&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;17&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3157&lt;&#x2F;td&gt;
      &lt;td&gt;2004&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;&#x2F;th&gt;
      &lt;td&gt;3716&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;8&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;332&lt;&#x2F;td&gt;
      &lt;td&gt;2001&lt;&#x2F;td&gt;
      &lt;td&gt;2011&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;&#x2F;th&gt;
      &lt;td&gt;4261&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;40&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;17311&lt;&#x2F;td&gt;
      &lt;td&gt;2007&lt;&#x2F;td&gt;
      &lt;td&gt;2009&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;5&lt;&#x2F;th&gt;
      &lt;td&gt;500&lt;&#x2F;td&gt;
      &lt;td&gt;59&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;4605&lt;&#x2F;td&gt;
      &lt;td&gt;2004&lt;&#x2F;td&gt;
      &lt;td&gt;2008&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;7&lt;&#x2F;th&gt;
      &lt;td&gt;749&lt;&#x2F;td&gt;
      &lt;td&gt;43&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3539&lt;&#x2F;td&gt;
      &lt;td&gt;2001&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;&#x2F;th&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412693&lt;&#x2F;th&gt;
      &lt;td&gt;490&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;12&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21435&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412694&lt;&#x2F;th&gt;
      &lt;td&gt;491&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;17&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21436&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412695&lt;&#x2F;th&gt;
      &lt;td&gt;490&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21435&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412696&lt;&#x2F;th&gt;
      &lt;td&gt;490&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21435&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412697&lt;&#x2F;th&gt;
      &lt;td&gt;491&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;17&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21436&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;230144 rows × 14 columns&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2004&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0         11.097410
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3         10.558414
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4          9.305651
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5         10.184900
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;7         10.203592
&lt;&#x2F;span&gt;&lt;span&gt;                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;412693     9.210340
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;412694     9.259130
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;412695     9.433484
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;412696     9.210340
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;412697     9.472705
&lt;&#x2F;span&gt;&lt;span&gt;    Name: SalePrice, Length: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;230144&lt;&#x2F;span&gt;&lt;span&gt;, dtype: float32
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X_trimmed &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=       &lt;&#x2F;span&gt;&lt;span&gt;X_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2004&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;X_valid_trimmed &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;X_valid_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt; &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2004&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;y_trimmed &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=       &lt;&#x2F;span&gt;&lt;span&gt;y&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;X_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;&amp;gt;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2004&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;rf_model_5 &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;random_forest&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_trimmed, y_trimmed&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Previous RMSE
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_4, X_final_2, X_valid_final_2, y, y_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.200645
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.227668
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.219294
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.9
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# New RMSE
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;print_rmse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;rf_model_5, X_trimmed, X_valid_trimmed, y_trimmed, y_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    Training &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.19193
&lt;&#x2F;span&gt;&lt;span&gt;    Validation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.227894
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;RMSE&lt;&#x2F;span&gt;&lt;span&gt;: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.218182
&lt;&#x2F;span&gt;&lt;span&gt;    Out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;of&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;-&lt;&#x2F;span&gt;&lt;span&gt;Bag Accuracy: &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.904
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;It’s a tiny bit better, which shows that you shouldn’t always use your entire dataset - sometimes a subset can be better.&lt;&#x2F;p&gt;
&lt;p&gt;Let’s see if using a neural network we can increase even further the accuracy.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-14-neural-networks-for-tabular-data&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-14-neural-networks-for-tabular-data&quot; aria-label=&quot;Anchor link for: 9-14-neural-networks-for-tabular-data&quot;&gt;9.14 Neural Networks for tabular data&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We can use the same approach to build a neural network model. Let’s first replicate
the steps we took to set up the TabularPandas object:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;X_final_2
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr &gt;
      &lt;th&gt;&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiSecondaryDesc&lt;&#x2F;th&gt;
      &lt;th&gt;fiModelDescriptor&lt;&#x2F;th&gt;
      &lt;th&gt;ProductSize&lt;&#x2F;th&gt;
      &lt;th&gt;fiProductClassDesc&lt;&#x2F;th&gt;
      &lt;th&gt;ProductGroup&lt;&#x2F;th&gt;
      &lt;th&gt;Drive_System&lt;&#x2F;th&gt;
      &lt;th&gt;Enclosure&lt;&#x2F;th&gt;
      &lt;th&gt;Hydraulics&lt;&#x2F;th&gt;
      &lt;th&gt;Tire_Size&lt;&#x2F;th&gt;
      &lt;th&gt;Coupler_System&lt;&#x2F;th&gt;
      &lt;th&gt;ModelID&lt;&#x2F;th&gt;
      &lt;th&gt;YearMade&lt;&#x2F;th&gt;
      &lt;th&gt;saleYear&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;&#x2F;th&gt;
      &lt;td&gt;963&lt;&#x2F;td&gt;
      &lt;td&gt;43&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;59&lt;&#x2F;td&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;17&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3157&lt;&#x2F;td&gt;
      &lt;td&gt;2004&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;&#x2F;th&gt;
      &lt;td&gt;1745&lt;&#x2F;td&gt;
      &lt;td&gt;57&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;62&lt;&#x2F;td&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;12&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;77&lt;&#x2F;td&gt;
      &lt;td&gt;1996&lt;&#x2F;td&gt;
      &lt;td&gt;2004&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;&#x2F;th&gt;
      &lt;td&gt;336&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;39&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;7009&lt;&#x2F;td&gt;
      &lt;td&gt;2001&lt;&#x2F;td&gt;
      &lt;td&gt;2004&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;&#x2F;th&gt;
      &lt;td&gt;3716&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;8&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;332&lt;&#x2F;td&gt;
      &lt;td&gt;2001&lt;&#x2F;td&gt;
      &lt;td&gt;2011&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;&#x2F;th&gt;
      &lt;td&gt;4261&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;40&lt;&#x2F;td&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;17311&lt;&#x2F;td&gt;
      &lt;td&gt;2007&lt;&#x2F;td&gt;
      &lt;td&gt;2009&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;&#x2F;th&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
      &lt;td&gt;...&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412693&lt;&#x2F;th&gt;
      &lt;td&gt;490&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;12&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21435&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412694&lt;&#x2F;th&gt;
      &lt;td&gt;491&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;17&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21436&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412695&lt;&#x2F;th&gt;
      &lt;td&gt;490&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21435&lt;&#x2F;td&gt;
      &lt;td&gt;2005&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412696&lt;&#x2F;th&gt;
      &lt;td&gt;490&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;13&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21435&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;th&gt;412697&lt;&#x2F;th&gt;
      &lt;td&gt;491&lt;&#x2F;td&gt;
      &lt;td&gt;108&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;17&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;21436&lt;&#x2F;td&gt;
      &lt;td&gt;2006&lt;&#x2F;td&gt;
      &lt;td&gt;2012&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;404710 rows × 14 columns&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df_nn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;pd.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;read_csv&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;TrainAndValid.csv&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;low_memory&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;False&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;astype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;category&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.cat.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;set_categories&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;sizes, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;ordered&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;inplace&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;True&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;] &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;np.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;log&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df_nn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;add_datepart&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df_nn, &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saledate&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;df_nn_final &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;df_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;list&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_final_2.columns&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;) + [&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Categorical columns are handled very differently in neural networks, compared to decision tree approaches - &lt;strong&gt;For Neural Networks we will use embeddings&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To create embeddings, fastai needs to determine which columns should be treated as categorical variables. It does this by comparing the number of distinct levels in the variable to the value of the &lt;code&gt;max_card&lt;&#x2F;code&gt; parameter. If it’s lower, fastai will treat the variable as categorical. Embedding sizes larger than 10,000 should generally be used only after you’ve tested whether there are better ways to group the variable, so we’ll use 9,000 as our max_card value:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;cont_nn, cat_nn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;cont_cat_split&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df_nn_final, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;max_card&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9000&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dep_var&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;cat_nn
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiSecondaryDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiModelDescriptor&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductSize&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;fiProductClassDesc&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ProductGroup&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Drive_System&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Enclosure&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Hydraulics&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Tire_Size&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;Coupler_System&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;ModelID&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;YearMade&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;,
&lt;&#x2F;span&gt;&lt;span&gt;     &amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;In this case, however, there’s one variable that we absolutely do not want to treat as
categorical: &lt;code&gt;saleYear&lt;&#x2F;code&gt;. A categorical variable cannot, by definition, extrapolate
outside the range of values that it has seen, but we want to be able to predict auction
sale prices in the future. Therefore, we need to make this a continuous variable:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;cont_nn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;append&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;cat_nn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;remove&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;saleYear&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;df_nn_final&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span&gt;cat_nn&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;nunique&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    fiModelDesc           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5059
&lt;&#x2F;span&gt;&lt;span&gt;    fiSecondaryDesc        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;177
&lt;&#x2F;span&gt;&lt;span&gt;    fiModelDescriptor      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;140
&lt;&#x2F;span&gt;&lt;span&gt;    ProductSize              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6
&lt;&#x2F;span&gt;&lt;span&gt;    fiProductClassDesc      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;74
&lt;&#x2F;span&gt;&lt;span&gt;    ProductGroup             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6
&lt;&#x2F;span&gt;&lt;span&gt;    Drive_System             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;4
&lt;&#x2F;span&gt;&lt;span&gt;    Enclosure                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;6
&lt;&#x2F;span&gt;&lt;span&gt;    Hydraulics              &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12
&lt;&#x2F;span&gt;&lt;span&gt;    Tire_Size               &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;17
&lt;&#x2F;span&gt;&lt;span&gt;    Coupler_System           &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2
&lt;&#x2F;span&gt;&lt;span&gt;    ModelID               &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5281
&lt;&#x2F;span&gt;&lt;span&gt;    YearMade                &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;73
&lt;&#x2F;span&gt;&lt;span&gt;    dtype: int64
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can create our TabularPandas object in the same way as when we created our random forest, with one very important addition: &lt;strong&gt;normalization&lt;&#x2F;strong&gt;. A random forest does not need any normalization—the tree building procedure cares only about the order of values in a variable, not at all about how they are scaled.&lt;&#x2F;p&gt;
&lt;p&gt;Neural networks are definitely affected by the scale of the values. Therefore, we add the Normalize processor when we build our TabularPandas object:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;procs_nn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= [&lt;&#x2F;span&gt;&lt;span&gt;Categorify, FillMissing, Normalize&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]
&lt;&#x2F;span&gt;&lt;span&gt;to_nn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;TabularPandas&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;df_nn_final,
&lt;&#x2F;span&gt;&lt;span&gt;                      procs_nn,
&lt;&#x2F;span&gt;&lt;span&gt;                      cat_nn,
&lt;&#x2F;span&gt;&lt;span&gt;                      cont_nn,
&lt;&#x2F;span&gt;&lt;span&gt;                      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;splits&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;splits,
&lt;&#x2F;span&gt;&lt;span&gt;                      &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y_names&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;SalePrice&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We load the data into a &lt;code&gt;DataLoader&lt;&#x2F;code&gt;, and set a range for the target. It’s a good idea to set &lt;code&gt;y_range&lt;&#x2F;code&gt; for regression models, so let’s find
the min and max of our dependent variable:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Features into the dataloader
&lt;&#x2F;span&gt;&lt;span&gt;dls &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;to_nn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;dataloaders&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1024&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Target
&lt;&#x2F;span&gt;&lt;span&gt;y &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;to_nn.train.y
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Range
&lt;&#x2F;span&gt;&lt;span&gt;y.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;min&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;,y.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;max&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.465899&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;11.863583&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Lastly, we build the model:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tabular_learner&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;dls, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;y_range&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;12&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;layers&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;500&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;250&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;n_out&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;loss_func&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;F.mse_loss&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;There’s no need to use fine_tune, so we’ll train with fit_one_cycle for a few epochs
and see how it looks:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;fit_one_cycle&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;, learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;lr_find&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;table class=&quot;dataframe&quot;&gt;
  &lt;thead&gt;
    &lt;tr style=&quot;text-align: left;&quot;&gt;
      &lt;th&gt;epoch&lt;&#x2F;th&gt;
      &lt;th&gt;train_loss&lt;&#x2F;th&gt;
      &lt;th&gt;valid_loss&lt;&#x2F;th&gt;
      &lt;th&gt;time&lt;&#x2F;th&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;&#x2F;td&gt;
      &lt;td&gt;0.040625&lt;&#x2F;td&gt;
      &lt;td&gt;0.051301&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;&#x2F;td&gt;
      &lt;td&gt;0.043241&lt;&#x2F;td&gt;
      &lt;td&gt;0.052923&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;&#x2F;td&gt;
      &lt;td&gt;0.043518&lt;&#x2F;td&gt;
      &lt;td&gt;0.053630&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;&#x2F;td&gt;
      &lt;td&gt;0.042394&lt;&#x2F;td&gt;
      &lt;td&gt;0.054047&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;4&lt;&#x2F;td&gt;
      &lt;td&gt;0.040913&lt;&#x2F;td&gt;
      &lt;td&gt;0.052986&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;&#x2F;td&gt;
      &lt;td&gt;0.040410&lt;&#x2F;td&gt;
      &lt;td&gt;0.052649&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;6&lt;&#x2F;td&gt;
      &lt;td&gt;0.038336&lt;&#x2F;td&gt;
      &lt;td&gt;0.051216&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;7&lt;&#x2F;td&gt;
      &lt;td&gt;0.037320&lt;&#x2F;td&gt;
      &lt;td&gt;0.052022&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;8&lt;&#x2F;td&gt;
      &lt;td&gt;0.036384&lt;&#x2F;td&gt;
      &lt;td&gt;0.051955&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
    &lt;tr&gt;
      &lt;td&gt;9&lt;&#x2F;td&gt;
      &lt;td&gt;0.036191&lt;&#x2F;td&gt;
      &lt;td&gt;0.051794&lt;&#x2F;td&gt;
      &lt;td&gt;00:03&lt;&#x2F;td&gt;
    &lt;&#x2F;tr&gt;
  &lt;&#x2F;tbody&gt;
&lt;&#x2F;table&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-fastai&#x2F;.&#x2F;images&#x2F;output_179_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can use our r_mse function to compare the result to the random forest result we
got earlier:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;preds,targs &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_preds&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;r_mse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;preds,targs&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.227582
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This gives us a similiar result than the best random forest achieved previously. Before we move on, let’s save our model in case we want to come back to it again later:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;save&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;nn&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Path&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;models&#x2F;nn.pth&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can always try to hypertune the model from here.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-15-ensembling&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-15-ensembling&quot; aria-label=&quot;Anchor link for: 9-15-ensembling&quot;&gt;9.15 Ensembling&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We have two very different models, trained using very different algorithms: random forest and neural networks.&lt;&#x2F;p&gt;
&lt;p&gt;It would be reasonable to expect that the kinds of errors that each one makes would be quite different. Therefore, we might &lt;strong&gt;expect that the average of their predictions would be better than either one’s individual predictions&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;When ensembling the results together, one minor issue we have to be aware of is that our PyTorch model and our sklearn model create data of different types.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;PyTorch gives us a rank-2 tensor (a column matrix)&lt;&#x2F;li&gt;
&lt;li&gt;NumPy gives us a rank-1 array (a vector).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;learn.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;get_preds&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.2192&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.0230&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.3750&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.3017&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.2062&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.2062&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]])&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;     &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;tensor&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.0432&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.0858&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.3927&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.3501&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.1050&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;             &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;[ &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;8.9554&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]]))
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;rf_model_5.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.07742579&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.03322471&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.35772406&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.34768389&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.24583077&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.24583077&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;code&gt;squeeze&lt;&#x2F;code&gt; removes any unit axes from a tensor, and to_np converts it into a Numpy array:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to_np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;preds.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;squeeze&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;())
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;array&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.219167&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10.023037&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.375016&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;...&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.301746&lt;&#x2F;span&gt;&lt;span&gt;,  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.206213&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;            &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;9.206213&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;]&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;dtype&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;float32&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Now that both predictions are in numpy arraym they can be ensembled.&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;ensemble_preds &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= (&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;to_np&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;preds.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;squeeze&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()) + &lt;&#x2F;span&gt;&lt;span&gt;rf_model_5.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;predict&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;X_valid_final_2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;))&#x2F;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;r_mse&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;ensemble_preds, y_valid&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.22322
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Notice that an RMSE of 0.223 is the best result so far - better than the most tunned random forest and the neural network!&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-16-boosting&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-16-boosting&quot; aria-label=&quot;Anchor link for: 9-16-boosting&quot;&gt;9.16 Boosting&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In another important approach to ensembling, called boosting, where we add models instead of averaging them.&lt;&#x2F;p&gt;
&lt;p&gt;Here is how boosting works:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Train a small model that underfits your dataset.&lt;&#x2F;li&gt;
&lt;li&gt;Calculate the predictions in the training set for this model.&lt;&#x2F;li&gt;
&lt;li&gt;Subtract the predictions from the targets; these are called the residuals and represent
the error for each point in the training set.&lt;&#x2F;li&gt;
&lt;li&gt;Go back to step 1, but &lt;strong&gt;instead of using the original targets, use the residuals as
the targets for the training&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;Continue doing this until you reach a stopping criterion, such as a maximum
number of trees, or you observe your validation set error getting worse.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Using this approach, each new tree will be attempting to fit the error of all of the previous
trees combined.&lt;&#x2F;p&gt;
&lt;p&gt;Note that, unlike with random forests, with this approach, &lt;strong&gt;there is nothing to stop us from overfitting&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Using more trees in a random forest does not lead to overfitting, because each tree is independent of the others. But &lt;strong&gt;in a boosted ensemble, the more trees you have, the better the training error becomes&lt;&#x2F;strong&gt;, and eventually you will see overfitting on the validation set.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-17-tabular-models-conclusion&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#9-17-tabular-models-conclusion&quot; aria-label=&quot;Anchor link for: 9-17-tabular-models-conclusion&quot;&gt;9.17 Tabular models conclusion&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We have discussed two approaches to tabular modeling: decision tree ensembles and neural networks. We’ve also mentioned two decision tree ensembles: random forests and gradient boosting machines. Each is effective but also requires compromises:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Random forests&lt;&#x2F;strong&gt; are the easiest to train, because they are extremely resilient to hyperparameter choices and require little preprocessing. They are fast to train, and should not overfit if you have enough trees. But they can be a little less accurate, especially if extrapolation is required, such as predicting future time periods.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gradient boosting&lt;&#x2F;strong&gt; machines in theory are just as fast to train as random forests, but in practice you will have to try lots of hyperparameters. They can overfit, but they are often a little more accurate than random forests.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Neural networks&lt;&#x2F;strong&gt; take the longest time to train and require extra preprocessing, such as normalization - this normalization needs to be used at inference time as well. They can provide great results and extrapolate well, but only if you are careful with your hyperparameters and take care to avoid overfitting.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;We suggest starting your analysis with a random forest&lt;&#x2F;strong&gt;. This will give you a strong baseline, and you can be confident that it’s a reasonable starting point. You can then use that model for feature selection and partial dependence analysis, to get a better understanding of your data.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>The 3 alarms, by Eric Partaker</title>
        <published>2022-01-01T00:00:00+00:00</published>
        <updated>2022-05-01T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-three-alarms/"/>
        <id>https://pipegalera.github.io/mostly_books/book-three-alarms/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-three-alarms/">&lt;p&gt;The 3 alarms are literally 3 alarms on your phone to remind you to take care of the important things.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;m.media-amazon.com&#x2F;images&#x2F;I&#x2F;41eOwXCoVbL._SY445_SX342_.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;What if I set three alarms on my phone to trigger me: one to remind me I want to be healthy, one to remind me I want to be wealthy, and one to remind me I want to be a great husband and dad?&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The idea behind is that you can only perform up to the maximum of your abilities (&lt;strong&gt;&quot;peak performance&quot;&lt;&#x2F;strong&gt;) if you take care of:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Your relationships.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Your health.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Your career.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Setting the 3 alarms help to refocus on the specific goal at that part of the day and zoom out of the others.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;When the alarm goes off, you can think of yourself as stepping into that new identity—transforming into you at your best within that area.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The book takes a holistic view of success. The 3 pilars are interconnected - if you neglect one it affect negatively the others.&lt;&#x2F;p&gt;
&lt;p&gt;This idea deeply resonated with me.&lt;&#x2F;p&gt;
&lt;p&gt;I&#x27;m sure that every person can pin-point a stage in life that they neglected their relationships, career, or health and had negative consequences in the their overall life and happiness.&lt;&#x2F;p&gt;
&lt;p&gt;The 3 alarms themselves are only &lt;strong&gt;reminders of the kind of person you &lt;em&gt;are&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;. It is more likely that you do things because it is part of your identity rather than because &quot;you have to do it&quot;. The concept is the same as identity-based habits, treated extensively in &lt;em&gt;Atomic Habits&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;You go to the gym because you are healthy. You take your job seriously because you are a professional. You take care of your relationships because you are a caring person. You do what you do because you &lt;strong&gt;are&lt;&#x2F;strong&gt; that kind of person, not because you &lt;strong&gt;have to&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;Let say that you have an alarm for 8:00 for work (&lt;em&gt;career&lt;&#x2F;em&gt;), 17:00 to stop working and taking care of your family (&lt;em&gt;relationships&lt;&#x2F;em&gt;) and at 20:00 to do a workout (&lt;em&gt;health&lt;&#x2F;em&gt;).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-productivity-tips-i-actually-use-from-the-book&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-productivity-tips-i-actually-use-from-the-book&quot; aria-label=&quot;Anchor link for: 3-productivity-tips-i-actually-use-from-the-book&quot;&gt;3 productivity tips I actually use from the book.&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;There are many &quot;productivity tips&quot; in the book, including:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Cultivate antifragility.&lt;&#x2F;li&gt;
&lt;li&gt;See actions under 80&#x2F;20 Pareto lenses.&lt;&#x2F;li&gt;
&lt;li&gt;Set big goals.&lt;&#x2F;li&gt;
&lt;li&gt;Set a calendar around your energy levels and goals.&lt;&#x2F;li&gt;
&lt;li&gt;Focus on starting.&lt;&#x2F;li&gt;
&lt;li&gt;Deep focus.&lt;&#x2F;li&gt;
&lt;li&gt;...&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;I have read the above in many other self-help books, and in my case they do not drive any real change. They all convey to the same techniques and it&#x27;s mostly &lt;a href=&quot;https:&#x2F;&#x2F;calebschoepp.com&#x2F;blog&#x2F;2022&#x2F;productivity-porn&#x2F;&quot;&gt;&quot;productiviy p*rn&quot;&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;However, there are 4 small bits that they are practical and they take from 20 seconds to 30 minutes max and are useful.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-collect-champion-proofs-not-items-in-checklist&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-collect-champion-proofs-not-items-in-checklist&quot; aria-label=&quot;Anchor link for: 1-collect-champion-proofs-not-items-in-checklist&quot;&gt;1. Collect champion proofs, not items in checklist.&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;These alarms are not reminders of what you have to do, they are reminders of the person are. Every time you actually follow the alarms with actions you prove yourself that you are that person.&lt;&#x2F;p&gt;
&lt;p&gt;Eric Partaker call these actions &lt;em&gt;&quot;champion proofs&quot;&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;I needed something to prove I was being who I said I wanted to be. So I created a concept called &quot;champion proofs&quot;. Champion proofs let you rack up small wins, day after day, to prove you&#x27;re stepping into the person you&#x27;re capable of being.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Go to bed everyday having done at least one champion proof&lt;&#x2F;strong&gt; for each identity that gets you closer to the person you want to be.&lt;&#x2F;p&gt;
&lt;p&gt;Try to do one action per day that contribute to your best-self identity. Even if it is a very small action for 30 minutes, it stack and compound over time.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;We &quot;win the day&quot; not because we achieved what we planned, but because we did what was most important despite our plans.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;If you feel overwhelmed, identify the biggest gap between your best-self and the daily actions. Is it health, relationships, or career ?&lt;&#x2F;p&gt;
&lt;p&gt;Focus on setting one champion proof every day for only for one of the 3 aspects, and progressively add champions proofs for the other two.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-digital-sunset&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-digital-sunset&quot; aria-label=&quot;Anchor link for: 2-digital-sunset&quot;&gt;2. Digital sunset.&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;blockquote&gt;
&lt;p&gt;Devices can reduce the production of the sleep-inducing hormone melatonin by up to 50%. Without enough melatonin in the body, you&#x27;ll find it difficult to sleep a full eight hours, and even if you do, it&#x27;s not likely to be as deep and restorative as it should.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;I like to turn off screens by 23:00, with the exception of my kindle.&lt;&#x2F;p&gt;
&lt;p&gt;I sleep better, and also start more energised my next day. The other side effect is that I read way more than before as I usually use the time before midnight reading instead of scrolling.&lt;&#x2F;p&gt;
&lt;p&gt;In the future I might do it an earlier digital sunset, but for now 23:00 does the trick.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-brief-weekly-reviews&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-brief-weekly-reviews&quot; aria-label=&quot;Anchor link for: 3-brief-weekly-reviews&quot;&gt;3. Brief weekly reviews.&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;This is a review your personal and professional life. What went good and bad in the week, and what could have planned better in hindsight.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Don&#x27;t take this as an opportunity to self inflict pain by going over what you should have done&lt;&#x2F;strong&gt;. For me, this is an exercise of self-awareness. I simply apply the learnings from one week to the next one.&lt;&#x2F;p&gt;
&lt;p&gt;As an &lt;strong&gt;example&lt;&#x2F;strong&gt;, this is how a random weekly review in my life looks like.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Successes:
&lt;ul&gt;
&lt;li&gt;Health. I went twice to the gym and went running once.&lt;&#x2F;li&gt;
&lt;li&gt;Relationship. Called my family and had a date out with my partner.&lt;&#x2F;li&gt;
&lt;li&gt;Career. I learnt how to do a radar visualisation with chartJS that I can use in my website.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Failures:
&lt;ul&gt;
&lt;li&gt;Planned too many activities per day.&lt;&#x2F;li&gt;
&lt;li&gt;I should not plan anything for 30min or less.&lt;&#x2F;li&gt;
&lt;li&gt;I wasn&#x27;t focused at work 2 straight days.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Insights&#x2F;Actions:
&lt;ul&gt;
&lt;li&gt;Running listening &quot;Dune&quot; is great. The chapter length match perfectly my pace.&lt;&#x2F;li&gt;
&lt;li&gt;Take care when you lift, don&#x27;t injure yourself.&lt;&#x2F;li&gt;
&lt;li&gt;Plan 1 hour minimum per activity, even if takes less.&lt;&#x2F;li&gt;
&lt;li&gt;If you are unfocused while working, take a walk or go to the office.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;This review not a complicated system. I write this as part of a journal routine in my calendar app every Sunday. It takes 5 to 10 minutes.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-three-alarms&#x2F;.&#x2F;images&#x2F;calendar.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;I also spend another 5 minutes adding the insights for the next week&#x27;s scheduled activities as a reminder.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;book-three-alarms&#x2F;.&#x2F;images&#x2F;insight.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The return on investment of these 15 minutes is huge. This exercise of reflection brings me instant clarity and increase the probability of achiving my goals for the following week.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Atomic Habits, by James Clear</title>
        <published>2021-12-30T00:00:00+00:00</published>
        <updated>2021-12-30T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/book-atomic-habits/"/>
        <id>https://pipegalera.github.io/mostly_books/book-atomic-habits/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/book-atomic-habits/">&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;cdn-images-1.medium.com&#x2F;max&#x2F;800&#x2F;1*q_ca6R_S8V1vlaGCc_VQxw.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The book starts with James own personal history about how setting little habits and compounding these habits into more and better ones lead him to recover from a serious accident to be a college elite baseball player.&lt;&#x2F;p&gt;
&lt;p&gt;The book uses these kinds of anecdotes about habits, and supports them with scientific research behind them. The combo makes the narrative pleasant, and not just a collection of peer-reviewed habit articles, as you can reflect on the characters of his stories.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-small-compounded-habits-drive-your-actions&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-small-compounded-habits-drive-your-actions&quot; aria-label=&quot;Anchor link for: 1-small-compounded-habits-drive-your-actions&quot;&gt;1. Small compounded habits drive your actions.&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Positive and consistent habits are important but most important is taking care of the negative ones. Strong negative habits can take over your potential gains from good habit formation.&lt;&#x2F;p&gt;
&lt;p&gt;You can still have bad habits and be successful. You just need the majority, and not all, of your actions heading to your aims.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;cdn-images-1.medium.com&#x2F;max&#x2F;800&#x2F;1*fnpdCDdus1zb4R4jQbj0kw.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-switch-from-outcome-based-habits-to-identity-based-habits&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-switch-from-outcome-based-habits-to-identity-based-habits&quot; aria-label=&quot;Anchor link for: 2-switch-from-outcome-based-habits-to-identity-based-habits&quot;&gt;2. Switch from Outcome-based habits to Identity-based habits.&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;With outcome-based habits, the focus is on what you want to achieve. Usually, high performers and low performers, rich and poor, lucky winners and unlucky losers have the same goals and dreams. Therefore, goals are not good grounds for building habits.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;cdn-images-1.medium.com&#x2F;max&#x2F;800&#x2F;1*YERK9dZOx76A6BabPy0tbA.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;With identity-based habits, the focus is on who you wish to become. Any change in habits starts with a change of beliefs about who you are. Decide the type of person that you want to be, and then act accordingly.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;cdn-images-1.medium.com&#x2F;max&#x2F;800&#x2F;1*-PM-GvQ2TonM2B73PPrpZg.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;What a responsible person would do in my position? What a healthy person would do? What a good person would do in this situation?&lt;&#x2F;p&gt;
&lt;p&gt;As you focus more on the mindset of your role models than in generic goals, you develop good systems that reinforce good habits. Forget about goals and focus on what an exemplary person would do in your shoes.&lt;&#x2F;p&gt;
&lt;p&gt;Create a feedback loop of action and identity. Every action you take is a vote for the type of person you wish to become. A single instance will not transform your beliefs, but as the votes build up, so does the evidence of your new identity.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-the-habit-loop-and-the-four-rules-of-habits&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-the-habit-loop-and-the-four-rules-of-habits&quot; aria-label=&quot;Anchor link for: 3-the-habit-loop-and-the-four-rules-of-habits&quot;&gt;3. The Habit Loop and The Four Rules of Habits&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;This is &quot;The Habit Loop&quot;. The four stages of habit are best described as a feedback loop.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;cdn-images-1.medium.com&#x2F;max&#x2F;800&#x2F;1*KwZrAqPowr7rLJfkPN1yew.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;They form an endless cycle that is running every moment you are alive. This &quot;habit loop&quot; is continually scanning the environment, predicting what will happen next, trying out different responses, and learning from the results.&lt;&#x2F;p&gt;
&lt;p&gt;The Four Rules of Habits describe how to design an environment that has the correct cues to make you crave good habits, take action and reward yourself for doing the right thing to keep the loop running.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;first-rule-make-it-obvious-control-the-cues&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#first-rule-make-it-obvious-control-the-cues&quot; aria-label=&quot;Anchor link for: first-rule-make-it-obvious-control-the-cues&quot;&gt;First rule: Make it obvious, control the cues&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The format for creating an implementation intention is: &quot;When situation X arises, I will perform response Y.&quot; The first part of the statement is the cue that leads to the habit that you want to implement. If you want to make a habit a big part of your life, make the cue a big part of your environment.&lt;&#x2F;p&gt;
&lt;p&gt;Stop thinking about your environment as filled with objects. Start thinking about it as filled with relationships. Think in terms of how you interact with the spaces around you. Design your surroundings with cues that trigger good habits.&lt;&#x2F;p&gt;
&lt;p&gt;One of the most practical ways to eliminate a bad habit is to reduce exposure to the cue that causes it. Examples: put your Xbox in the drawer or keep your phone home when you go studying to the library.
Get as many cues as you can that crave good habits. For bad habits do the opposite, reduce the cues and increase the friction or make it inconvenient&#x2F;impossible.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;second-rule-make-it-attractive-control-the-dopamine&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#second-rule-make-it-attractive-control-the-dopamine&quot; aria-label=&quot;Anchor link for: second-rule-make-it-attractive-control-the-dopamine&quot;&gt;Second rule: Make it attractive, control the dopamine&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Dopamine&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Before a habit is learned, dopamine is released when the reward is experienced for the first time. Once you start doing something more and more, dopamine rises before taking action, immediately after a cue is recognized. This spike leads to a feeling of desire and a craving to take action whenever the cue is spotted. Dopamine rises as a craving builds.&lt;&#x2F;p&gt;
&lt;p&gt;Stacking habits and keeping habit streaks will increase the likelihood that you&#x27;ll stick with a habit. You create cravings that lead to dopamine for your brain. One action leads to the next (stack) and the previous action to the next one (streak). If you want to master a habit, the key is to start with repetition, not perfection.&lt;&#x2F;p&gt;
&lt;p&gt;We need to make our habits attractive because it is the expectations of a rewarding experience that motivates us doing it in the first place.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Social norms&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Behaviors are attractive when they help us fit in. We imitate the habits of three groups in particular: The close, the many and the powerful. We don&#x27;t choose our earliest habits, we imitate them.&lt;&#x2F;p&gt;
&lt;p&gt;We follow the script handed down by our friends and family, our church or school, our local community and society at large. There is tremendous internal pressure to comply with the norms of the group.&lt;&#x2F;p&gt;
&lt;p&gt;Every behavior has a surface level craving and a deeper, underlying motive: from conservative energy to win social acceptance and approval.&lt;&#x2F;p&gt;
&lt;p&gt;One of the most effective things you can do to build better habits is to join a culture where (1) your desired behavior is the normal behavior and (2) you already have something in common with the group.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;third-rule-make-it-easy-control-the-amount-of-friction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#third-rule-make-it-easy-control-the-amount-of-friction&quot; aria-label=&quot;Anchor link for: third-rule-make-it-easy-control-the-amount-of-friction&quot;&gt;Third rule: Make it easy, control the amount of friction&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The central idea is to create an environment where doing the right thing is as easy as possible. Researchers estimate that 40 to 50 percent of our actions on any given day are done out of habit.&lt;&#x2F;p&gt;
&lt;p&gt;Every day, there are a handful of moments that deliver an outsized impact. I refer to these little choices as decisive moments. The key is to change the decisive moments such that it requires more work to get out of the good habit than to get started on it.&lt;&#x2F;p&gt;
&lt;p&gt;Automate. When you automate as much of your life as possible, you can spend your effort on the tasks machines cannot do yet. Each habit that we hand over to the authority of technology frees up time and energy to pour into the next stage of growth.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;fourth-rule-make-it-satisfying-control-the-habit-loop&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#fourth-rule-make-it-satisfying-control-the-habit-loop&quot; aria-label=&quot;Anchor link for: fourth-rule-make-it-satisfying-control-the-habit-loop&quot;&gt;Fourth rule: Make it satisfying, control the habit loop&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;What is immediately rewarded is repeated. What is immediately punished is avoided. Because of how we are wired, most of us will spend all day chasing quick hits of satisfaction. It&#x27;s possible to train yourself to delay gratification, but it&#x27;s easier to use gratification to your advantage.&lt;&#x2F;p&gt;
&lt;p&gt;It is important to select short-term rewards that reinforce your identity rather than ones that conflict with it.&lt;&#x2F;p&gt;
&lt;p&gt;Tiny changes. Remarkable results. That&#x27;s the power of atomic habits.&lt;&#x2F;p&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Course Notes - MIT 6.S191, Introduction to Deep Learning</title>
        <published>2021-06-08T00:00:00+00:00</published>
        <updated>2021-06-08T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/course-mit-intro-deep-learning/"/>
        <id>https://pipegalera.github.io/mostly_books/course-mit-intro-deep-learning/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/course-mit-intro-deep-learning/">&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;intro_banner.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;My personal notes of the MIT course &lt;a href=&quot;http:&#x2F;&#x2F;introtodeeplearning.com&#x2F;&quot;&gt;MIT 6.S191, Introduction to Deep Learning&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;NOTE: Diagrams only look good in &lt;code&gt;light&lt;&#x2F;code&gt; mode.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lecture-1-introduction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#lecture-1-introduction&quot; aria-label=&quot;Anchor link for: lecture-1-introduction&quot;&gt;Lecture 1 - Introduction&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-1-perceptron&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-1-perceptron&quot; aria-label=&quot;Anchor link for: 1-1-perceptron&quot;&gt;1.1 Perceptron&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_perceptron.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;If we denote $\widehat{y}$ as the output:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\begin{array}{c}
\widehat{y}=g\left(w_{0}+\sum_{i=1}^{m} x_{i} w_{i}\right)
\end{array}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Being $g$ , for example, a Sigmoid, Tangent or ReLU function:&lt;&#x2F;p&gt;
&lt;p&gt;$$
g(z)=\frac{1}{1+e^{-z}} \quad , \quad g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \quad , \quad g(z)=\max (0, z)
$$&lt;&#x2F;p&gt;
&lt;p&gt;The purpose of activation functions is to introduce non-linearity into the network:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_linear.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Linear activation functions produce linear decisions no matter the network size while non-linearities allow approximating arbitrarily complex functions.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-2-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-2-neural-networks&quot; aria-label=&quot;Anchor link for: 1-2-neural-networks&quot;&gt;1.2 Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Taking the previous perceptron and simplifying the output to be $z$:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_image2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We can try with different weights, that would produce different outputs $z_1$ and $z_2$:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_image3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Neural Network is made stacking those different outputs. Notice that this is just a stack of dot products of the same features and different weights ($W^{(1)}$).&lt;&#x2F;p&gt;
&lt;p&gt;These outputs in the hidden layer have a different range of values, but there are only 2 possible final outputs: $\widehat{y_1}$ and $\widehat{y_2}$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How we classify a label as $\widehat{y_1}$ or $\widehat{y_2}$.?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In this step the non-linear or transformation function g$ trigger the outcomes to being one or the other.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the outcome value is more than the function threshold, the outcome is transformed to 1 (the label of $\widehat{y_1}$).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;If the value is less than the threshold, the outcome is transformed to 0 (the label of $\widehat{y_2}$).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_network.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Neural Network application in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;tensorflow &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;tf
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;tf.keras.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Sequential&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Hidden layers with n neurons
&lt;&#x2F;span&gt;&lt;span&gt;        tf.keras.layers.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Output layer with 2 neurons
&lt;&#x2F;span&gt;&lt;span&gt;        tf.keras.layers.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;em&gt;Dense&lt;&#x2F;em&gt; means that the layers are fully connected, all the neuron&#x27;s weight counts in the dot product calculation.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-3-forward-propagation-in-matrix-notation-extra-explanation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-3-forward-propagation-in-matrix-notation-extra-explanation&quot; aria-label=&quot;Anchor link for: 1-3-forward-propagation-in-matrix-notation-extra-explanation&quot;&gt;1.3 Forward propagation in Matrix notation (extra explanation)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;For example, let&#x27;s say that we have 3 observations, we know 2 features of them, and we want to construct a Neural Network with 1 hidden layer containing 3 neurons.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;In a first step (1), we calculate manually the dot product of $X$ and $W^{(1)}$:&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$$Z = XW^{1}$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The shape of $Z$ is always a product of: &lt;em&gt;(observations, features) x (features, n neurons in the layer)&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The columns of the first element have to be equal to the rows of the second element. It is necessary for matrix calculation.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The second step (2), we take the outputs of the hidden layer, apply the non-linear transformation, and calculate the dot product with respect to the second layer of weights:&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;$\widehat{y} = g(Z)W^{2}$&lt;&#x2F;p&gt;
&lt;p&gt;Here is an example of how to calculate $\widehat{y}$ using the dot product for a made-up dataset:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_matrix.jpg&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The final output is 3 predictions (&lt;em&gt;real numbers&lt;&#x2F;em&gt;) for the 3 observations. Imagine that all the notations denoted with $w$ are constants chosen randomly. Then, every matrix product is also constants as the only variable that is an incognita are these weights.&lt;&#x2F;p&gt;
&lt;p&gt;Weight updating is made by the network by backward propagation (later explained).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-4-deep-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-4-deep-neural-networks&quot; aria-label=&quot;Anchor link for: 1-4-deep-neural-networks&quot;&gt;1.4 Deep Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To make a Neural Network deep, we just add more layers. The number of layers and the number of neurons of each layer has to be defined beforehand (parameters to optimize) by us, humans. The model is only tunning the weights.&lt;&#x2F;p&gt;
&lt;p&gt;Neural Network application in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;tensorflow &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;tf
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;tf.keras.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Sequential&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Hidden layers with n neurons
&lt;&#x2F;span&gt;&lt;span&gt;        tf.keras.layers.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Hidden layers with n neurons
&lt;&#x2F;span&gt;&lt;span&gt;        tf.keras.layers.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span&gt;n&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;# Output layer with 2 neurons
&lt;&#x2F;span&gt;&lt;span&gt;        tf.keras.layers.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;1-5-the-loss-function&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-5-the-loss-function&quot; aria-label=&quot;Anchor link for: 1-5-the-loss-function&quot;&gt;1.5 The loss function&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Initiating random values of $W$, will give a prediction. A terrible one, as the model has no idea yet if the prediction is good, or how to measure how good is it.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The measure of how good is a prediction is will be determined by the &lt;em&gt;Loss function&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;The &quot;Loss function&quot; measures how bad is the prediction. The final output predictions compares the predicted values with the actual ones:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{L}\left(f\left(x^{(i)} ; \boldsymbol{W}\right), y^{(i)}\right)
$$&lt;&#x2F;p&gt;
&lt;p&gt;The more the difference, the worse the prediction as predicted values are far away from the real ones. We want to minimize the loss function.&lt;&#x2F;p&gt;
&lt;p&gt;On average, for all the $n$ observations:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\boldsymbol{J}(\boldsymbol{W})=\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(f\left(x^{(i)} ; \boldsymbol{W}\right), y^{(i)}\right)
$$&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-6-training-the-neural-network-gradient-descent-and-backpropagation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-6-training-the-neural-network-gradient-descent-and-backpropagation&quot; aria-label=&quot;Anchor link for: 1-6-training-the-neural-network-gradient-descent-and-backpropagation&quot;&gt;1.6 Training the Neural Network: Gradient Descent and Backpropagation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The final goal of every Neural Network is find the weights that achieve the lowest loss:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\boldsymbol{W}^{*}=\underset{\boldsymbol{W}}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(f\left(x^{(i)} ; \boldsymbol{W}\right), y^{(i)}\right)
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
\boldsymbol{W}^{*}=\underset{\boldsymbol{W}}{\operatorname{argmin}} J(\boldsymbol{W})
$$&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How the Neural Network finds the optimal ${W}^{*}$?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;By gradient descent. Gradient descent algorithm:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize wrights randomly.&lt;&#x2F;li&gt;
&lt;li&gt;Compute the gradient.&lt;&#x2F;li&gt;
&lt;li&gt;Update the weights according to the direction of the gradient and the learning rate.&lt;&#x2F;li&gt;
&lt;li&gt;Loop until convergence 2 and 3.&lt;&#x2F;li&gt;
&lt;li&gt;Return optimal weights.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;1-7-backpropagation&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-7-backpropagation&quot; aria-label=&quot;Anchor link for: 1-7-backpropagation&quot;&gt;1.7 Backpropagation&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In the second step, the algorithm computes the gradient by a process called backpropagation. &lt;strong&gt;Backpropagation is just the efficient application of the chain rule&lt;&#x2F;strong&gt; for finding the derivative of the loss function with respect to the neuron weights.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_backpropagation.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;When training a neural net, the goal is to find neuron parameters (weights) that cause the output of the NN to best fit the data, right? The chain rule is the way the NN can &quot;connect&quot; the loss function and outputs with the weight parametrization.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If the loss function is less than the previous value using the current weights, then the gradient is in a good direction.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;If the loss function is more than the previous, it goes in the opposite direction.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Repeat until the loss function is zero or cannot make it lower (&lt;em&gt;convergence&lt;&#x2F;em&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;When the Neural Network converged, it found a spot in the loss function that increasing or decreasing the weight values makes the loss function increasing.&lt;&#x2F;p&gt;
&lt;p&gt;Note that it might be the case that the optimal weights are not optimal for the entire loss function space because they converged in a local minimum. In practice, finding the global minimum is very difficult as the algorithm is very prompt to get stuck in these local minimums along the way of convergence.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_gradient_landscape.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-8-learning-rates&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-8-learning-rates&quot; aria-label=&quot;Anchor link for: 1-8-learning-rates&quot;&gt;1.8 Learning rates&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;The learning rate is how much increase the weight in the updating step of the gradient descent.&lt;&#x2F;strong&gt;. If the gradient calculates the direction of the algorithm to find the minimum, the learning rate sets the magnitude of every weight try.&lt;&#x2F;p&gt;
&lt;p&gt;Setting a stable learning rate is key to find the global minimums. It should be large enough that avoid local minimums, but small enough that is not being able to convergence (&lt;strong&gt;Exploding Gradient Problem or Divergence&lt;&#x2F;strong&gt;). Stable learning rates converge smoothly and avoid local minima.&lt;&#x2F;p&gt;
&lt;p&gt;In practice, a usual approach is trying a lot of different learning rates and see what works. A better one is to design an adaptative learning rate that &quot;adapts&quot; to the loss function or landscape. In this second approach, the learning rate is no longer a constant or fixed number but a rate that gets smaller or bigger depending on how large the gradient is, how fast the learning is happening, the size of the particular weights, and so forth.&lt;&#x2F;p&gt;
&lt;p&gt;In Tensorflow, these are called optimizers. They are many learning rate optimizers that make the NN coverage more quickly and generally better such as Adaptive Gradient Algorithm (Adam) or Adadelta.&lt;&#x2F;p&gt;
&lt;p&gt;Optimizers application in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tf.keras.optimizers.Adam
&lt;&#x2F;span&gt;&lt;span&gt;tf.keras.optimizers.Adadelta
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h3 id=&quot;1-9-batching-and-stochastic-gradient-descent&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-9-batching-and-stochastic-gradient-descent&quot; aria-label=&quot;Anchor link for: 1-9-batching-and-stochastic-gradient-descent&quot;&gt;1.9 Batching and Stochastic gradient descent&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;When we talked about backpropagation and computing the gradient, I did not mention how computationally expensive this can be. In practice, calculating the chain rule for hundreds of layers using the entire training set every time the algorithm loops is not feasible.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Instead of looping through the entire training set, we can pick a random sub-sample of the data. This process is also called &lt;em&gt;Batching&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; as it divides the training sets into small batches of data that feed the NN. The gradient computation is passed only through a small batch of data $B$:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\frac{\partial J(W)}{\partial W}=\frac{1}{B} \sum_{k=1}^{B} \frac{\partial J_{k}(W)}{\partial W}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Then the weights are updated accordingly and the process starts again with another sub-sample or batch.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;This process is called &lt;em&gt;Stochastic gradient descent&lt;&#x2F;em&gt;, as it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data).&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-10-regularization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-10-regularization&quot; aria-label=&quot;Anchor link for: 1-10-regularization&quot;&gt;1.10 Regularization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;A technique that &lt;strong&gt;constrains the optimization problem&lt;&#x2F;strong&gt; to discourage complex models to avoid overfitting.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Dropout&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;strong&gt;For every iteration, the Neural Network drops a percentage of the neurons.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Using Dropout the Neural Network doesn&#x27;t rely on a pathway or very heavy weighting on certain features and overfitting, making the Neural Network more prompt to generalize to new data.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_dropout.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Dropout regularization in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;tf.keras.layers.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dropout&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;p&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Early stopping&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;First, we monitor the process of minimizing the loss function of training and testing data at the same time.&lt;&#x2F;p&gt;
&lt;p&gt;When the loss function starts increasing in the test data (more difference between predicted and real outputs), stop the Neural Network.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L1_early_stopping.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lecture-2-recurrent-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#lecture-2-recurrent-neural-networks&quot; aria-label=&quot;Anchor link for: lecture-2-recurrent-neural-networks&quot;&gt;Lecture 2 - Recurrent Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;From a single perceptron, we can extend the number of inputs, neurons and yield multi-dimensional outputs:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_multi_output.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;But this multi perceptron, or Neural Network, doesn&#x27;t have a sense of time or element sequence. Every input and output is a specific time step.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_sequence.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This lack of connection between time steps is problematic in predicting problems that involves time or sequences. In a sequence, the inputs are correlated with each other. They are not independent. For example, future sales in a given shop are correlated with previous sales, they are not independent events.&lt;&#x2F;p&gt;
&lt;p&gt;Expressing it in the above graph, the output of $\hat{y}_2$ not only depends on $X_2$, but also on $X_0$ and $X_1$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-1-the-missing-piece-the-cell-state&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-1-the-missing-piece-the-cell-state&quot; aria-label=&quot;Anchor link for: 2-1-the-missing-piece-the-cell-state&quot;&gt;2.1 The missing piece, the Cell state&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;To make use of the correlation of the inputs in sequence, the network would need to have a connection that allows to look forward. This connection is called internal memory or &lt;strong&gt;cell state&lt;&#x2F;strong&gt; $h_t$:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_multi_output.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The memory or cell state pass the current information in the step $t$ to the next step $t+1$&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-2-recurrent-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-2-recurrent-neural-networks&quot; aria-label=&quot;Anchor link for: 2-2-recurrent-neural-networks&quot;&gt;2.2 Recurrent Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Recurrent Neural Networks are the result of incorporating the idea of using cell states to pass throw information between time steps. &lt;strong&gt;They can be thought of as multiple copies of the same network, each passing the new cell state value to a successor network&lt;&#x2F;strong&gt;. Every network is a time step of the &lt;em&gt;global&lt;&#x2F;em&gt; neural network.&lt;&#x2F;p&gt;
&lt;p&gt;RNNs have a state $h_t$, that is updated at each time step as a sequence is processed. The recurrent relation applied at each and every time step is defined as:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_rec_rel.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The function is going to be parametrized by a set of weights that is leaned throughout training the model. &lt;strong&gt;The same function and the very same parameters are applied every step of processing the sequence (every iteration of the model)&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_rnn_ac.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$W_{xh}$ denotes the weight matrix optimized for that specific step of the sequence.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$W_{hh}$ denotes the weight matrix of the memory cell, reused every step for the entire sequence.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$W_{hy}$ denotes the weight matrix of a combination of both the specific optimization of the weights for that step, and the memory cell matrix.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In practice, you won&#x27;t see the cell states weighting the outputs of the next step outputs, or multiple networks one after the other. The loop is made inside one single architecture. The RNN algorithm can be simplified as:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_rnn_eq.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-3-examples-of-rnn-application&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-3-examples-of-rnn-application&quot; aria-label=&quot;Anchor link for: 2-3-examples-of-rnn-application&quot;&gt;2.3 Examples of RNN application&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Recurrent Neural Networks are usually used in text problems such as sentiment classification, text generation from an image, generation of image title or translation.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_cat_words.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This is an example using &lt;strong&gt;many&lt;&#x2F;strong&gt; words &lt;strong&gt;to predict the one&lt;&#x2F;strong&gt; next word in the sentence. Depending on the problem, the number of inputs and outputs change, that modify the NN architecture:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_examples_rnn.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-4-making-neural-networks-understand-text-embedding&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-4-making-neural-networks-understand-text-embedding&quot; aria-label=&quot;Anchor link for: 2-4-making-neural-networks-understand-text-embedding&quot;&gt;2.4 Making Neural Networks understand text: Embedding&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Neural Networks do not understand word language, or images, they only understand numbers. They require the words to be parsed as vectors or arrays of numbers:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_words.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How are this vectors made?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The computer&#x2F;algorithm gets all the words and create a &lt;strong&gt;vocabulary&lt;&#x2F;strong&gt; with them.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Then, it creates its own dictionary to understand them, assigning a number to each different word (&lt;strong&gt;indexing&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The numbers form vectors of a fixed size that captures the content of the word (&lt;strong&gt;embedding&lt;&#x2F;strong&gt;).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;By using vectors and not single numbers, you can compare how close are vectors to each other. And comparing distance is key because the words that usually go together in a phase must be represented by vectors close to each other. For example, the vector of &lt;em&gt;dog&lt;&#x2F;em&gt; is closer to the vector of &lt;em&gt;cat&lt;&#x2F;em&gt; than to the vector of &lt;em&gt;sad&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Embedding gather words together by similarity using the distance between vectors.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_embedding.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-5-model-design-criteria-or-why-rnn-are-good&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-5-model-design-criteria-or-why-rnn-are-good&quot; aria-label=&quot;Anchor link for: 2-5-model-design-criteria-or-why-rnn-are-good&quot;&gt;2.5 Model Design Criteria, or why RNN are good&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Any recurrent model architecture must the following design criteria:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Must handle variable-length sequences (RNN ✔️)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_length.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Must track long-term dependencies (RNN ✔️)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_long_dep.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Must maintain information about order (RNN ✔️)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_order.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Must share parameters across the sequence (RNN ✔️)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;In RNNs the same memory cell is reused every step for the entire sequence, as explained previusly.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-6-rnn-illustrated-example-from-michael-phi&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-6-rnn-illustrated-example-from-michael-phi&quot; aria-label=&quot;Anchor link for: 2-6-rnn-illustrated-example-from-michael-phi&quot;&gt;2.6 RNN Illustrated example (from Michael Phi)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Let&#x27;s say that we want to do a many-to-one prediction in which the inputs are words in this cereal review and the output is a positive or negative sentiment analysis.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*YHjfAgozQaghcsEvsBEu2g.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;First the words are transformed to vectors by embedding.&lt;&#x2F;p&gt;
&lt;p&gt;From:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_LSTM_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;To:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*AQ52bwW55GsJt6HTxPDuMA.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*o-Cq5U8-tfa1_ve2Pf3nfg.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For every of theses steps or layers, the input and previous hidden state are combined to form a vector. It goes through a tanh activation, and the output is the new hidden state $h_t$. The tanh function ensures that the values stay between -1 and 1.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*WMnFSJHzOloFlJHU6fVN-g.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-7-backpropagation-through-time-bptt&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-7-backpropagation-through-time-bptt&quot; aria-label=&quot;Anchor link for: 2-7-backpropagation-through-time-bptt&quot;&gt;2.7 Backpropagation Through Time (BPTT)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The usual NN backpropagation algorithm:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Take the derivative (gradient) of the loss with respect to each parameter $W$.&lt;&#x2F;li&gt;
&lt;li&gt;Shift parameters to minimize loss.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;With a basic Neural Network, the backpropagation errors goes trough a single feedforward network for a single time step.&lt;&#x2F;p&gt;
&lt;p&gt;Recurrent Network backpropagation needs a twist, as it contains multiple steps and a memory cell. In RNNs, &lt;strong&gt;the errors are backpropagating from the overall loss through each time step&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_BPTT.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The key difference is that the gradients for $W$ at each time step are summed. A traditional NN doesn&#x27;t share parameters across layers. Every input is different and have different weights $W$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-8-problems-with-backpropagation-in-rnn&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-8-problems-with-backpropagation-in-rnn&quot; aria-label=&quot;Anchor link for: 2-8-problems-with-backpropagation-in-rnn&quot;&gt;2.8 Problems with backpropagation in RNN&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Computing the gradient with respect to the initial $h_0$ involves many matrix multiplications between the memory cell $h_t$ and the weights $W_hh$.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;exploiting-gradients-gradients-1-0&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#exploiting-gradients-gradients-1-0&quot; aria-label=&quot;Anchor link for: exploiting-gradients-gradients-1-0&quot;&gt;Exploiting gradients (gradients &amp;gt; 1.0)&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;In the the process of backpropagation the gradients get multiplied by each other over and over again. If they are larger than 1.0, the end matrix of weights is huge.&lt;&#x2F;p&gt;
&lt;p&gt;As a silly example: 0.5 times 1.5 is 0.75, 0.5 times 1.5^200 is 8.2645996e34. This can give you a perspective of how matrix multiplication can &quot;exploit&quot; by multiplying constantly by 1.X.&lt;&#x2F;p&gt;
&lt;p&gt;These huge gradients can become extremely large as the result of matrix and the loss function cannot be minimized.&lt;&#x2F;p&gt;
&lt;p&gt;The usual solution is change the derivative of the errors before they propagate through the network, so they don&#x27;t become huge. Basically, you can create a threshold that the gradients cannot surpass. &lt;em&gt;Create a threshold&lt;&#x2F;em&gt; means that you set a value, such as 1.0, that forces the values to be 1.0 at maximum.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;avoid-exploiting-gradients-gradient-thresholds&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#avoid-exploiting-gradients-gradient-thresholds&quot; aria-label=&quot;Anchor link for: avoid-exploiting-gradients-gradient-thresholds&quot;&gt;Avoid exploiting gradients: Gradient thresholds&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;There are two ways to create these thresholds:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. Gradient Norm Scaling&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Gradient norm scaling rescales the matrix so the gradient equals 1.0 if the a gradient exceeds 1.0.&lt;&#x2F;p&gt;
&lt;p&gt;Gradient Norm Scaling in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;  opt &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;SGD&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;lr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;clipnorm&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1.0&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;2. Gradient Value Clipping&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Gradient value clipping simply forces all the values above the threshold to be the threshold, without changing the matrix. If the clip is 0.5, all the gradient values less than -0.5 are set to -0.5 and all the gradients more than 0.5 set to 0.5.&lt;&#x2F;p&gt;
&lt;p&gt;Gradient Norm Scaling in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span&gt;  opt &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;SGD&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;lr&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.01&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;clipvalue&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;0.5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h4 id=&quot;vanishing-gradients-gradients-1&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#vanishing-gradients-gradients-1&quot; aria-label=&quot;Anchor link for: vanishing-gradients-gradients-1&quot;&gt;Vanishing gradients (gradients &amp;lt; 1)&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;As gradients can become huge they can also become tiny to the point that it is not possible to effectively train the network.&lt;&#x2F;p&gt;
&lt;p&gt;This is a problem because the errors further back in time are not being propagated. It would cause that the long-term errors are vanished and bias the model only to capture short-term dependencies.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;avoid-vanishing-gradients&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#avoid-vanishing-gradients&quot; aria-label=&quot;Anchor link for: avoid-vanishing-gradients&quot;&gt;Avoid vanishing gradients&lt;&#x2F;a&gt;&lt;&#x2F;h4&gt;
&lt;p&gt;The basic recipe to solve vanishing gradients is use a ReLU activation function, chaning to a smart weight initialization and&#x2F;or use a different RNN architecture.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. Change activation function to ReLU.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Why ReLu?&lt;&#x2F;p&gt;
&lt;p&gt;Because when the cell or instance gets activated (weight 0 or more), by definition the derivative or gradient is 1.0 or more:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_activation_trick.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. Change weight initialization.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For example to the &lt;strong&gt;Xavier initialization&#x2F;Glorot initialization&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;Changing the weight activation in Tensorflow:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;keras.models &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;Sequential
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;from &lt;&#x2F;span&gt;&lt;span&gt;keras.layers &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;Dense, Activation
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Sequential&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;16&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;input_shape&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1&lt;&#x2F;span&gt;&lt;span&gt;,&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;5&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;relu&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;32&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;relu&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kernel_initializer&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;glorot_uniform&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;    &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;=&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;softmax&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;&lt;strong&gt;3. Change Network architecture.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;More complex RNNs such as &lt;strong&gt;LSTM or GRU&lt;&#x2F;strong&gt; can control the information that is passing through. Long Short Term Memory networks (&lt;strong&gt;LSTM&lt;&#x2F;strong&gt;) and Gated Recurrent Units (&lt;strong&gt;GRU&lt;&#x2F;strong&gt;) are special kinds of RNN, capable of learning long-term dependencies.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_activation_trick3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;They can keep informed of long-term dependencies &lt;strong&gt;using filters or gates&lt;&#x2F;strong&gt;. In essence, these gates decide how much information to keep of the previous neuron state or values, and how much to drop. This makes the optimization problem or the Neural Network less prompt to vanishing or exploding gradient problems.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-9-long-short-term-memory-networks-lstm&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-9-long-short-term-memory-networks-lstm&quot; aria-label=&quot;Anchor link for: 2-9-long-short-term-memory-networks-lstm&quot;&gt;2.9 Long Short-Term Memory networks (LSTM)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In a simple RNN, the information goes though every step with the input of that time step ($x_t$), the previous step memory cell ($h_{t-1}$) and an output for every step ($y_t$).&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_rnn_arq.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The structure of a LSTM is more complex. &lt;strong&gt;LSTM forces the matrix inputs in every step to go through gates&lt;&#x2F;strong&gt;, or internal mechanism to keep long-term information.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L2_lstm_arq.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;LSTM Gates system&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;They 4 types of gates interacting within each step layer:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Forget gate&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;: Remove the irrelevant information.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Information from the previous hidden state and the current input is passed through the sigmoid function. Values come out between 0 and 1.&lt;&#x2F;p&gt;
&lt;p&gt;The closer to 0 means to forget, and the closer to 1 means to keep.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*GjehOa513_BgpDDP6Vkw2Q.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Store gate&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;: Store relevant information.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The same previous $h_{t-1}$ and the current inputs goes into two transformations:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sigmoid transformation. It is the same operation as before, but in another gate. Instead of forget and keep, it will decide the information to update or not update.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Than transformation. It helps to regulate the network by squishing values between -1.0 and 1.0.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The matrix multiplication of the tanh outputs with the sigmoid outputs decides which information is important, and store it in a cell state $\bigotimes$.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*TTmYy7Sy8uUXxUXfzmoKbA.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Update gate&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;: update the separated cell state.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The update gate takes the previous cell state vector $c_{t-1}$ and multiply by the forget vector (from the forget gate), that allows to drop non-important information.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Then, it adds the store vector from the store gate, as this information is important to keep from the current step.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*S0rXIeO_VoUVOyrYHckUWg.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The update gate takes the information to the other 2 gates to decide what to forget and what to keep, updating the cell state.&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;&lt;strong&gt;&lt;em&gt;Output gate&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;: decides what the next hidden state $h_{t+1}$.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;The previous hidden state and the current input into a sigmoid function.&lt;&#x2F;li&gt;
&lt;li&gt;Then the newly modified cell state pass the tanh function.&lt;&#x2F;li&gt;
&lt;li&gt;By multiplying the two vectors it decides what information the hidden state should carry.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*VOXRGhOShoWWks6ouoDN3Q.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-10-gated-recurrent-unit-networks-gru&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-10-gated-recurrent-unit-networks-gru&quot; aria-label=&quot;Anchor link for: 2-10-gated-recurrent-unit-networks-gru&quot;&gt;2.10 Gated Recurrent Unit networks (GRU)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s. There isn’t a clear winner which one is better, try both to determine which one works better for their use case.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;miro.medium.com&#x2F;max&#x2F;1400&#x2F;1*jhi5uOm9PvZfmxvfaCektw.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lecture-3-convolutional-neural-network&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#lecture-3-convolutional-neural-network&quot; aria-label=&quot;Anchor link for: lecture-3-convolutional-neural-network&quot;&gt;Lecture 3 - Convolutional Neural Network&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;3-1-computer-vision-introduction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-1-computer-vision-introduction&quot; aria-label=&quot;Anchor link for: 3-1-computer-vision-introduction&quot;&gt;3.1 Computer Vision Introduction&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;We can train computers to understand the world of images, mapping where things are, what actions are taking place, and making them to predict and anticipate events in the world. For example, in this image, the computer can pick up that people are crossing the street, so the black car must be not moving.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_cars.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What computers &lt;em&gt;see&lt;&#x2F;em&gt; ?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Task that for us are trivial, for a computer is not. To a computer, the images are 2-dimensional arrays of numbers.&lt;&#x2F;p&gt;
&lt;p&gt;Taking the following image, we are able to see that is a Lincoln portrait but the computer sees a 1080x1080x3 vector of numbers.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_lincoln.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The classification of an image by a computer is made by picking up clues, or features, from the image. If the particular features of the image are more present in Lincoln images, it will be classified as Lincoln.&lt;&#x2F;p&gt;
&lt;p&gt;The algorithm, to perform this task well, should be able to differentiate between unique features and modifications of the same features. For example, it should classify as &quot;Dog&quot; a photo of dogs taken from different angles or a dog hidden in a tree.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_transformation_images.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The computer must be invariant of all those variations, as humans recognize the same image changing its viewpoint or scale.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-2-learning-visual-features&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-2-learning-visual-features&quot; aria-label=&quot;Anchor link for: 3-2-learning-visual-features&quot;&gt;3.2 Learning Visual features&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Computers learn hierarchically from the features&lt;&#x2F;strong&gt; in an image. For example, in face recognition the algorithm learn in order:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Facial structure.&lt;&#x2F;li&gt;
&lt;li&gt;Eyes, ears, nose.&lt;&#x2F;li&gt;
&lt;li&gt;Edges, dark spots&lt;&#x2F;li&gt;
&lt;li&gt;...&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;A fully connected neural network can take as input an image in the shape of a 2D number array, and classify it. What would be the problem of using a Multilayer Perceptron to classify images?&lt;&#x2F;p&gt;
&lt;p&gt;It&#x27;s not able to capture is no &lt;strong&gt;spatial information&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;If each feature of the image is an individual characteristic, all the connections between the image characteristics are lost. For example, a MLP architecture is not able to pick that the inner array of pixels the ears must be close to the outer array of pixels of the facial structure.&lt;&#x2F;p&gt;
&lt;p&gt;How can we use spatial structure in the input to inform the architecture of the network?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-3-patching&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-3-patching&quot; aria-label=&quot;Anchor link for: 3-3-patching&quot;&gt;3.3 Patching&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Spatial 2D pixel arrays are correlated to each other. By using a spatial structure, it would preserve the correlation of the pixels and its spatial architecture.&lt;&#x2F;p&gt;
&lt;p&gt;We can think about a neural network architecture that takes different parts of the images in different layers and connects somehow the images. How would looks like?&lt;&#x2F;p&gt;
&lt;p&gt;In a neural network with spatial structure each neuron takes a small pixel of the entire image and try to extract it&#x27;s feature information. Only a small region of the image, a &lt;strong&gt;patch&lt;&#x2F;strong&gt;, affects a concrete neuron and not the entire image.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_patches.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The next neuron afterwards takes a shifted patch of pixels. The process is repeated for all the neurons until the entire image is taken as input by patches&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;As you can see in the image below , some of the patched pixels took from the first neuron in the left overlap some of the pixels patched in the right neuron.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_patches_connected.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The overlapping of pixels preserves the spatial component of the image. Every patch is intended to reveal features characteristic of the image.&lt;&#x2F;p&gt;
&lt;p&gt;But...how the algorithm learn the features? How it knows to detect the ears or eyes in a patch? The process is called &lt;em&gt;local feature extraction&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-4-local-feature-extraction&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-4-local-feature-extraction&quot; aria-label=&quot;Anchor link for: 3-4-local-feature-extraction&quot;&gt;3.4 Local feature Extraction&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The neural network identify the features patches by weighting the pixels.&lt;&#x2F;p&gt;
&lt;p&gt;Take the following image. The idea is that the neural network have to classify the right image as an X or not a X.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_xisx.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;While for us humans is simple to see that is an X, the pixel arrays do not match. After all, computers cannot see images, only arrays of numbers that do not match.&lt;&#x2F;p&gt;
&lt;p&gt;By the process of patching, the neural network takes images with different pixel position that share same features:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_xisxfeatures.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Multiple patches in the X images are similar, or equal.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How the model calculates this similarity?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;By &lt;strong&gt;the convolutional operation&lt;&#x2F;strong&gt;. While the name seems scary, it is just multiplying each pixel value element-wise between the filter matrix (&lt;em&gt;real X patch&lt;&#x2F;em&gt;) and the patch of the input image, and adding the outputs together.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_convolutional_operation.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In other words, comparing the pixels between the &lt;em&gt;&quot;proper X patch&quot;&lt;&#x2F;em&gt; and the input patch that &quot;&lt;em&gt;might or might not be an X patch&lt;&#x2F;em&gt;&quot;, in an a numerical way.&lt;&#x2F;p&gt;
&lt;p&gt;By going through local patches, the algorithm can identify and extract local features for each patch:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_convolutional_operation_gif.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The end matrix from the convolutional operation is called &lt;strong&gt;feature map&lt;&#x2F;strong&gt;, as it mapped the features of the input image.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-5-convolutional-neural-netowrk-operations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-5-convolutional-neural-netowrk-operations&quot; aria-label=&quot;Anchor link for: 3-5-convolutional-neural-netowrk-operations&quot;&gt;3.5 Convolutional Neural Netowrk operations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;CNNs are neural networks that apply the concept of patching, and are able to learn from spatial numerical arrays. &lt;strong&gt;The word &lt;em&gt;Convolutional&lt;&#x2F;em&gt; is a way too say that this neural network architecture handles cross-correlated 2D arrays of numbers.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Three CNN core operations are:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Convolution.&lt;&#x2F;li&gt;
&lt;li&gt;Apply a non-linear filter, often ReLU.&lt;&#x2F;li&gt;
&lt;li&gt;Pooling: a downsampling operation that allows to scale down the size of each feature map.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_CNN.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. Convolution, or Convolutional Operations.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The operation described in the above section. Each neuron takes &lt;strong&gt;only the input from the patch&lt;&#x2F;strong&gt;, computes the weighted sum, and applies bias that passes through a non-linear function (as usual in NN). Every neuron takes a different shifted patch.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_feature_map.gif&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Take into account that there are not only one feature map in the neural network. &lt;strong&gt;A feature map is specific for a feature&lt;&#x2F;strong&gt;. As images have multiple features, multiple feature map or layers are needed.&lt;&#x2F;p&gt;
&lt;p&gt;Think about a human portrait. Taking only the feature &lt;em&gt;&quot;oval shape of the face&quot;&lt;&#x2F;em&gt; the algorithm could confuse a potato as a human face, as is oval as well.&lt;&#x2F;p&gt;
&lt;p&gt;By applying multiple filters, or layers, the CNN learns hierarchically from the features in an image.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. ReLU filter.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;After each convolutional operation, it needed to apply a ReLU activation function to the output volume of that layer.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why using a ReLU activation function?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For any given neuron in the hidden layer, there are two possible (fuzzy) cases: either that neuron is relevant, or it isn’t. We need a function that shuts down the non-relevant neurons that do not contain a positive value.&lt;&#x2F;p&gt;
&lt;p&gt;ReLU replaces all the negative values with zero and keeps all the positive values with whatever the value was.&lt;&#x2F;p&gt;
&lt;p&gt;Think it this way: if the output of the convolutional operation is negative it means that the sample image patch doesn&#x27;t look similar to the real image patch. We don&#x27;t care how different it looks (how negative is the output), we only want that this neuron is not taken into account to train the model.&lt;&#x2F;p&gt;
&lt;p&gt;ReLU is also computationally cheap in comparison with other non-linear functions. It involves only a comparison between its input and the value 0.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;3. Pooling.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Pooling is an operation to &lt;strong&gt;reduce the dimensionality&lt;&#x2F;strong&gt; of the inputs while still &lt;strong&gt;preserving spatial invariants&lt;&#x2F;strong&gt;. For example, a MaxPool2D takes a 4x4 patch matrix and convert it into a 2x2 patch by taking only the maximum value of each patch:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_maxpool.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-6-convolutional-neural-netowrka-for-image-classification&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-6-convolutional-neural-netowrka-for-image-classification&quot; aria-label=&quot;Anchor link for: 3-6-convolutional-neural-netowrka-for-image-classification&quot;&gt;3.6 Convolutional Neural Netowrka for Image Classification&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Using CNNs for image classification can be broken down into 2 parts: learning and classification.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;1. Feature learning.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The convolutional, ReLU and pooling matrix operations, the model to learn the features from an images. These feature maps get the important features of an image in the shape of weighted 2D arrays.&lt;&#x2F;p&gt;
&lt;p&gt;For example, a CNN architecture can learn from a set of images of cars and then distinguish between &lt;em&gt;car&lt;&#x2F;em&gt; features and &lt;em&gt;not car&lt;&#x2F;em&gt; features using the three key operations, but is still unable to classify images into labels.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;2. Classification part.&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;The second part of the CNN structure is using a second normal MPL to classify the label of the image&lt;&#x2F;strong&gt;. After capturing the features of a car by convolutional operations and pooling, the lower-dimensional feature arrays feed this neural network to perform the classification.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L3_CNN_classification_prob.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Why not using a second CNN structure or any other NN complex architecture?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Because you don&#x27;t need a neural network that handle sense of space or cross-corrlation for this task. It is a simple classification task. The inputs are not even an image anymore, they are features coded as number vectors. They don&#x27;t need patching.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Softmax function&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Given that the classification is into more than one category, the neural network output is filtered with a &lt;strong&gt;softmax non-linear function to get the results in terms of probabilities&lt;&#x2F;strong&gt;. The output of a softmax represents a categorical probability distribution. Following the car classification example, if the input image is a car it could give a 0.85 probability of being a car, 0.05 of being a van, a 0.01 of being a truck, and so forth.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-7-code-example&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#3-7-code-example&quot; aria-label=&quot;Anchor link for: 3-7-code-example&quot;&gt;3.7 Code example&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;CNN &quot;vehicle classifier&quot; in Tensorflow:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;filters&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; refers to the number of feature maps. For the first layer we set 32 feature maps, for the second 64.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;kernel_size&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; refers to the height and width of the 2D convolution window. 3 means 3x3 pixel window patching.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;strides&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; refers to how far the pooling window moves for each pooling step. With stride 2, the neurons moves in 2x2 pixels windows.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;pool_size&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; refers to the window size over which to take the maximum when calculating the pooling operation. With 2, it will take the max value over a 2x2 pooling window.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;units&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt; refers to the number of outputs. 10 lasting outputs representing the 10 classes of vehicles.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;pre data-lang=&quot;python&quot; style=&quot;background-color:#002b36;color:#839496;&quot; class=&quot;language-python &quot;&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;import &lt;&#x2F;span&gt;&lt;span&gt;tensorflow &lt;&#x2F;span&gt;&lt;span style=&quot;color:#cb4b16;&quot;&gt;as &lt;&#x2F;span&gt;&lt;span&gt;tf
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;def &lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;vehicles_classifier_CNN&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;():
&lt;&#x2F;span&gt;&lt;span&gt;  model &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;tf.keras.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Sequential&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;([
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;########First part: Feature learning ########
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## CONVOLUTION + RELU
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Conv2D&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;filters &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;32&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kernel_size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;relu&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## POOLING
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;MaxPool2D&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;pool_size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;strides &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## CONVOLUTION + RELU
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Conv2D&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;filters &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;64&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;kernel_size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;3&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;                        &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;relu&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## POOLING
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;MaxPool2D&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;pool_size &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;strides &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;2&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;######## Second part: Classification ########
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## FLATTEN
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Flatten&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;()&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## FULLY CONNECTED
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;units &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;1024&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;relu&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)&lt;&#x2F;span&gt;&lt;span&gt;,
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#586e75;&quot;&gt;## SOFTMAX
&lt;&#x2F;span&gt;&lt;span&gt;  tf.keras.layer.&lt;&#x2F;span&gt;&lt;span style=&quot;color:#b58900;&quot;&gt;Dense&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;(&lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;units &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span style=&quot;color:#6c71c4;&quot;&gt;10&lt;&#x2F;span&gt;&lt;span&gt;, &lt;&#x2F;span&gt;&lt;span style=&quot;color:#268bd2;&quot;&gt;activation &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;= &lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#2aa198;&quot;&gt;softmax&lt;&#x2F;span&gt;&lt;span&gt;&amp;#39;&lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;)
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#657b83;&quot;&gt;])
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;  &lt;&#x2F;span&gt;&lt;span style=&quot;color:#859900;&quot;&gt;return &lt;&#x2F;span&gt;&lt;span&gt;model
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;lecture-4-deep-generative-modeling&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#lecture-4-deep-generative-modeling&quot; aria-label=&quot;Anchor link for: lecture-4-deep-generative-modeling&quot;&gt;Lecture 4 - Deep Generative Modeling&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Deep Generative Modeling is part of &lt;strong&gt;unsupervised learning: the models receive the data but not the respective labels&lt;&#x2F;strong&gt;. The goal is to take as input the training samples from some distribution and learn a model that represents that distribution.&lt;&#x2F;p&gt;
&lt;p&gt;Another way to define this goal is to &lt;strong&gt;find ways to learn the underlying and hidden latent variables&lt;&#x2F;strong&gt; in the data even when the generative model is only given the representation of the variables.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_images_generated.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Deep generative models are very useful to create synthetic samples using the probability density function of the samples provided.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-1-use-examples&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-1-use-examples&quot; aria-label=&quot;Anchor link for: 4-1-use-examples&quot;&gt;4.1 Use examples&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Debiasing image recognition&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Let&#x27;s say that you have a silly algorithm that takes facial expressions and the goal is classifying between &lt;em&gt;pretty&lt;&#x2F;em&gt; or &lt;em&gt;non pretty&lt;&#x2F;em&gt;. However, all your faces are either &lt;em&gt;white-blond-people smiling at the camera&lt;&#x2F;em&gt; or portraits of &lt;em&gt;drug addicts&lt;&#x2F;em&gt;. This algorithm won&#x27;t create a boundary between pretty and not, it would define a boundary between white-blond-smiling people and drug users. Generative models can follow the facial distribution of the existing sample to create new samples of portraits with different skin tones, postures and attributes.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Outlier detection in images&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Rare events in tail distributions, such as people crossing the street in red, accidents, or sudden impacts can be created by generative models as samples to train the model of self-driving cars. The benefit is that the car would know what to do in these extreme scenarios even if it hasn&#x27;t seen it before in the sample.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-2-autoencoding&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-2-autoencoding&quot; aria-label=&quot;Anchor link for: 4-2-autoencoding&quot;&gt;4.2 Autoencoding&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Autoencoding means &lt;strong&gt;auto&lt;&#x2F;strong&gt;matically &lt;strong&gt;enconding&lt;&#x2F;strong&gt; data. In Generative Modeling, the &lt;em&gt;Encoder&lt;&#x2F;em&gt; learns to map from the data $x$ into a low-dimensional vector $z$:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_autoencoder.png&quot; alt=&quot;&quot; title=&quot;Autoencoders: background&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Autoencoding is a form of compression&lt;&#x2F;strong&gt;. A smaller dimensionality of the latent space means that we can compress the data into smaller latent factors that keep the feature representation.&lt;&#x2F;p&gt;
&lt;p&gt;However, the dimensionality of the latent space will also influence the reconstruction quality. The smaller the latent space the poorer and less quality the generated images have, as will force a larger training to bottleneck.&lt;&#x2F;p&gt;
&lt;p&gt;But wait, the input data has no labeled. Therefor, $z$ cannot be a &lt;em&gt;feature map&lt;&#x2F;em&gt; of the attributes of 2s, as this algorithm doesn&#x27;t know is a 2 in the first place!&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What is this $z$ then?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;$z$ is vector of latent variables&lt;&#x2F;strong&gt;. It represent the features of the image in a lower dimensional vector space, in this case the features of a 2.&lt;&#x2F;p&gt;
&lt;p&gt;The model uses the features created in this latent space $z$ to construct a new observations $\hat{x}$ following the features of the original $x$. It &quot;decodes&quot; the original images to create new images.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;How the algorithm knows that the atributes in $z$ are right?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The model learns by comparing the difference between the new synthetic image and the original image in terms of pixels.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_autoencoder2.png&quot; alt=&quot;&quot; title=&quot;Autoencoders: mapping the latent space&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Therefore, can be trained to minimize the Mean Squared Error between the sample inputs $x$ and ouput synthetic samples $\hat{x}$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-3-variational-autoencoders-vaes&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-3-variational-autoencoders-vaes&quot; aria-label=&quot;Anchor link for: 4-3-variational-autoencoders-vaes&quot;&gt;4.3 Variational Autoencoders (VAEs)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;In the previous image, the latent space $z$ acts as a &quot;normal&quot; layer in a Neural Network. Is deterministic in the sense that it would yield the same latent variable space $z$ every time we use the same image as input.&lt;&#x2F;p&gt;
&lt;p&gt;In contrast, &lt;strong&gt;VAEs impose a variational or stochastic&lt;&#x2F;strong&gt; twist to the architecture to generate smoother and different representations of the images:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_autoencoder3.png&quot; alt=&quot;&quot; title=&quot;Variational Autoencoders&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For each variable, the VAE learns a mean and a variance associated with that latent variable. Instead of using the vector latent variables $z$ straight, the model uses the vector of means and a vector of variances to define the probability distributions for each of the latent variables.&lt;&#x2F;p&gt;
&lt;p&gt;The goal of this twist is to generate slightly different new images from the samples, not to imitate perfectly them perfectly.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-4-vae-operations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-4-vae-operations&quot; aria-label=&quot;Anchor link for: 4-4-vae-operations&quot;&gt;4.4 VAE Operations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;VAEs optimization process can be dividing into: &lt;strong&gt;encoding and decoding&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_autoencoder4.png&quot; alt=&quot;&quot; title=&quot;Variational Autoencoders Optimization&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Encoding&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The first part of the process is called &lt;em&gt;encoding&lt;&#x2F;em&gt;, as it encode or define the latent space $z$ given $x$ observations.&lt;&#x2F;p&gt;
&lt;p&gt;Learning the structure of the input images by deconstruction, comparing the differences between the distribution of features input images and new images (log-likelihood). Optimizing the $q_{\phi}$ weights.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Decoding&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;The second part of the process is called &lt;em&gt;decoding&lt;&#x2F;em&gt;, as it decodes or extract the features of the latent space $z$ to make new observations $\hat{x}$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-5-the-vae-regularization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-5-the-vae-regularization&quot; aria-label=&quot;Anchor link for: 4-5-the-vae-regularization&quot;&gt;4.5 The VAE regularization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The training phase will change as a result of these two different tasks. The loss function cannot be only calculated as the difference in similarity between input and output images, as they must be different by definition. This is the stochastic &lt;em&gt;twist&lt;&#x2F;em&gt; necessary to create new images, not just copies.&lt;&#x2F;p&gt;
&lt;p&gt;The optimization function must include a new term, the &lt;strong&gt;VAE loss:&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;&lt;center&gt;VAE Loss function = (reconstruction loss) + (regularization term)&lt;&#x2F;center&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As in any other neural network, the regularization term avoids overfitting. In this neural network architecture overfitting would mean replicating the same exact images of $x$ into $\hat{x}$. We don&#x27;t want the same images, we want different images that follows the latent varaibles of the original sample.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_autoencoder5.png&quot; alt=&quot;&quot; title=&quot;Variational Autoencoders Optimization: Loss function&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;By adding this new parameter $D$ to the loss function, the Neural Network will try to reduce not only the errors extracting the latent variables (reconstruction loss) but also avoid overfitting the model so it doesn&#x27;t create identical copies of the input images (regularization term).&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s analyze this regularization term analytically: $D\left(q_{\phi}(\mathrm{z} \mid x) | p(z)\right)$&lt;&#x2F;p&gt;
&lt;p&gt;$D$ is a function of:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;$q_{\phi}(z \mid x)$: the encoding. Imposes to the new synthetic images $\hat{x}$ to follow a inferred latent distribution of the latent variables $z$.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;$p(z)$: the decoding. Imposes to the new synthetic images $\hat{x}$ to follow a prior &lt;strong&gt;fixed prior&lt;&#x2F;strong&gt; distribution of $z$&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Finally, the two vertical lines between the elements of the function is a reciprocal math operator. Effectively, it means that $D$ is function of the difference between the two elements, the &lt;strong&gt;inferrerd&lt;&#x2F;strong&gt; and the &lt;strong&gt;fixed prior&lt;&#x2F;strong&gt; distribution.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In other words, $D$ is a parameter that represents the divergence of what the encoder is trying to infer and a prior distribution of $z$.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;inferred&lt;&#x2F;strong&gt; distribution of $z$ is easy to understand, as it is just the latent variables of the images created by using the mean and standard deviation of each input.&lt;&#x2F;p&gt;
&lt;p&gt;However...&lt;strong&gt;What is a fixed prior distribution? How it calculates the $p(z)$ ?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-6-priors-on-the-latent-distribution&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-6-priors-on-the-latent-distribution&quot; aria-label=&quot;Anchor link for: 4-6-priors-on-the-latent-distribution&quot;&gt;4.6 Priors on the latent distribution&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The usual prior distribution choice is the &lt;strong&gt;normal Gaussian distribution&lt;&#x2F;strong&gt; (means equal 0 and standard deviations equals 1). In practical terms, this prior makes the model cut the features that are way out of a normal distribution, such as outliers or edge cases in the data.&lt;&#x2F;p&gt;
&lt;p&gt;The new samples generated $\hat{x}$ follows the inferred distribution but also this fixed prior. The loss funtion optimize the inferred latent distribution and also penalize extreme cases outside the normal distribution (&lt;em&gt;weird or non-common elements in the images&lt;&#x2F;em&gt;)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_normaldis.png&quot; alt=&quot;&quot; title=&quot;Normal Gaussian distribution used to setting prior distribution&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We said that the regularization term &lt;em&gt;D&lt;&#x2F;em&gt; is a function of the difference between the inferred latent distribution and a Gaussian prior distribution. This difference is called &lt;strong&gt;KL-divergence&lt;&#x2F;strong&gt;(Kullback-Leibler) or &lt;strong&gt;relative entropy&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;$$
D\left(q_{\phi}(\mathrm{z} \mid x) | p(z)\right) =-\frac{1}{2} \sum_{j=0}^{k-1}\left(\sigma_{j}+\mu_{j}^{2}-1-\log \sigma_{j}\right)
$$&lt;&#x2F;p&gt;
&lt;p&gt;While the form of the function looks &lt;em&gt;unfriendly&lt;&#x2F;em&gt;, it is just a measure of how one probability distribution is different from a second reference probability distribution.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-7-why-vae-regularization-is-important&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-7-why-vae-regularization-is-important&quot; aria-label=&quot;Anchor link for: 4-7-why-vae-regularization-is-important&quot;&gt;4.7 Why VAE regularization is important?&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The VAE regularization creates:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Continuity&lt;&#x2F;strong&gt;. Data points that are similar in the latent space should result in similar content after the decoding.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Completeness&lt;&#x2F;strong&gt;. New samples out of the latent space should resemble meaningful content after decoding.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Without regularization (a loss function that just tries to minimize the encoding loss), the model could group images that are similar in real life in different clusters because of the small variations. We want input images with close latent features to have very similar distributions that the model can use to create new ones.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_regularization.png&quot; alt=&quot;&quot; title=&quot;Not regularized vs regularized data points in latent and output space&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The normal prior force the latent learned distribution to overlap. For example, if we want to create faces with VAEs the fixed distribution forces images of faces to place the eyes, mouth, and ears within the same regions.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_regularization2.png&quot; alt=&quot;&quot; title=&quot;Regularized data points in latent and output space clustered in close distributions&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-8-vae-backpropagation-re-parametrization&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-8-vae-backpropagation-re-parametrization&quot; aria-label=&quot;Anchor link for: 4-8-vae-backpropagation-re-parametrization&quot;&gt;4.8 VAE Backpropagation: Re-parametrization&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Backpropagation in Neural Networks requires deterministic nodes and layers (constant weights). Weights need to remain constant to calculate the chain rule to optimize the loss by gradient descent.&lt;&#x2F;p&gt;
&lt;p&gt;But remember that &lt;strong&gt;VAs impose a variational or stochastic&lt;&#x2F;strong&gt; twist in the forward propagation to generate new images and therefore you cannot backpropagate a sampling layer.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_backpropagation.png&quot; alt=&quot;&quot; title=&quot;Backpropagation on $z$ original form&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Well, you actually can. The hidden latent space $z$ variation is not stochastic itself, it includes a random constant $\varepsilon$. Therefore, $z$ can be &lt;em&gt;reparametrized&lt;&#x2F;em&gt;, as $\varepsilon$ is just a constant that follows a normal distribution.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_backpropagation2.png&quot; alt=&quot;&quot; title=&quot;Backpropagation on $z$ reparametrized form&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Notice that $z$ goes from being a stochastic node (left) to being a deterministic one (rigth). Again, this is because $z$ can be derived taking $\varepsilon$ as a random constant that follows a normal distribution. The function loss can be minimized since the chain rule can be applied to optimize the weigths of the encoding loss $q_{\phi}$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-9-generative-adversarial-networks-gans&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-9-generative-adversarial-networks-gans&quot; aria-label=&quot;Anchor link for: 4-9-generative-adversarial-networks-gans&quot;&gt;4.9 Generative Adversarial Networks (GANs)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;GANs is another architecture to generate new data following the same distribution of the input data. The &lt;em&gt;Adversarial&lt;&#x2F;em&gt; part comes because in &lt;strong&gt;this architecture two neural networks contesting with each other in a zero-sum game where one network gain is the other network loss&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Generator Network&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This network is trained to get random noise data and &lt;strong&gt;produce new (fake) samples that represent the noise distribution&lt;&#x2F;strong&gt; as much as possible. The random noise can be created sample out of a Gaussian distribution.&lt;&#x2F;p&gt;
&lt;p&gt;There are no encoding loss, the new features extracted comes from noise. Therefore, the distribution that the model is trying to learn comes from a random sampling, not real images.&lt;&#x2F;p&gt;
&lt;p&gt;Here in the next image, the Generator Network $G$ learns from a normal Gaussian distribution $z$ and creates new samples $X_{fake}$ that follows this distribution.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_generator.png&quot; alt=&quot;&quot; title=&quot;Generator Network producing a feature latent space out of noise&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Discriminator Network&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;This network takes the fake features from the Generator Network and real features from real images data. With both inputs, &lt;strong&gt;the Discriminator task is to identify the fake features from the real ones&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L4_gan.png&quot; alt=&quot;&quot; title=&quot;Generative Adversarial Network structure&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;G&lt;&#x2F;em&gt; tries to synthesize fake instances that fool &lt;em&gt;D&lt;&#x2F;em&gt;, and &lt;em&gt;D&lt;&#x2F;em&gt; tries to identify these from real ones.&lt;&#x2F;p&gt;
&lt;p&gt;The two networks interact with each other, &lt;strong&gt;the better the Generator Network gets the hardest is for the Discriminator to tell apart&lt;&#x2F;strong&gt; fake from real features.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-10-gans-loss&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#4-10-gans-loss&quot; aria-label=&quot;Anchor link for: 4-10-gans-loss&quot;&gt;4.10 GANs Loss&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;As they have different goals, the Generator and Discriminator network have different loss functions that combines into a &lt;em&gt;total&lt;&#x2F;em&gt; GAN loss function.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Discriminator Network Loss&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;$$
\arg \max_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]
$$&lt;&#x2F;p&gt;
&lt;p&gt;It maximizes the probability of the fake data to be identified as fake: $\log D(G(\mathbf{z}))$, and the real data being identified as real: $\log (1-D(\mathbf{x})$.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Generator Netowork Loss&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;$$
\arg \min_{G} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]
$$&lt;&#x2F;p&gt;
&lt;p&gt;It minimizes the probability of the Discriminator Network &lt;em&gt;D&lt;&#x2F;em&gt; to identify fake data as fake: $\log D(G(\mathbf{z})$, and the real data being identified as real: $\log (1-D(\mathbf{x}))$.&lt;&#x2F;p&gt;
&lt;p&gt;We can combine both loss functions as the GANs Loss function:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\arg \min_{G} \max_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))]
$$&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lecture-6-reinforced-learning&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#lecture-6-reinforced-learning&quot; aria-label=&quot;Anchor link for: lecture-6-reinforced-learning&quot;&gt;Lecture 6 - Reinforced Learning&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;In Reinforced Learning the data is not a matrix of features and set of targets (supervised learning), neither a matrix of features without targets that an algorithm has to cluster (unsupervised learning). The input data is a state-action pairs.&lt;&#x2F;p&gt;
&lt;p&gt;The algorithm &lt;strong&gt;learns from the consequences of some action in a certain state&lt;&#x2F;strong&gt;. If the action contributes to a reward function, then is encourage and if it doesn&#x27;t the action is avoided. The &lt;strong&gt;goal&lt;&#x2F;strong&gt; of the algorithm is to maximize the total reward of the agent over time.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_classes.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;key concepts&lt;&#x2F;strong&gt; to understand the mechanics of Reinforced Learning are:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Actions&lt;&#x2F;strong&gt;: behaviors that the system takes when it sees those states.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Agent&lt;&#x2F;strong&gt;: It takes decisions in the environment.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Environment&lt;&#x2F;strong&gt;: Where the agent takes action.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Action space A&lt;&#x2F;strong&gt;: the set of possible actions an agent can make in the environment.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_actions.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Observations&lt;&#x2F;strong&gt;: who the environment reacts to the agent&#x27;s actions.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;State&lt;&#x2F;strong&gt;: a situation which the agent perceives.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_observations.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Reward&lt;&#x2F;strong&gt;: feedback that measures the success or failure of the agent&#x27;s action.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The &lt;strong&gt;Total Reward&lt;&#x2F;strong&gt; is a function of the sum of rewards through time:&lt;&#x2F;p&gt;
&lt;p&gt;$$
R_{t}=\sum_{i=t}^{\infty} r_{i}=r_{t}+r_{t+1} \ldots+r_{t+n}+\cdots
$$&lt;&#x2F;p&gt;
&lt;p&gt;However, the time of the reward is an important element in the choice of action of the agent. Is not the same $20 today that $20 in 10 years.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;Discounted Total Reward&lt;&#x2F;strong&gt; is calculated as the sum of rewards through time, times a discount factor between 0 and 1:&lt;&#x2F;p&gt;
&lt;p&gt;$$
R_{t}=\sum_{i=t}^{\infty} \gamma^{i} r_{i}=\gamma^{t} r_{t}+\gamma^{t+1} r_{t+1} \ldots+\gamma^{t+n} r_{t+n}+\cdots
$$&lt;&#x2F;p&gt;
&lt;p&gt;This reward function is also called the &lt;em&gt;Q-function&lt;&#x2F;em&gt;.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-1-the-q-function&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-1-the-q-function&quot; aria-label=&quot;Anchor link for: 6-1-the-q-function&quot;&gt;6.1 The Q-function&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The Q-function &lt;strong&gt;captures the expected total future reward&lt;&#x2F;strong&gt; $R$ an agent in the state $s$ can receive by executing a certain action $a$:&lt;&#x2F;p&gt;
&lt;p&gt;$$
Q\left(s_{t}, a_{t}\right)=\mathbb{E}\left[R_{t} \mid s_{t}, a_{t}\right]
$$&lt;&#x2F;p&gt;
&lt;p&gt;When the agent has to decide on an environment, the agent considers the possible Q-functions to compare rewards. The higher the Q-function the better, as the reward is higher. We will call all the possible alternatives &lt;em&gt;policies&lt;&#x2F;em&gt;, denoted by ($\pi$), that the algorithm compares.&lt;&#x2F;p&gt;
&lt;p&gt;The optimal policy is inferred by the best action $a$. In formal terms, this represents the &lt;strong&gt;strategy&lt;&#x2F;strong&gt; for the agent. Choosing the optimal policy that maximizes the total discounted future reward:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\pi^{*}(s)=\underset{a}{\operatorname{argmax}} Q(s, a)
$$&lt;&#x2F;p&gt;
&lt;p&gt;Therefore, the agent chooses actions $a$ that maximize the Q-function.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-2-deep-q-networks-dqn&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-2-deep-q-networks-dqn&quot; aria-label=&quot;Anchor link for: 6-2-deep-q-networks-dqn&quot;&gt;6.2 Deep Q Networks (DQN)&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Imagine the following Deep Neural Network in which the state is an Atari videogame and the agent has the actions of moving right or left to destroy the bricks (reward). The goal of the game is to destroy all the bricks using the least possible moves.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_atari.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;first solution&lt;&#x2F;strong&gt; that comes to mind is making a Neural Network that iterates over &lt;strong&gt;all the possible state-action pairs&lt;&#x2F;strong&gt; and takes the one that maximizes the expected reward returns.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_state_actions.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;However, even for this simple environment &lt;strong&gt;the amount of actions $a$ would be enormous&lt;&#x2F;strong&gt;. It would need to calculate the rewards over the time of the entire game, every time it chooses right or left. Every time, for every move $t$, calculating the optimal action. This method to find a strategy is highly inefficient and computationally very expensive.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;A more efficient way to input only the state and let the Neural Network give all the possible set of Q-functions for each action&lt;&#x2F;strong&gt;. Using this method, the Neural Network does not have to compare over Q-functions, it returns all of them for every possible action.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_dqn.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Notice that we don&#x27;t feed the state, only actions, so the possibilities do not grow exponentially.&lt;&#x2F;p&gt;
&lt;p&gt;The main issue that comes here is how to train this Neural Network. If the Network does not compare between the Q-functions, how it knows which one is better so the model can be trained?&lt;&#x2F;p&gt;
&lt;p&gt;We can introduce a benchmark that the algorithm compares with. A perfect score that the algorithm has to reach by comparing Q-functions. In other words, &lt;strong&gt;Creating a ground-truth Q-target&lt;&#x2F;strong&gt;. Therefore, the distance with this perfect Q-target is our loss function (MSE) to train the Neural Network:&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{L}=\mathbb{E}[| \overbrace{\left(r+\gamma \max_{a \prime} Q\left(s^{\prime}, a^{\prime}\right)\right.}^{\text {target }})-\overbrace{\left.Q(s, a) |^{2}\right]}^{\text {predicted }}
$$&lt;&#x2F;p&gt;
&lt;p&gt;The Network predicts state-action pairs $Q(s,a_{n})$ that closes the Q-target as much as possible. The better match with the target function, the smaller is the loss and the better is the policy.&lt;&#x2F;p&gt;
&lt;p&gt;Going back to the &lt;strong&gt;Atari example&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_atari_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For a given state $s_t$ the ball is coming towards the slider. The Neural Network creates 3 Q-functions: one for the slider going left, one for the slider staying, and one for the slider going right.&lt;&#x2F;p&gt;
&lt;p&gt;The action $a_1$ (slider to the left) has a better payoff reward, so the optimal policy $\pi$ or strategy for this state $s_t$ is going left.&lt;&#x2F;p&gt;
&lt;p&gt;Then it stats over for the state $s_{t+1}$.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-3-downsides-of-q-learning&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-3-downsides-of-q-learning&quot; aria-label=&quot;Anchor link for: 6-3-downsides-of-q-learning&quot;&gt;6.3 Downsides of Q-Learning&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Complexity&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;The models can only use scenarios where the action space is discrete and small&lt;&#x2F;strong&gt;. That is why Q-learning is so popular in video games because is a controlled environment with a limited set of actions. As an example of a continuous and infinite space, consider a driving car with the possibility to go towards any direction (infinite angles of directions).&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Flexibility&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The optimal policy is &lt;strong&gt;deterministically computed from the Q-function&lt;&#x2F;strong&gt; by maximizing the reward. It cannot compute the reward of any distribution of data outside de range of possible environment scenarios. It cannot learn stochastic policies that might get better rewards.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-4-policy-learning&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-4-policy-learning&quot; aria-label=&quot;Anchor link for: 6-4-policy-learning&quot;&gt;6.4 Policy Learning&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Until now, we were taking the state as fixed and looking for the best set of actions that maximizes the rewards. The algorithm learns using the possible actions. Instead of optimizing for actions, we can optimize for policies.&lt;&#x2F;p&gt;
&lt;p&gt;The difference is subtle but important. To illustrate we go back to the &lt;strong&gt;Atari example&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_atari_3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The Neural Network does not compare actions, it compares the probability of the actions being the correct and sample from that. In this Atari example, going left has a 90% probability of reaching the optimal policy. Therefore, the algorithm choose $a_1$ (slider left) 90% of the time, $a_2$ (center) 10% of the time and $a_3$ (right) 0% of the time. The probabilities sum to 1. This is called &lt;strong&gt;Policy gradient&lt;&#x2F;strong&gt;, as it directly optimizes the policy without calculating the rewards.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What advantages has Policy Learning over Q-Learning?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;With policy learning, the models can handle continuous spaces. We can calculate not only the left or right (discrete choice) but the speed of the slider (continuous choice). For example, the slider can go 1 meter, 2 meters, or 20 meters to the left. Each continuous set of actions has attached a probability of being the best policy.&lt;&#x2F;p&gt;
&lt;p&gt;In the following image, the highest probability action is going to the -1 meters&#x2F;second in a continuous probability space with many other rates:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L5_atari_4.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;From this probability, the optimal policy derived is going -0.8 meters&#x2F;second, taking into account the variance.&lt;&#x2F;p&gt;
&lt;p&gt;This opens the possibility of computing in environments in which the actions are continuous like autonomous vehicles.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-5-policy-learning-gradients&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#6-5-policy-learning-gradients&quot; aria-label=&quot;Anchor link for: 6-5-policy-learning-gradients&quot;&gt;6.5 Policy Learning Gradients&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The loss function is calculated by multiplying 2 terms: the probability of an action given the state, and the reward attached to that action. These elements are captured by the negative log-likelihood of the policy ($-log...$) and the total discounted returns ($R_t$)&lt;&#x2F;p&gt;
&lt;p&gt;$$
\mathcal{L}= -log P(a_t|a_s)R_t
$$&lt;&#x2F;p&gt;
&lt;p&gt;The algorithm wants to minimize this loss function as much as possible. Therefore, the larger the probability of the policy and the reward, the more negative is the loss function.&lt;&#x2F;p&gt;
&lt;p&gt;Also, it amplifies the actions with high reward but low probability, and low reward but a high probability of happening.&lt;&#x2F;p&gt;
&lt;p&gt;The gradient descent or weight updating of the neural network works like in the vanilla neural network gradient descent: computes the gradient and updates the weights according to the direction of the gradient. As we said, the gradient descent is made with respect to the policy, not the actions:&lt;&#x2F;p&gt;
&lt;p&gt;$$
w&#x27; = w - \nabla\mathcal{L}
$$&lt;&#x2F;p&gt;
&lt;p&gt;$$
w&#x27; = w + \nabla \underbrace{\log P(a_t|a_s)R_t}_\text{Policy gradient}
$$&lt;&#x2F;p&gt;
&lt;p&gt;Notice that by using the &lt;strong&gt;negative&lt;&#x2F;strong&gt; log-likelihood instead of positive, it turns the weight updating to positive. Hence, the network weighs more the set of actions with a combination of high probability&#x2F;high reward.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;lecture-7-deep-learning-limitations-and-new-frontiers&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#lecture-7-deep-learning-limitations-and-new-frontiers&quot; aria-label=&quot;Anchor link for: lecture-7-deep-learning-limitations-and-new-frontiers&quot;&gt;Lecture 7 - Deep Learning Limitations and New Frontiers&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;The rise and hype of Deep Learning led to the general public to see Machine Learning, Deep Learning, and the whole AI field like some kind of &lt;strong&gt;alchemy&lt;&#x2F;strong&gt;. Any problem in which we have data can be solved by AI. The reality is that only very specific problems can be solved by AI, and feeding poor data in a random network architecture will produce no value at all.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;What Deep Learning is good at?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Deep Neural Networks are extremely good at finding a pattern &lt;strong&gt;in the existing data&lt;&#x2F;strong&gt;. A recent paper by Google Brain&#x2F;Deepmind researchers&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#1&quot;&gt;1&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; shows that a neural network can be 100% accurate fed &lt;strong&gt;by random label images&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_understanding.png&quot; alt=&quot;&quot; &#x2F;&gt;
&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_understanding_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A model is as good as the data you feed it. If you have trained a model with a banana image and a tree image labeled as &quot;dog&quot;, every time that the model sees &lt;strong&gt;those exact images&lt;&#x2F;strong&gt; it will classify them as the label you have used. The problem comes when it sees other bananas or tree images. The accuracy could be 100% in the training set and close-to-random accuracy in the test set:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_understanding_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Random labeling led to random accuracy in the test data. The model overfitted the specific images to the specific label and has &lt;strong&gt;no generalization power&lt;&#x2F;strong&gt; to predict new unseen data. &lt;strong&gt;Without generalization, any neural network is worthless.&lt;&#x2F;strong&gt; A Neural Network can approximate any seen distribution, but how do we know what and how it is going to predict in unseen data?&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_approximation.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-1-limitations-uncertainty&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-1-limitations-uncertainty&quot; aria-label=&quot;Anchor link for: 7-1-limitations-uncertainty&quot;&gt;7.1 Limitations: Uncertainty&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Part of the new frontiers in AI tries to solve &lt;strong&gt;the problem of uncertainty&lt;&#x2F;strong&gt;. Or how to make models that infer the right choice when it faces data that it has not being trained to interact with. For example, this is especially important in the field of autonomous vehicles, in which a change in the construction of a road can lead to terrible results:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_uncertainty.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Autonomous cars should be able to identify this &lt;strong&gt;uncertainty&lt;&#x2F;strong&gt; or unseen state of the road and not crash the car, even if the car is being trained to go in that direction.&lt;&#x2F;p&gt;
&lt;p&gt;Let&#x27;s take the classical toy model of &lt;strong&gt;classifying an image of a dog vs cat&lt;&#x2F;strong&gt;. The model takes only inputs of cat and dog images and returns the probability of being a cat vs a dog. What happens if we ask the model to predict an image of a dog and a cat together? What happens if we ask the model to predict the probability of a cat&#x2F;dog feeding the image of a horse?&lt;&#x2F;p&gt;
&lt;p&gt;By definition, &lt;strong&gt;the model gives just the probability of dog and cat&lt;&#x2F;strong&gt;. It cannot output the probability of random data or the confidence in that prediction.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_horse.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We need an uncertainty metric to assess the noise inherent to the data (&lt;em&gt;aleatoric uncertainty&lt;&#x2F;em&gt;) and to &lt;strong&gt;assess the network&#x27;s confidence&lt;&#x2F;strong&gt; in its predictions (&lt;em&gt;epistemic uncertainty&lt;&#x2F;em&gt;).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-2-frontiers-evidential-neural-networks&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-2-frontiers-evidential-neural-networks&quot; aria-label=&quot;Anchor link for: 7-2-frontiers-evidential-neural-networks&quot;&gt;7.2 Frontiers: Evidential Neural Networks&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;New research&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#2&quot;&gt;2&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; using &lt;em&gt;adversarial attacks&lt;&#x2F;em&gt; tries to introduce perturbations into the data so the networks it is &lt;strong&gt;not only optimized by modifying the weights but also optimized by modifying the input images&lt;&#x2F;strong&gt;. Given an input, the network is trained to predict the parameters of what they called an evidential distribution.&lt;&#x2F;p&gt;
&lt;p&gt;The network can model a higher-order probability distribution over the individual likelihood parameters. Taking the cat&#x2F;dog model example, this means that a network trained with photos of cats and dogs and fed with a horse image can output
cat probability of 0 and a dog probability of 0.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_deep_regression.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Evidential regression simultaneously learns a continuous target along with aleatoric
uncertainty from the data and epistemic uncertainty from the model.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-3-frontiers-automated-machine-learning&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#7-3-frontiers-automated-machine-learning&quot; aria-label=&quot;Anchor link for: 7-3-frontiers-automated-machine-learning&quot;&gt;7.3 Frontiers: Automated Machine Learning&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Standard deep neural networks are optimized &lt;strong&gt;for a single task&lt;&#x2F;strong&gt;. It often requires expert knowledge to build an architecture for any task. What if we could build a learning algorithm or system that &lt;strong&gt;learns which model&lt;&#x2F;strong&gt; to use to solve a given problem?&lt;&#x2F;p&gt;
&lt;p&gt;Automated Machine Learning (AutoML) is a growing field of AI&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; that uses auto-detection of network architectures, so it relies less on human choice and expertise as it learns the model architectures directly on the dataset of interest.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;concept&lt;&#x2F;strong&gt; of this method is simple to understand. It is a system &lt;strong&gt;has a &lt;em&gt;controller network&lt;&#x2F;em&gt; and a &lt;em&gt;child network&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;. The controller samples an initial architecture and the child uses that architecture in a dataset. For every architecture sample, the child network gets an accuracy value that is used to update the controller.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_autoML.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The better the architecture and parameters proposed by the controller is, the better the results of the child network, and the more the controller knows is getting good architecture proposals.&lt;&#x2F;p&gt;
&lt;p&gt;This last step is key. &lt;strong&gt;How the controller learns?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The controller is a Recurrent Neural Network, with &lt;em&gt;N&lt;&#x2F;em&gt; layers corresponding to different architectures and parameters to choose from. One layer represents a combination of model&#x2F;parameters to try.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_autoML_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;The controller samples these different networks with a parametrization.&lt;&#x2F;li&gt;
&lt;li&gt;The controller feed variations to the child network.&lt;&#x2F;li&gt;
&lt;li&gt;The child produces an accuracy &lt;em&gt;R&lt;&#x2F;em&gt; that is used to train the weights of the controller.&lt;&#x2F;li&gt;
&lt;li&gt;Once it has the best parameter, the one with better accuracy in the child network, that layer is optimized and jumps to the next one.&lt;&#x2F;li&gt;
&lt;li&gt;Repeat until all the layers (parameters) converge.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;While AutoML can be seen as a shortcut, &lt;strong&gt;this system can produce state-of-the-art results&lt;&#x2F;strong&gt;&lt;sup class=&quot;footnote-reference&quot;&gt;&lt;a href=&quot;#3&quot;&gt;3&lt;&#x2F;a&gt;&lt;&#x2F;sup&gt; in image recognition, getting better results and being more efficient than human-created network architectures:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-mit-intro-deep-learning&#x2F;.&#x2F;images&#x2F;L6_autoML_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;NASNet&lt;&#x2F;em&gt; stands for &lt;em&gt;Neural Architecture Search Network&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;1&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;1&lt;&#x2F;sup&gt;
&lt;p&gt;C. Zhang et al. (2016) - Understanding deep learning requires rethinking generalization: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1611.03530&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;2&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;2&lt;&#x2F;sup&gt;
&lt;p&gt;A Amini et al. (2019) - Deep Evidential Regression: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1910.02600&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
&lt;div class=&quot;footnote-definition&quot; id=&quot;3&quot;&gt;&lt;sup class=&quot;footnote-definition-label&quot;&gt;3&lt;&#x2F;sup&gt;
&lt;p&gt;B. Zoph (2017) - Learning Transferable Architectures for Scalable Image Recognition: https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;1707.07012&lt;&#x2F;p&gt;
&lt;&#x2F;div&gt;
</content>
        
    </entry>
    <entry xml:lang="en">
        <title>Course Notes - Share Data Through the Art of Visualization</title>
        <published>2021-04-26T00:00:00+00:00</published>
        <updated>2021-04-26T00:00:00+00:00</updated>
        
        <author>
          <name>
            
              Unknown
            
          </name>
        </author>
        
        <link rel="alternate" type="text/html" href="https://pipegalera.github.io/mostly_books/course-google-viz/"/>
        <id>https://pipegalera.github.io/mostly_books/course-google-viz/</id>
        
        <content type="html" xml:base="https://pipegalera.github.io/mostly_books/course-google-viz/">&lt;p&gt;My personal notes and presentations tips of Google&#x27;s course &lt;a href=&quot;https:&#x2F;&#x2F;www.coursera.org&#x2F;learn&#x2F;visualize-data&#x2F;&quot;&gt;Share Data Through the Art of Visualization&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;banner.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;1-visualizations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-visualizations&quot; aria-label=&quot;Anchor link for: 1-visualizations&quot;&gt;1. Visualizations&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Tips for creating and designing powerful visualizations effectively.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-1-four-rule-of-thumb-for-better-visualizations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-1-four-rule-of-thumb-for-better-visualizations&quot; aria-label=&quot;Anchor link for: 1-1-four-rule-of-thumb-for-better-visualizations&quot;&gt;1.1 Four rule of thumb for better visualizations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;The Five-second rule: A data visualization should be &lt;strong&gt;clear, effective, and convincing&lt;&#x2F;strong&gt; enough to be absorbed in 5 seconds or less. A visualization must have &lt;strong&gt;enough details to understand, no all the details&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Graphs and charts should use a &lt;strong&gt;diverging color palette&lt;&#x2F;strong&gt; to show contrast between elements.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The visualization should align with &lt;strong&gt;audience expectations&lt;&#x2F;strong&gt; and &lt;strong&gt;cultural conventions&lt;&#x2F;strong&gt;. For example, if the majority of your audience associates green with a positive concept and red with a negative one, your visualization should follow this convention.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Visualizations should use as &lt;strong&gt;few labels&lt;&#x2F;strong&gt; as it takes to make sense.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;1-2-decision-tree-for-choosing-visualizations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-2-decision-tree-for-choosing-visualizations&quot; aria-label=&quot;Anchor link for: 1-2-decision-tree-for-choosing-visualizations&quot;&gt;1.2 Decision Tree for choosing visualizations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;This decision tree is useful to choose what kind of data visualizations would be better.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;C6_viz_decision_tree.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-3-designing-a-visualization-in-60-minutes&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-3-designing-a-visualization-in-60-minutes&quot; aria-label=&quot;Anchor link for: 1-3-designing-a-visualization-in-60-minutes&quot;&gt;1.3 Designing a visualization in 60 minutes&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Prep (5 min)&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Create the mental and physical space necessary for an environment of comprehensive thinking. This means allow yourself room to brainstorm how you want your data to appear while considering the amount and type of data that you have.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Talk and listen (15 min)&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Identify the object of your work by getting to the “ask behind the ask” and establishing expectations. Ask questions and really concentrate on feedback from stakeholders regarding your projects to help you hone how to lay out your data.&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;&lt;strong&gt;Sketch and design (20 min)&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Draft your approach to the problem. Define the timing and output of your work to get a clear and concise idea of what you are crafting.&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;&lt;strong&gt;Prototype and improve (20 min)&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Generate a visual solution and gauge its effectiveness at accurately communicating your data. Take your time and repeat the process until a final visual is produced. It is alright if you go through several visuals until you find the perfect fit.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;C6_viz_in60.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h3 id=&quot;1-4-introducing-visualizations-in-a-presentation-the-mccandless-method&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#1-4-introducing-visualizations-in-a-presentation-the-mccandless-method&quot; aria-label=&quot;Anchor link for: 1-4-introducing-visualizations-in-a-presentation-the-mccandless-method&quot;&gt;1.4 Introducing visualizations in a presentation: The McCandless Method&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;Use this method to introduce data visualizations during the presentations.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Introduce the graphic by name&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Answer obvious questions before they&#x27;re asked&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;One good way to make sure to follow these 2 steps is introducing the slide with the graphic with only the graphic name and the visualization. &lt;strong&gt;No text&lt;&#x2F;strong&gt;. Then, you describe what they are seeing and what the data comes from (answer obvious questions beforehand)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;C6_mccandless_1.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;State the insight of your graphic (&lt;em&gt;key takeaways&lt;&#x2F;em&gt;)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;As you discuss the main insights of the graphic, show the key takeaways. &lt;strong&gt;The overall text on the screen only populated as you began to discuss it&lt;&#x2F;strong&gt;. So the audience knows exactly what to be listening to when you talking&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;C6_mccandless_2.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;4&quot;&gt;
&lt;li&gt;Call out data to support that insight&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;For example, the arrow of &lt;em&gt;&quot;Happiness Score&quot;&lt;&#x2F;em&gt; helps directing the attention to the country with the highest score and darker blue color.&lt;&#x2F;p&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Tell your audience why it matters&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Include conclusions with evidence or recommendations. Make sure that all the information that they are been listening worth something and they got something out of the presentation.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;C6_mccandless_4.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-presentations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-presentations&quot; aria-label=&quot;Anchor link for: 2-presentations&quot;&gt;2. Presentations&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;p&gt;Tips for preparing thoughtful presentation - beyond the visuals.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-1-preparing-a-presentation-by-using-design-thinking&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-1-preparing-a-presentation-by-using-design-thinking&quot; aria-label=&quot;Anchor link for: 2-1-preparing-a-presentation-by-using-design-thinking&quot;&gt;2.1 Preparing a presentation by using Design thinking&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The design-thinking framework follows an overall flow of 1) understand, 2) explore, and 3) materialize. Within these larger buckets fall the 6 phases: empathize, define, ideate, prototype, test, and implement.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;design-thinking.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Any visualization or presentation can follow design thinking:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Empathize&lt;&#x2F;strong&gt; the emotions and needs of the target audience.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Define&lt;&#x2F;strong&gt; the audience’s needs, problems, and insights.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ideate&lt;&#x2F;strong&gt; by using your findings from the previous phases to begin to create data visualizations.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Prototype&lt;&#x2F;strong&gt; or start “putting it all together” - in this case, into a presentation or dashboard.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Test&lt;&#x2F;strong&gt; that your prototype is effective. You can show your visualizations to team members before the presentation.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Implement&lt;&#x2F;strong&gt; the feedback to improve the final visualization.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;2-2-tips-to-improve-presentations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-2-tips-to-improve-presentations&quot; aria-label=&quot;Anchor link for: 2-2-tips-to-improve-presentations&quot;&gt;2.2 Tips to improve presentations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Think the presentation from the &lt;strong&gt;audience&#x27;s point of view&lt;&#x2F;strong&gt;. The only thought that&#x27;s going though their head is &lt;em&gt;&quot;where should my focus be? where I should be looking?&quot;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Twenty five-words rule&lt;&#x2F;strong&gt;: every slide in a presentation should have &lt;strong&gt;at maximum&lt;&#x2F;strong&gt; 25 words in 5 lines. The audience must focus on what your saying and not reading the slides.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Use &lt;strong&gt;arrows and call-outs&lt;&#x2F;strong&gt; to direct the attention to the points that you want them to look at in the graphs.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;pipegalera.github.io&#x2F;mostly_books&#x2F;course-google-viz&#x2F;.&#x2F;images&#x2F;C6_mccandless_3.png&quot; alt=&quot;&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Include transition slides or keynotes&lt;&#x2F;strong&gt;. Those are slides that indicates that the previous slide presentation is over and indicating what to expect in the next slide. &lt;em&gt;&quot;Moving on, the next tackles the effect of...&quot;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Always include a conclusion with the &lt;strong&gt;most powerful discoveries&lt;&#x2F;strong&gt; of you analysis.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Link the visuals&lt;&#x2F;strong&gt;, instead of copy-pasting them. This way all the changes made in the data and visualization tool are updated and reflected in the presentation.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Keep things &lt;strong&gt;&lt;em&gt;&quot;kindergarten simple&quot;&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;: keep the concepts that you&#x27;re presenting as simple and straightforward as possible.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Make your presentation &lt;strong&gt;fun&lt;&#x2F;strong&gt;. Nobody wants to be in a room were you are talking for a full hour and the only voice they are hearing is your own. Prepare quizzes, videos or ask questions to create interaction.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Consider &lt;strong&gt;audience questions&lt;&#x2F;strong&gt;: keep your responses short and to the point.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;You can &lt;strong&gt;repeat the question&lt;&#x2F;strong&gt; to involve the whole audience and give yourself time to process the question. Also, take into account that they can always follow-up asking for more deep details so don&#x27;t elaborate on the topic.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-3-data-storytelling&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-3-data-storytelling&quot; aria-label=&quot;Anchor link for: 2-3-data-storytelling&quot;&gt;2.3 Data storytelling&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Data analysis isn&#x27;t about graphics and visualizations, it&#x27;s about telling a story.&lt;&#x2F;strong&gt; Look at data the way a detective examines a crime scene. Try to understand what happened and what evidence needs to be collected. The visualization—it can be a chart, map or single number—will come naturally once the mystery is solved. The focus is the story.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Data storytelling&lt;&#x2F;strong&gt; means communicating the insights behind a dataset with visuals and a narrative that are customized for each particular audience. Data storytelling is about communicating your insights effectively, giving your data a voice.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-4-spotlighting-technique&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-4-spotlighting-technique&quot; aria-label=&quot;Anchor link for: 2-4-spotlighting-technique&quot;&gt;2.4 Spotlighting technique&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;A tool or strategy to find the compelling arguments that you want to emphasize in your storytelling.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Write notes on a white board&lt;&#x2F;strong&gt; that contain the data analysis insights.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Identify ideas&lt;&#x2F;strong&gt; or concepts that arise repeatedly.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Search for broad, &lt;strong&gt;universal ideas&lt;&#x2F;strong&gt; and messages.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Use it once the first draft of the narrative is over, to make sure that the insights are transmitted clearly and ideas are not repeated.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-5-how-to-create-a-story&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-5-how-to-create-a-story&quot; aria-label=&quot;Anchor link for: 2-5-how-to-create-a-story&quot;&gt;2.5 How to create a story&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;Engage your audience&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Know your audience point of view. &lt;strong&gt;How your data project impact them?&lt;&#x2F;strong&gt; What is their stake in the project? What do they hope to get from the data insights I deliver?&lt;&#x2F;p&gt;
&lt;p&gt;Not every peace of information is relevant, part of the job as a data analyst is to eliminate the less important details.&lt;&#x2F;p&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;Create compelling visuals&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;A dashboard keeps information presented in a neat and tidy way. The elements on the dashboard need to be &lt;strong&gt;cohesive and balanced&lt;&#x2F;strong&gt;. Cohesive means that you use a consistent theme and you are making a good use of the available space.&lt;&#x2F;p&gt;
&lt;p&gt;Always ask yourself:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;em&gt;&quot;Does this data point or chart support the point I want people to walk away with?&quot;&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Tell a story in an interesting narrative. A compelling story needs the following attributes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Characters&lt;&#x2F;strong&gt;: why they care? Make characters resemble your audience.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Setting&lt;&#x2F;strong&gt;: what&#x27;s going on? The presentation must describe the current situation.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Plot&lt;&#x2F;strong&gt;: what creates tension or needs to be fix? Set the ground for a new solution that solves the tension. Frame it as an opportunity that the company can&#x27;t pass up.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Big reveal&lt;&#x2F;strong&gt;: show the solution to the problems the characters are facing, that makes them more competitive, improving a process, a new system...&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Aha moment&lt;&#x2F;strong&gt;: why this solution is successful for the company?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-6-about-messy-presentations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-6-about-messy-presentations&quot; aria-label=&quot;Anchor link for: 2-6-about-messy-presentations&quot;&gt;2.6 About messy presentations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;The main problem with the messy presentation is the lack of a logical flow, so &lt;strong&gt;people don’t know where to focus their attention&lt;&#x2F;strong&gt; . The audience has no sense of what they are looking at and why, end up being lost, confused, and unclear about any actions they need to take.&lt;&#x2F;p&gt;
&lt;p&gt;Typical messy presentation includes:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;No story or logical flow.&lt;&#x2F;li&gt;
&lt;li&gt;No axis and graph titles.&lt;&#x2F;li&gt;
&lt;li&gt;Too much text.&lt;&#x2F;li&gt;
&lt;li&gt;Hard to understand.&lt;&#x2F;li&gt;
&lt;li&gt;Inconsistent format (no theme).&lt;&#x2F;li&gt;
&lt;li&gt;No recommendation or conclusion at the end.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-7-anticipate-the-questions&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-7-anticipate-the-questions&quot; aria-label=&quot;Anchor link for: 2-7-anticipate-the-questions&quot;&gt;2.7 Anticipate the questions&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;p&gt;If you misunderstood your stakeholders&#x27; expectations or the project objectives, you won&#x27;t be able to correctly answer their questions. Think ahead.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Expectations&lt;&#x2F;strong&gt;: What the stakeholders are suppose to get out of the presentation?&lt;&#x2F;p&gt;
&lt;p&gt;Avoid any jargon, acronyms, past events, or other necessary background information. Don&#x27;t assume that they know what all the concepts or data sources in the presentations. &lt;strong&gt;Start with zero assumptions&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Project objective&lt;&#x2F;strong&gt;: Did you clearly address the initial goal of the project in the presentation?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-8-tips-before-important-presentations&quot;&gt;&lt;a class=&quot;zola-anchor&quot; href=&quot;#2-8-tips-before-important-presentations&quot; aria-label=&quot;Anchor link for: 2-8-tips-before-important-presentations&quot;&gt;2.8 Tips before important presentations&lt;&#x2F;a&gt;&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Assemble and prepare your questions.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Discuss your presentation with your manager, other analysts, or other friendly contacts in your organization.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ask a manager or other analysts what sort of questions were normally asked&lt;&#x2F;strong&gt; by your specific audience in the past.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Seek comments, feedback, and questions on the deck or the document of your analysis.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;At least 24 hours ahead of the presentation, try and brainstorm tricky questions or unclear parts you may come across- this helps avoid surprises.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;It never hurts to practice what you will be presenting, to account for any missing information or simply to calm your nerves.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Put supplementary visualizations and content in the appendix to help answer questions.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</content>
        
    </entry>
</feed>
