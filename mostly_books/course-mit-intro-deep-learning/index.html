<!doctype html><html class="dark light" lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://www.pipegalera.com/ name=base><title>
            
                Course Notes - MIT 6.S191, Introduction to Deep Learning
            
        </title><meta content="Course Notes - MIT 6.S191, Introduction to Deep Learning" property=og:title><meta content="My personal notes of MIT 6.S19." property=og:description><meta content="My personal notes of MIT 6.S19." name=description><link href=https://cdn.jsdelivr.net/npm/jetbrains-mono@1.0.6/css/jetbrains-mono.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/@fontsource/space-grotesk@4.5.8/index.min.css rel=stylesheet><script src=https://www.pipegalera.com/js/codeblock.js></script><script src=https://www.pipegalera.com/js/toc.js></script><script src=https://www.pipegalera.com/js/note.js></script><script>MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
              }
            };</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link href=https://www.pipegalera.com/atom.xml rel=alternate title=~/.pipe_galera type=application/atom+xml><link href=https://www.pipegalera.com/theme/light.css rel=stylesheet><link href=https://www.pipegalera.com/main.css media=screen rel=stylesheet><script src="https://www.pipegalera.com/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><div class=content><header><div class=main><a href=https://www.pipegalera.com/>~/.pipe_galera</a><div class=socials><a class=social href=https://www.linkedin.com/in/pipegalera/ rel=me> <img alt=linkedin src=https://www.pipegalera.com/icons/social/linkedin.svg> </a><a class=social href=https://github.com/pipegalera/ rel=me> <img alt=github src=https://www.pipegalera.com/icons/social/github.svg> </a></div></div><nav><a href=https://www.pipegalera.com/posts style=margin-left:.25em>/posts</a><a href=https://www.pipegalera.com/mostly_books style=margin-left:.25em>/mostly_books</a><a href=https://www.pipegalera.com/tags style=margin-left:.25em>/tags</a><button title="$SHORTCUT to open search" class=search-button id=search-button><img alt=Search class=search-icon src=https://www.pipegalera.com/icons/search.svg></button><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><div id=modal-content><h1 class=page-header id=modalTitle>Search</h1><div id=searchBar><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search... role=combobox spellcheck=false><button title="Clear search" class=clear-button id=clear-search><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></button></div><div id=results-container><div id=results-info><span id=zero_results style=display:none>No results</span><span id=one_result style=display:none>1 result</span><span id=many_results style=display:none>$NUMBER results</span></div><div id=results role=listbox></div></div></div></div></nav></header><main><article><div class=title><div class=page-header>Course Notes - MIT 6.S191, Introduction to Deep Learning<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2021-06-08</time> :: 9506 Words <span class=tags-label>:: Tags:</span><span class=tags> <a class=post-tag href=https://www.pipegalera.com/tags/courses/>courses</a> , <a class=post-tag href=https://www.pipegalera.com/tags/machine-learning/>machine learning</a> , <a class=post-tag href=https://www.pipegalera.com/tags/deep-learning/>deep learning</a> , <a class=post-tag href=https://www.pipegalera.com/tags/python/>python</a> </span></div></div><div class=toc-container><h1 class=toc-title>Table of Contents</h1><ul class=toc-list><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#lecture-1-introduction>Lecture 1 - Introduction</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-1-perceptron>1.1 Perceptron</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-2-neural-networks>1.2 Neural Networks</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-3-forward-propagation-in-matrix-notation-extra-explanation>1.3 Forward propagation in Matrix notation (extra explanation)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-4-deep-neural-networks>1.4 Deep Neural Networks</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-5-the-loss-function>1.5 The loss function</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-6-training-the-neural-network-gradient-descent-and-backpropagation>1.6 Training the Neural Network: Gradient Descent and Backpropagation</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-7-backpropagation>1.7 Backpropagation</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-8-learning-rates>1.8 Learning rates</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-9-batching-and-stochastic-gradient-descent>1.9 Batching and Stochastic gradient descent</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#1-10-regularization>1.10 Regularization</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#lecture-2-recurrent-neural-networks>Lecture 2 - Recurrent Neural Networks</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-1-the-missing-piece-the-cell-state>2.1 The missing piece, the Cell state</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-2-recurrent-neural-networks>2.2 Recurrent Neural Networks</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-3-examples-of-rnn-application>2.3 Examples of RNN application</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-4-making-neural-networks-understand-text-embedding>2.4 Making Neural Networks understand text: Embedding</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-5-model-design-criteria-or-why-rnn-are-good>2.5 Model Design Criteria, or why RNN are good</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-6-rnn-illustrated-example-from-michael-phi>2.6 RNN Illustrated example (from Michael Phi)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-7-backpropagation-through-time-bptt>2.7 Backpropagation Through Time (BPTT)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-8-problems-with-backpropagation-in-rnn>2.8 Problems with backpropagation in RNN</a></li><ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#exploiting-gradients-gradients-1-0>Exploiting gradients (gradients > 1.0)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#avoid-exploiting-gradients-gradient-thresholds>Avoid exploiting gradients: Gradient thresholds</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#vanishing-gradients-gradients-1>Vanishing gradients (gradients < 1)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#avoid-vanishing-gradients>Avoid vanishing gradients</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-9-long-short-term-memory-networks-lstm>2.9 Long Short-Term Memory networks (LSTM)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#2-10-gated-recurrent-unit-networks-gru>2.10 Gated Recurrent Unit networks (GRU)</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#lecture-3-convolutional-neural-network>Lecture 3 - Convolutional Neural Network</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-1-computer-vision-introduction>3.1 Computer Vision Introduction</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-2-learning-visual-features>3.2 Learning Visual features</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-3-patching>3.3 Patching</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-4-local-feature-extraction>3.4 Local feature Extraction</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-5-convolutional-neural-netowrk-operations>3.5 Convolutional Neural Netowrk operations</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-6-convolutional-neural-netowrka-for-image-classification>3.6 Convolutional Neural Netowrka for Image Classification</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#3-7-code-example>3.7 Code example</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#lecture-4-deep-generative-modeling>Lecture 4 - Deep Generative Modeling</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-1-use-examples>4.1 Use examples</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-2-autoencoding>4.2 Autoencoding</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-3-variational-autoencoders-vaes>4.3 Variational Autoencoders (VAEs)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-4-vae-operations>4.4 VAE Operations</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-5-the-vae-regularization>4.5 The VAE regularization</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-6-priors-on-the-latent-distribution>4.6 Priors on the latent distribution</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-7-why-vae-regularization-is-important>4.7 Why VAE regularization is important?</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-8-vae-backpropagation-re-parametrization>4.8 VAE Backpropagation: Re-parametrization</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-9-generative-adversarial-networks-gans>4.9 Generative Adversarial Networks (GANs)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#4-10-gans-loss>4.10 GANs Loss</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#lecture-6-reinforced-learning>Lecture 6 - Reinforced Learning</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#6-1-the-q-function>6.1 The Q-function</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#6-2-deep-q-networks-dqn>6.2 Deep Q Networks (DQN)</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#6-3-downsides-of-q-learning>6.3 Downsides of Q-Learning</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#6-4-policy-learning>6.4 Policy Learning</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#6-5-policy-learning-gradients>6.5 Policy Learning Gradients</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#lecture-7-deep-learning-limitations-and-new-frontiers>Lecture 7 - Deep Learning Limitations and New Frontiers</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#7-1-limitations-uncertainty>7.1 Limitations: Uncertainty</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#7-2-frontiers-evidential-neural-networks>7.2 Frontiers: Evidential Neural Networks</a><li><a href=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/#7-3-frontiers-automated-machine-learning>7.3 Frontiers: Automated Machine Learning</a></ul></ul></div><section class=body><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/intro_banner.png><p>My personal notes of the MIT course <a href=http://introtodeeplearning.com/>MIT 6.S191, Introduction to Deep Learning</a><p>NOTE: Diagrams only look good in <code>light</code> mode.<h2 id=lecture-1-introduction><a aria-label="Anchor link for: lecture-1-introduction" class=zola-anchor href=#lecture-1-introduction>Lecture 1 - Introduction</a></h2><h3 id=1-1-perceptron><a aria-label="Anchor link for: 1-1-perceptron" class=zola-anchor href=#1-1-perceptron>1.1 Perceptron</a></h3><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_perceptron.png><p>If we denote $\widehat{y}$ as the output:<p>$$ \begin{array}{c} \widehat{y}=g\left(w_{0}+\sum_{i=1}^{m} x_{i} w_{i}\right) \end{array} $$<p>Being $g$ , for example, a Sigmoid, Tangent or ReLU function:<p>$$ g(z)=\frac{1}{1+e^{-z}} \quad , \quad g(z)=\frac{e^{z}-e^{-z}}{e^{z}+e^{-z}} \quad , \quad g(z)=\max (0, z) $$<p>The purpose of activation functions is to introduce non-linearity into the network:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_linear.png><p>Linear activation functions produce linear decisions no matter the network size while non-linearities allow approximating arbitrarily complex functions.<h3 id=1-2-neural-networks><a aria-label="Anchor link for: 1-2-neural-networks" class=zola-anchor href=#1-2-neural-networks>1.2 Neural Networks</a></h3><p>Taking the previous perceptron and simplifying the output to be $z$:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_image2.png><p>We can try with different weights, that would produce different outputs $z_1$ and $z_2$:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_image3.png><p>Neural Network is made stacking those different outputs. Notice that this is just a stack of dot products of the same features and different weights ($W^{(1)}$).<p>These outputs in the hidden layer have a different range of values, but there are only 2 possible final outputs: $\widehat{y_1}$ and $\widehat{y_2}$.<p><strong>How we classify a label as $\widehat{y_1}$ or $\widehat{y_2}$.?</strong><p>In this step the non-linear or transformation function g$ trigger the outcomes to being one or the other.<ul><li><p>If the outcome value is more than the function threshold, the outcome is transformed to 1 (the label of $\widehat{y_1}$).</p><li><p>If the value is less than the threshold, the outcome is transformed to 0 (the label of $\widehat{y_2}$).</p></ul><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_network.png><p>Neural Network application in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#cb4b16>import </span><span>tensorflow </span><span style=color:#cb4b16>as </span><span>tf
</span><span>
</span><span>model </span><span style=color:#657b83>= </span><span>tf.keras.</span><span style=color:#b58900>Sequential</span><span style=color:#657b83>([
</span><span>        </span><span style=color:#586e75># Hidden layers with n neurons
</span><span>        tf.keras.layers.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span>n</span><span style=color:#657b83>)</span><span>,
</span><span>        </span><span style=color:#586e75># Output layer with 2 neurons
</span><span>        tf.keras.layers.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#6c71c4>2</span><span style=color:#657b83>)
</span><span style=color:#657b83>])
</span></code></pre><p><em>Dense</em> means that the layers are fully connected, all the neuron's weight counts in the dot product calculation.<h3 id=1-3-forward-propagation-in-matrix-notation-extra-explanation><a aria-label="Anchor link for: 1-3-forward-propagation-in-matrix-notation-extra-explanation" class=zola-anchor href=#1-3-forward-propagation-in-matrix-notation-extra-explanation>1.3 Forward propagation in Matrix notation (extra explanation)</a></h3><p>For example, let's say that we have 3 observations, we know 2 features of them, and we want to construct a Neural Network with 1 hidden layer containing 3 neurons.<ul><li>In a first step (1), we calculate manually the dot product of $X$ and $W^{(1)}$:</ul><p>$$Z = XW^{1}$$<p><strong>The shape of $Z$ is always a product of: <em>(observations, features) x (features, n neurons in the layer)</em></strong>.<p>The columns of the first element have to be equal to the rows of the second element. It is necessary for matrix calculation.<ul><li>The second step (2), we take the outputs of the hidden layer, apply the non-linear transformation, and calculate the dot product with respect to the second layer of weights:</ul><p>$\widehat{y} = g(Z)W^{2}$<p>Here is an example of how to calculate $\widehat{y}$ using the dot product for a made-up dataset:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_matrix.jpg><p>The final output is 3 predictions (<em>real numbers</em>) for the 3 observations. Imagine that all the notations denoted with $w$ are constants chosen randomly. Then, every matrix product is also constants as the only variable that is an incognita are these weights.<p>Weight updating is made by the network by backward propagation (later explained).<h3 id=1-4-deep-neural-networks><a aria-label="Anchor link for: 1-4-deep-neural-networks" class=zola-anchor href=#1-4-deep-neural-networks>1.4 Deep Neural Networks</a></h3><p>To make a Neural Network deep, we just add more layers. The number of layers and the number of neurons of each layer has to be defined beforehand (parameters to optimize) by us, humans. The model is only tunning the weights.<p>Neural Network application in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#cb4b16>import </span><span>tensorflow </span><span style=color:#cb4b16>as </span><span>tf
</span><span>
</span><span>model </span><span style=color:#657b83>= </span><span>tf.keras.</span><span style=color:#b58900>Sequential</span><span style=color:#657b83>([
</span><span>        </span><span style=color:#586e75># Hidden layers with n neurons
</span><span>        tf.keras.layers.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span>n</span><span style=color:#657b83>)</span><span>,
</span><span>        </span><span style=color:#586e75># Hidden layers with n neurons
</span><span>        tf.keras.layers.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span>n</span><span style=color:#657b83>)</span><span>,
</span><span>        </span><span style=color:#586e75># Output layer with 2 neurons
</span><span>        tf.keras.layers.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#6c71c4>2</span><span style=color:#657b83>)
</span><span style=color:#657b83>])
</span></code></pre><h3 id=1-5-the-loss-function><a aria-label="Anchor link for: 1-5-the-loss-function" class=zola-anchor href=#1-5-the-loss-function>1.5 The loss function</a></h3><p>Initiating random values of $W$, will give a prediction. A terrible one, as the model has no idea yet if the prediction is good, or how to measure how good is it.<p><strong>The measure of how good is a prediction is will be determined by the <em>Loss function</em></strong>.<p>The "Loss function" measures how bad is the prediction. The final output predictions compares the predicted values with the actual ones:<p>$$ \mathcal{L}\left(f\left(x^{(i)} ; \boldsymbol{W}\right), y^{(i)}\right) $$<p>The more the difference, the worse the prediction as predicted values are far away from the real ones. We want to minimize the loss function.<p>On average, for all the $n$ observations:<p>$$ \boldsymbol{J}(\boldsymbol{W})=\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(f\left(x^{(i)} ; \boldsymbol{W}\right), y^{(i)}\right) $$<h3 id=1-6-training-the-neural-network-gradient-descent-and-backpropagation><a aria-label="Anchor link for: 1-6-training-the-neural-network-gradient-descent-and-backpropagation" class=zola-anchor href=#1-6-training-the-neural-network-gradient-descent-and-backpropagation>1.6 Training the Neural Network: Gradient Descent and Backpropagation</a></h3><p>The final goal of every Neural Network is find the weights that achieve the lowest loss:<p>$$ \boldsymbol{W}^{*}=\underset{\boldsymbol{W}}{\operatorname{argmin}} \frac{1}{n} \sum_{i=1}^{n} \mathcal{L}\left(f\left(x^{(i)} ; \boldsymbol{W}\right), y^{(i)}\right) $$<p>$$ \boldsymbol{W}^{*}=\underset{\boldsymbol{W}}{\operatorname{argmin}} J(\boldsymbol{W}) $$<p><strong>How the Neural Network finds the optimal ${W}^{*}$?</strong><p>By gradient descent. Gradient descent algorithm:<ol><li>Initialize wrights randomly.<li>Compute the gradient.<li>Update the weights according to the direction of the gradient and the learning rate.<li>Loop until convergence 2 and 3.<li>Return optimal weights.</ol><h3 id=1-7-backpropagation><a aria-label="Anchor link for: 1-7-backpropagation" class=zola-anchor href=#1-7-backpropagation>1.7 Backpropagation</a></h3><p>In the second step, the algorithm computes the gradient by a process called backpropagation. <strong>Backpropagation is just the efficient application of the chain rule</strong> for finding the derivative of the loss function with respect to the neuron weights.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_backpropagation.png><p>When training a neural net, the goal is to find neuron parameters (weights) that cause the output of the NN to best fit the data, right? The chain rule is the way the NN can "connect" the loss function and outputs with the weight parametrization.<ul><li><p>If the loss function is less than the previous value using the current weights, then the gradient is in a good direction.</p><li><p>If the loss function is more than the previous, it goes in the opposite direction.</p><li><p>Repeat until the loss function is zero or cannot make it lower (<em>convergence</em>).</p></ul><p>When the Neural Network converged, it found a spot in the loss function that increasing or decreasing the weight values makes the loss function increasing.<p>Note that it might be the case that the optimal weights are not optimal for the entire loss function space because they converged in a local minimum. In practice, finding the global minimum is very difficult as the algorithm is very prompt to get stuck in these local minimums along the way of convergence.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_gradient_landscape.png><h3 id=1-8-learning-rates><a aria-label="Anchor link for: 1-8-learning-rates" class=zola-anchor href=#1-8-learning-rates>1.8 Learning rates</a></h3><p><strong>The learning rate is how much increase the weight in the updating step of the gradient descent.</strong>. If the gradient calculates the direction of the algorithm to find the minimum, the learning rate sets the magnitude of every weight try.<p>Setting a stable learning rate is key to find the global minimums. It should be large enough that avoid local minimums, but small enough that is not being able to convergence (<strong>Exploding Gradient Problem or Divergence</strong>). Stable learning rates converge smoothly and avoid local minima.<p>In practice, a usual approach is trying a lot of different learning rates and see what works. A better one is to design an adaptative learning rate that "adapts" to the loss function or landscape. In this second approach, the learning rate is no longer a constant or fixed number but a rate that gets smaller or bigger depending on how large the gradient is, how fast the learning is happening, the size of the particular weights, and so forth.<p>In Tensorflow, these are called optimizers. They are many learning rate optimizers that make the NN coverage more quickly and generally better such as Adaptive Gradient Algorithm (Adam) or Adadelta.<p>Optimizers application in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span>tf.keras.optimizers.Adam
</span><span>tf.keras.optimizers.Adadelta
</span></code></pre><h3 id=1-9-batching-and-stochastic-gradient-descent><a aria-label="Anchor link for: 1-9-batching-and-stochastic-gradient-descent" class=zola-anchor href=#1-9-batching-and-stochastic-gradient-descent>1.9 Batching and Stochastic gradient descent</a></h3><p>When we talked about backpropagation and computing the gradient, I did not mention how computationally expensive this can be. In practice, calculating the chain rule for hundreds of layers using the entire training set every time the algorithm loops is not feasible.<p><strong>Instead of looping through the entire training set, we can pick a random sub-sample of the data. This process is also called <em>Batching</em></strong> as it divides the training sets into small batches of data that feed the NN. The gradient computation is passed only through a small batch of data $B$:<p>$$ \frac{\partial J(W)}{\partial W}=\frac{1}{B} \sum_{k=1}^{B} \frac{\partial J_{k}(W)}{\partial W} $$<p>Then the weights are updated accordingly and the process starts again with another sub-sample or batch.<p><strong>This process is called <em>Stochastic gradient descent</em>, as it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data).</strong><h3 id=1-10-regularization><a aria-label="Anchor link for: 1-10-regularization" class=zola-anchor href=#1-10-regularization>1.10 Regularization</a></h3><p>A technique that <strong>constrains the optimization problem</strong> to discourage complex models to avoid overfitting.<ol><li>Dropout</ol><p><strong>For every iteration, the Neural Network drops a percentage of the neurons.</strong><p>Using Dropout the Neural Network doesn't rely on a pathway or very heavy weighting on certain features and overfitting, making the Neural Network more prompt to generalize to new data.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_dropout.png><p>Dropout regularization in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span>tf.keras.layers.</span><span style=color:#b58900>Dropout</span><span style=color:#657b83>(</span><span style=color:#268bd2>p</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.5</span><span style=color:#657b83>)
</span></code></pre><ol start=2><li>Early stopping</ol><p>First, we monitor the process of minimizing the loss function of training and testing data at the same time.<p>When the loss function starts increasing in the test data (more difference between predicted and real outputs), stop the Neural Network.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L1_early_stopping.png><h2 id=lecture-2-recurrent-neural-networks><a aria-label="Anchor link for: lecture-2-recurrent-neural-networks" class=zola-anchor href=#lecture-2-recurrent-neural-networks>Lecture 2 - Recurrent Neural Networks</a></h2><p>From a single perceptron, we can extend the number of inputs, neurons and yield multi-dimensional outputs:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_multi_output.png><p>But this multi perceptron, or Neural Network, doesn't have a sense of time or element sequence. Every input and output is a specific time step.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_sequence.png><p>This lack of connection between time steps is problematic in predicting problems that involves time or sequences. In a sequence, the inputs are correlated with each other. They are not independent. For example, future sales in a given shop are correlated with previous sales, they are not independent events.<p>Expressing it in the above graph, the output of $\hat{y}_2$ not only depends on $X_2$, but also on $X_0$ and $X_1$.<h3 id=2-1-the-missing-piece-the-cell-state><a aria-label="Anchor link for: 2-1-the-missing-piece-the-cell-state" class=zola-anchor href=#2-1-the-missing-piece-the-cell-state>2.1 The missing piece, the Cell state</a></h3><p>To make use of the correlation of the inputs in sequence, the network would need to have a connection that allows to look forward. This connection is called internal memory or <strong>cell state</strong> $h_t$:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_multi_output.png><p><strong>The memory or cell state pass the current information in the step $t$ to the next step $t+1$</strong>.<h3 id=2-2-recurrent-neural-networks><a aria-label="Anchor link for: 2-2-recurrent-neural-networks" class=zola-anchor href=#2-2-recurrent-neural-networks>2.2 Recurrent Neural Networks</a></h3><p>Recurrent Neural Networks are the result of incorporating the idea of using cell states to pass throw information between time steps. <strong>They can be thought of as multiple copies of the same network, each passing the new cell state value to a successor network</strong>. Every network is a time step of the <em>global</em> neural network.<p>RNNs have a state $h_t$, that is updated at each time step as a sequence is processed. The recurrent relation applied at each and every time step is defined as:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_rec_rel.png><p>The function is going to be parametrized by a set of weights that is leaned throughout training the model. <strong>The same function and the very same parameters are applied every step of processing the sequence (every iteration of the model)</strong>.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_rnn_ac.png><ul><li><p>$W_{xh}$ denotes the weight matrix optimized for that specific step of the sequence.</p><li><p>$W_{hh}$ denotes the weight matrix of the memory cell, reused every step for the entire sequence.</p><li><p>$W_{hy}$ denotes the weight matrix of a combination of both the specific optimization of the weights for that step, and the memory cell matrix.</p></ul><p>In practice, you won't see the cell states weighting the outputs of the next step outputs, or multiple networks one after the other. The loop is made inside one single architecture. The RNN algorithm can be simplified as:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_rnn_eq.png><h3 id=2-3-examples-of-rnn-application><a aria-label="Anchor link for: 2-3-examples-of-rnn-application" class=zola-anchor href=#2-3-examples-of-rnn-application>2.3 Examples of RNN application</a></h3><p>Recurrent Neural Networks are usually used in text problems such as sentiment classification, text generation from an image, generation of image title or translation.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_cat_words.png><p>This is an example using <strong>many</strong> words <strong>to predict the one</strong> next word in the sentence. Depending on the problem, the number of inputs and outputs change, that modify the NN architecture:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_examples_rnn.png><h3 id=2-4-making-neural-networks-understand-text-embedding><a aria-label="Anchor link for: 2-4-making-neural-networks-understand-text-embedding" class=zola-anchor href=#2-4-making-neural-networks-understand-text-embedding>2.4 Making Neural Networks understand text: Embedding</a></h3><p>Neural Networks do not understand word language, or images, they only understand numbers. They require the words to be parsed as vectors or arrays of numbers:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_words.png><p><strong>How are this vectors made?</strong><ol><li><p>The computer/algorithm gets all the words and create a <strong>vocabulary</strong> with them.</p><li><p>Then, it creates its own dictionary to understand them, assigning a number to each different word (<strong>indexing</strong>).</p><li><p>The numbers form vectors of a fixed size that captures the content of the word (<strong>embedding</strong>).</p></ol><p>By using vectors and not single numbers, you can compare how close are vectors to each other. And comparing distance is key because the words that usually go together in a phase must be represented by vectors close to each other. For example, the vector of <em>dog</em> is closer to the vector of <em>cat</em> than to the vector of <em>sad</em>.<p><strong>Embedding gather words together by similarity using the distance between vectors.</strong><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_embedding.png><h3 id=2-5-model-design-criteria-or-why-rnn-are-good><a aria-label="Anchor link for: 2-5-model-design-criteria-or-why-rnn-are-good" class=zola-anchor href=#2-5-model-design-criteria-or-why-rnn-are-good>2.5 Model Design Criteria, or why RNN are good</a></h3><p>Any recurrent model architecture must the following design criteria:<ol><li>Must handle variable-length sequences (RNN ✔️)</ol><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_length.png><ol start=2><li>Must track long-term dependencies (RNN ✔️)</ol><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_long_dep.png><ol start=3><li>Must maintain information about order (RNN ✔️)</ol><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_order.png><ol start=4><li>Must share parameters across the sequence (RNN ✔️)</ol><p>In RNNs the same memory cell is reused every step for the entire sequence, as explained previusly.<h3 id=2-6-rnn-illustrated-example-from-michael-phi><a aria-label="Anchor link for: 2-6-rnn-illustrated-example-from-michael-phi" class=zola-anchor href=#2-6-rnn-illustrated-example-from-michael-phi>2.6 RNN Illustrated example (from Michael Phi)</a></h3><p>Let's say that we want to do a many-to-one prediction in which the inputs are words in this cereal review and the output is a positive or negative sentiment analysis.<p><img alt src=https://miro.medium.com/max/1400/1*YHjfAgozQaghcsEvsBEu2g.png><p>First the words are transformed to vectors by embedding.<p>From:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_LSTM_1.png><p>To:<p><img alt src=https://miro.medium.com/max/1400/1*AQ52bwW55GsJt6HTxPDuMA.gif><p>While processing, it passes the previous hidden state to the next step of the sequence. The hidden state acts as the neural networks memory. It holds information on previous data the network has seen before.<p><img alt src=https://miro.medium.com/max/1400/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif><p>For every of theses steps or layers, the input and previous hidden state are combined to form a vector. It goes through a tanh activation, and the output is the new hidden state $h_t$. The tanh function ensures that the values stay between -1 and 1.<p><img alt src=https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif><h3 id=2-7-backpropagation-through-time-bptt><a aria-label="Anchor link for: 2-7-backpropagation-through-time-bptt" class=zola-anchor href=#2-7-backpropagation-through-time-bptt>2.7 Backpropagation Through Time (BPTT)</a></h3><p>The usual NN backpropagation algorithm:<ol><li>Take the derivative (gradient) of the loss with respect to each parameter $W$.<li>Shift parameters to minimize loss.</ol><p>With a basic Neural Network, the backpropagation errors goes trough a single feedforward network for a single time step.<p>Recurrent Network backpropagation needs a twist, as it contains multiple steps and a memory cell. In RNNs, <strong>the errors are backpropagating from the overall loss through each time step</strong>:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_BPTT.png><p>The key difference is that the gradients for $W$ at each time step are summed. A traditional NN doesn't share parameters across layers. Every input is different and have different weights $W$.<h3 id=2-8-problems-with-backpropagation-in-rnn><a aria-label="Anchor link for: 2-8-problems-with-backpropagation-in-rnn" class=zola-anchor href=#2-8-problems-with-backpropagation-in-rnn>2.8 Problems with backpropagation in RNN</a></h3><p>Computing the gradient with respect to the initial $h_0$ involves many matrix multiplications between the memory cell $h_t$ and the weights $W_hh$.<h4 id=exploiting-gradients-gradients-1-0><a aria-label="Anchor link for: exploiting-gradients-gradients-1-0" class=zola-anchor href=#exploiting-gradients-gradients-1-0>Exploiting gradients (gradients > 1.0)</a></h4><p>In the the process of backpropagation the gradients get multiplied by each other over and over again. If they are larger than 1.0, the end matrix of weights is huge.<p>As a silly example: 0.5 times 1.5 is 0.75, 0.5 times 1.5^200 is 8.2645996e34. This can give you a perspective of how matrix multiplication can "exploit" by multiplying constantly by 1.X.<p>These huge gradients can become extremely large as the result of matrix and the loss function cannot be minimized.<p>The usual solution is change the derivative of the errors before they propagate through the network, so they don't become huge. Basically, you can create a threshold that the gradients cannot surpass. <em>Create a threshold</em> means that you set a value, such as 1.0, that forces the values to be 1.0 at maximum.<h4 id=avoid-exploiting-gradients-gradient-thresholds><a aria-label="Anchor link for: avoid-exploiting-gradients-gradient-thresholds" class=zola-anchor href=#avoid-exploiting-gradients-gradient-thresholds>Avoid exploiting gradients: Gradient thresholds</a></h4><p>There are two ways to create these thresholds:<p><strong>1. Gradient Norm Scaling</strong><p>Gradient norm scaling rescales the matrix so the gradient equals 1.0 if the a gradient exceeds 1.0.<p>Gradient Norm Scaling in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span>  opt </span><span style=color:#657b83>= </span><span style=color:#b58900>SGD</span><span style=color:#657b83>(</span><span style=color:#268bd2>lr</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.01</span><span>, </span><span style=color:#268bd2>clipnorm</span><span style=color:#657b83>=</span><span style=color:#6c71c4>1.0</span><span style=color:#657b83>)
</span></code></pre><p><strong>2. Gradient Value Clipping</strong><p>Gradient value clipping simply forces all the values above the threshold to be the threshold, without changing the matrix. If the clip is 0.5, all the gradient values less than -0.5 are set to -0.5 and all the gradients more than 0.5 set to 0.5.<p>Gradient Norm Scaling in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span>  opt </span><span style=color:#657b83>= </span><span style=color:#b58900>SGD</span><span style=color:#657b83>(</span><span style=color:#268bd2>lr</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.01</span><span>, </span><span style=color:#268bd2>clipvalue</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.5</span><span style=color:#657b83>)
</span></code></pre><h4 id=vanishing-gradients-gradients-1><a aria-label="Anchor link for: vanishing-gradients-gradients-1" class=zola-anchor href=#vanishing-gradients-gradients-1>Vanishing gradients (gradients < 1)</a></h4><p>As gradients can become huge they can also become tiny to the point that it is not possible to effectively train the network.<p>This is a problem because the errors further back in time are not being propagated. It would cause that the long-term errors are vanished and bias the model only to capture short-term dependencies.<h4 id=avoid-vanishing-gradients><a aria-label="Anchor link for: avoid-vanishing-gradients" class=zola-anchor href=#avoid-vanishing-gradients>Avoid vanishing gradients</a></h4><p>The basic recipe to solve vanishing gradients is use a ReLU activation function, chaning to a smart weight initialization and/or use a different RNN architecture.<p><strong>1. Change activation function to ReLU.</strong><p>Why ReLu?<p>Because when the cell or instance gets activated (weight 0 or more), by definition the derivative or gradient is 1.0 or more:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_activation_trick.png><p><strong>2. Change weight initialization.</strong><p>For example to the <strong>Xavier initialization/Glorot initialization</strong>:<p>Changing the weight activation in Tensorflow:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#cb4b16>from </span><span>keras.models </span><span style=color:#cb4b16>import </span><span>Sequential
</span><span style=color:#cb4b16>from </span><span>keras.layers </span><span style=color:#cb4b16>import </span><span>Dense, Activation
</span><span>
</span><span>model </span><span style=color:#657b83>= </span><span style=color:#b58900>Sequential</span><span style=color:#657b83>([
</span><span>    </span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#6c71c4>16</span><span>, </span><span style=color:#268bd2>input_shape</span><span style=color:#657b83>=(</span><span style=color:#6c71c4>1</span><span>,</span><span style=color:#6c71c4>5</span><span style=color:#657b83>)</span><span>, </span><span style=color:#268bd2>activation</span><span style=color:#657b83>=</span><span>'</span><span style=color:#2aa198>relu</span><span>'</span><span style=color:#657b83>)</span><span>,
</span><span>    </span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#6c71c4>32</span><span>, </span><span style=color:#268bd2>activation</span><span style=color:#657b83>=</span><span>'</span><span style=color:#2aa198>relu</span><span>', </span><span style=color:#268bd2>kernel_initializer</span><span style=color:#657b83>=</span><span>'</span><span style=color:#2aa198>glorot_uniform</span><span>'</span><span style=color:#657b83>)</span><span>,
</span><span>    </span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#6c71c4>2</span><span>, </span><span style=color:#268bd2>activation</span><span style=color:#657b83>=</span><span>'</span><span style=color:#2aa198>softmax</span><span>'</span><span style=color:#657b83>)
</span><span style=color:#657b83>])
</span></code></pre><p><strong>3. Change Network architecture.</strong><p>More complex RNNs such as <strong>LSTM or GRU</strong> can control the information that is passing through. Long Short Term Memory networks (<strong>LSTM</strong>) and Gated Recurrent Units (<strong>GRU</strong>) are special kinds of RNN, capable of learning long-term dependencies.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_activation_trick3.png><p>They can keep informed of long-term dependencies <strong>using filters or gates</strong>. In essence, these gates decide how much information to keep of the previous neuron state or values, and how much to drop. This makes the optimization problem or the Neural Network less prompt to vanishing or exploding gradient problems.<h3 id=2-9-long-short-term-memory-networks-lstm><a aria-label="Anchor link for: 2-9-long-short-term-memory-networks-lstm" class=zola-anchor href=#2-9-long-short-term-memory-networks-lstm>2.9 Long Short-Term Memory networks (LSTM)</a></h3><p>In a simple RNN, the information goes though every step with the input of that time step ($x_t$), the previous step memory cell ($h_{t-1}$) and an output for every step ($y_t$).<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_rnn_arq.png><p>The structure of a LSTM is more complex. <strong>LSTM forces the matrix inputs in every step to go through gates</strong>, or internal mechanism to keep long-term information.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L2_lstm_arq.png><p><strong>LSTM Gates system</strong><p>They 4 types of gates interacting within each step layer:<ol><li><strong><em>Forget gate</em></strong>: Remove the irrelevant information.</ol><p>Information from the previous hidden state and the current input is passed through the sigmoid function. Values come out between 0 and 1.<p>The closer to 0 means to forget, and the closer to 1 means to keep.<p><img alt src=https://miro.medium.com/max/1400/1*GjehOa513_BgpDDP6Vkw2Q.gif><ol start=2><li><strong><em>Store gate</em></strong>: Store relevant information.</ol><p>The same previous $h_{t-1}$ and the current inputs goes into two transformations:<ul><li><p>Sigmoid transformation. It is the same operation as before, but in another gate. Instead of forget and keep, it will decide the information to update or not update.</p><li><p>Than transformation. It helps to regulate the network by squishing values between -1.0 and 1.0.</p></ul><p>The matrix multiplication of the tanh outputs with the sigmoid outputs decides which information is important, and store it in a cell state $\bigotimes$.<p><img alt src=https://miro.medium.com/max/1400/1*TTmYy7Sy8uUXxUXfzmoKbA.gif><ol start=3><li><strong><em>Update gate</em></strong>: update the separated cell state.</ol><ul><li><p>The update gate takes the previous cell state vector $c_{t-1}$ and multiply by the forget vector (from the forget gate), that allows to drop non-important information.</p><li><p>Then, it adds the store vector from the store gate, as this information is important to keep from the current step.</p></ul><p><img alt src=https://miro.medium.com/max/1400/1*S0rXIeO_VoUVOyrYHckUWg.gif><p>The update gate takes the information to the other 2 gates to decide what to forget and what to keep, updating the cell state.<ol start=4><li><strong><em>Output gate</em></strong>: decides what the next hidden state $h_{t+1}$.</ol><ul><li>The previous hidden state and the current input into a sigmoid function.<li>Then the newly modified cell state pass the tanh function.<li>By multiplying the two vectors it decides what information the hidden state should carry.</ul><p><img alt src=https://miro.medium.com/max/1400/1*VOXRGhOShoWWks6ouoDN3Q.gif><h3 id=2-10-gated-recurrent-unit-networks-gru><a aria-label="Anchor link for: 2-10-gated-recurrent-unit-networks-gru" class=zola-anchor href=#2-10-gated-recurrent-unit-networks-gru>2.10 Gated Recurrent Unit networks (GRU)</a></h3><p>GRU’s has fewer tensor operations; therefore, they are a little speedier to train then LSTM’s. There isn’t a clear winner which one is better, try both to determine which one works better for their use case.<p><img alt src=https://miro.medium.com/max/1400/1*jhi5uOm9PvZfmxvfaCektw.png><h2 id=lecture-3-convolutional-neural-network><a aria-label="Anchor link for: lecture-3-convolutional-neural-network" class=zola-anchor href=#lecture-3-convolutional-neural-network>Lecture 3 - Convolutional Neural Network</a></h2><h3 id=3-1-computer-vision-introduction><a aria-label="Anchor link for: 3-1-computer-vision-introduction" class=zola-anchor href=#3-1-computer-vision-introduction>3.1 Computer Vision Introduction</a></h3><p>We can train computers to understand the world of images, mapping where things are, what actions are taking place, and making them to predict and anticipate events in the world. For example, in this image, the computer can pick up that people are crossing the street, so the black car must be not moving.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_cars.png><p><strong>What computers <em>see</em> ?</strong><p>Task that for us are trivial, for a computer is not. To a computer, the images are 2-dimensional arrays of numbers.<p>Taking the following image, we are able to see that is a Lincoln portrait but the computer sees a 1080x1080x3 vector of numbers.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_lincoln.png><p>The classification of an image by a computer is made by picking up clues, or features, from the image. If the particular features of the image are more present in Lincoln images, it will be classified as Lincoln.<p>The algorithm, to perform this task well, should be able to differentiate between unique features and modifications of the same features. For example, it should classify as "Dog" a photo of dogs taken from different angles or a dog hidden in a tree.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_transformation_images.png><p>The computer must be invariant of all those variations, as humans recognize the same image changing its viewpoint or scale.<h3 id=3-2-learning-visual-features><a aria-label="Anchor link for: 3-2-learning-visual-features" class=zola-anchor href=#3-2-learning-visual-features>3.2 Learning Visual features</a></h3><p><strong>Computers learn hierarchically from the features</strong> in an image. For example, in face recognition the algorithm learn in order:<ol><li>Facial structure.<li>Eyes, ears, nose.<li>Edges, dark spots<li>...</ol><p>A fully connected neural network can take as input an image in the shape of a 2D number array, and classify it. What would be the problem of using a Multilayer Perceptron to classify images?<p>It's not able to capture is no <strong>spatial information</strong>.<p>If each feature of the image is an individual characteristic, all the connections between the image characteristics are lost. For example, a MLP architecture is not able to pick that the inner array of pixels the ears must be close to the outer array of pixels of the facial structure.<p>How can we use spatial structure in the input to inform the architecture of the network?<h3 id=3-3-patching><a aria-label="Anchor link for: 3-3-patching" class=zola-anchor href=#3-3-patching>3.3 Patching</a></h3><p>Spatial 2D pixel arrays are correlated to each other. By using a spatial structure, it would preserve the correlation of the pixels and its spatial architecture.<p>We can think about a neural network architecture that takes different parts of the images in different layers and connects somehow the images. How would looks like?<p>In a neural network with spatial structure each neuron takes a small pixel of the entire image and try to extract it's feature information. Only a small region of the image, a <strong>patch</strong>, affects a concrete neuron and not the entire image.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_patches.png><p><strong>The next neuron afterwards takes a shifted patch of pixels. The process is repeated for all the neurons until the entire image is taken as input by patches</strong>.<p>As you can see in the image below , some of the patched pixels took from the first neuron in the left overlap some of the pixels patched in the right neuron.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_patches_connected.png><p>The overlapping of pixels preserves the spatial component of the image. Every patch is intended to reveal features characteristic of the image.<p>But...how the algorithm learn the features? How it knows to detect the ears or eyes in a patch? The process is called <em>local feature extraction</em>.<h3 id=3-4-local-feature-extraction><a aria-label="Anchor link for: 3-4-local-feature-extraction" class=zola-anchor href=#3-4-local-feature-extraction>3.4 Local feature Extraction</a></h3><p>The neural network identify the features patches by weighting the pixels.<p>Take the following image. The idea is that the neural network have to classify the right image as an X or not a X.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_xisx.png><p>While for us humans is simple to see that is an X, the pixel arrays do not match. After all, computers cannot see images, only arrays of numbers that do not match.<p>By the process of patching, the neural network takes images with different pixel position that share same features:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_xisxfeatures.png><p>Multiple patches in the X images are similar, or equal.<p><strong>How the model calculates this similarity?</strong><p>By <strong>the convolutional operation</strong>. While the name seems scary, it is just multiplying each pixel value element-wise between the filter matrix (<em>real X patch</em>) and the patch of the input image, and adding the outputs together.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_convolutional_operation.png><p>In other words, comparing the pixels between the <em>"proper X patch"</em> and the input patch that "<em>might or might not be an X patch</em>", in an a numerical way.<p>By going through local patches, the algorithm can identify and extract local features for each patch:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_convolutional_operation_gif.gif><p>The end matrix from the convolutional operation is called <strong>feature map</strong>, as it mapped the features of the input image.<h3 id=3-5-convolutional-neural-netowrk-operations><a aria-label="Anchor link for: 3-5-convolutional-neural-netowrk-operations" class=zola-anchor href=#3-5-convolutional-neural-netowrk-operations>3.5 Convolutional Neural Netowrk operations</a></h3><p>CNNs are neural networks that apply the concept of patching, and are able to learn from spatial numerical arrays. <strong>The word <em>Convolutional</em> is a way too say that this neural network architecture handles cross-correlated 2D arrays of numbers.</strong><p>Three CNN core operations are:<ol><li>Convolution.<li>Apply a non-linear filter, often ReLU.<li>Pooling: a downsampling operation that allows to scale down the size of each feature map.</ol><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_CNN.png><p><strong>1. Convolution, or Convolutional Operations.</strong><p>The operation described in the above section. Each neuron takes <strong>only the input from the patch</strong>, computes the weighted sum, and applies bias that passes through a non-linear function (as usual in NN). Every neuron takes a different shifted patch.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_feature_map.gif><p>Take into account that there are not only one feature map in the neural network. <strong>A feature map is specific for a feature</strong>. As images have multiple features, multiple feature map or layers are needed.<p>Think about a human portrait. Taking only the feature <em>"oval shape of the face"</em> the algorithm could confuse a potato as a human face, as is oval as well.<p>By applying multiple filters, or layers, the CNN learns hierarchically from the features in an image.<p><strong>2. ReLU filter.</strong><p>After each convolutional operation, it needed to apply a ReLU activation function to the output volume of that layer.<p><strong>Why using a ReLU activation function?</strong><p>For any given neuron in the hidden layer, there are two possible (fuzzy) cases: either that neuron is relevant, or it isn’t. We need a function that shuts down the non-relevant neurons that do not contain a positive value.<p>ReLU replaces all the negative values with zero and keeps all the positive values with whatever the value was.<p>Think it this way: if the output of the convolutional operation is negative it means that the sample image patch doesn't look similar to the real image patch. We don't care how different it looks (how negative is the output), we only want that this neuron is not taken into account to train the model.<p>ReLU is also computationally cheap in comparison with other non-linear functions. It involves only a comparison between its input and the value 0.<p><strong>3. Pooling.</strong><p>Pooling is an operation to <strong>reduce the dimensionality</strong> of the inputs while still <strong>preserving spatial invariants</strong>. For example, a MaxPool2D takes a 4x4 patch matrix and convert it into a 2x2 patch by taking only the maximum value of each patch:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_maxpool.png><h3 id=3-6-convolutional-neural-netowrka-for-image-classification><a aria-label="Anchor link for: 3-6-convolutional-neural-netowrka-for-image-classification" class=zola-anchor href=#3-6-convolutional-neural-netowrka-for-image-classification>3.6 Convolutional Neural Netowrka for Image Classification</a></h3><p>Using CNNs for image classification can be broken down into 2 parts: learning and classification.<p><strong>1. Feature learning.</strong><p>The convolutional, ReLU and pooling matrix operations, the model to learn the features from an images. These feature maps get the important features of an image in the shape of weighted 2D arrays.<p>For example, a CNN architecture can learn from a set of images of cars and then distinguish between <em>car</em> features and <em>not car</em> features using the three key operations, but is still unable to classify images into labels.<p><strong>2. Classification part.</strong><p><strong>The second part of the CNN structure is using a second normal MPL to classify the label of the image</strong>. After capturing the features of a car by convolutional operations and pooling, the lower-dimensional feature arrays feed this neural network to perform the classification.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L3_CNN_classification_prob.png><p><strong>Why not using a second CNN structure or any other NN complex architecture?</strong><p>Because you don't need a neural network that handle sense of space or cross-corrlation for this task. It is a simple classification task. The inputs are not even an image anymore, they are features coded as number vectors. They don't need patching.<p><strong>Softmax function</strong><p>Given that the classification is into more than one category, the neural network output is filtered with a <strong>softmax non-linear function to get the results in terms of probabilities</strong>. The output of a softmax represents a categorical probability distribution. Following the car classification example, if the input image is a car it could give a 0.85 probability of being a car, 0.05 of being a van, a 0.01 of being a truck, and so forth.<h3 id=3-7-code-example><a aria-label="Anchor link for: 3-7-code-example" class=zola-anchor href=#3-7-code-example>3.7 Code example</a></h3><p>CNN "vehicle classifier" in Tensorflow:<ul><li><p><strong><em>filters</em></strong> refers to the number of feature maps. For the first layer we set 32 feature maps, for the second 64.</p><li><p><strong><em>kernel_size</em></strong> refers to the height and width of the 2D convolution window. 3 means 3x3 pixel window patching.</p><li><p><strong><em>strides</em></strong> refers to how far the pooling window moves for each pooling step. With stride 2, the neurons moves in 2x2 pixels windows.</p><li><p><strong><em>pool_size</em></strong> refers to the window size over which to take the maximum when calculating the pooling operation. With 2, it will take the max value over a 2x2 pooling window.</p><li><p><strong><em>units</em></strong> refers to the number of outputs. 10 lasting outputs representing the 10 classes of vehicles.</p></ul><pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#cb4b16>import </span><span>tensorflow </span><span style=color:#cb4b16>as </span><span>tf
</span><span>
</span><span style=color:#859900>def </span><span style=color:#b58900>vehicles_classifier_CNN</span><span style=color:#657b83>():
</span><span>  model </span><span style=color:#657b83>= </span><span>tf.keras.</span><span style=color:#b58900>Sequential</span><span style=color:#657b83>([
</span><span>
</span><span>  </span><span style=color:#586e75>########First part: Feature learning ########
</span><span>
</span><span>  </span><span style=color:#586e75>## CONVOLUTION + RELU
</span><span>  tf.keras.layer.</span><span style=color:#b58900>Conv2D</span><span style=color:#657b83>(</span><span style=color:#268bd2>filters </span><span style=color:#657b83>= </span><span style=color:#6c71c4>32</span><span>,
</span><span>                        </span><span style=color:#268bd2>kernel_size </span><span style=color:#657b83>= </span><span style=color:#6c71c4>3</span><span>,
</span><span>                        </span><span style=color:#268bd2>activation </span><span style=color:#657b83>= </span><span>'</span><span style=color:#2aa198>relu</span><span>'</span><span style=color:#657b83>)</span><span>,
</span><span>  </span><span style=color:#586e75>## POOLING
</span><span>  tf.keras.layer.</span><span style=color:#b58900>MaxPool2D</span><span style=color:#657b83>(</span><span style=color:#268bd2>pool_size </span><span style=color:#657b83>= </span><span style=color:#6c71c4>2</span><span>, </span><span style=color:#268bd2>strides </span><span style=color:#657b83>= </span><span style=color:#6c71c4>2</span><span style=color:#657b83>)</span><span>,
</span><span>  </span><span style=color:#586e75>## CONVOLUTION + RELU
</span><span>  tf.keras.layer.</span><span style=color:#b58900>Conv2D</span><span style=color:#657b83>(</span><span style=color:#268bd2>filters </span><span style=color:#657b83>= </span><span style=color:#6c71c4>64</span><span>,
</span><span>                        </span><span style=color:#268bd2>kernel_size </span><span style=color:#657b83>= </span><span style=color:#6c71c4>3</span><span>,
</span><span>                        </span><span style=color:#268bd2>activation </span><span style=color:#657b83>= </span><span>'</span><span style=color:#2aa198>relu</span><span>'</span><span style=color:#657b83>)</span><span>,
</span><span>  </span><span style=color:#586e75>## POOLING
</span><span>  tf.keras.layer.</span><span style=color:#b58900>MaxPool2D</span><span style=color:#657b83>(</span><span style=color:#268bd2>pool_size </span><span style=color:#657b83>= </span><span style=color:#6c71c4>2</span><span>, </span><span style=color:#268bd2>strides </span><span style=color:#657b83>= </span><span style=color:#6c71c4>2</span><span style=color:#657b83>)</span><span>,
</span><span>
</span><span>  </span><span style=color:#586e75>######## Second part: Classification ########
</span><span>
</span><span>  </span><span style=color:#586e75>## FLATTEN
</span><span>  tf.keras.layer.</span><span style=color:#b58900>Flatten</span><span style=color:#657b83>()</span><span>,
</span><span>  </span><span style=color:#586e75>## FULLY CONNECTED
</span><span>  tf.keras.layer.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#268bd2>units </span><span style=color:#657b83>= </span><span style=color:#6c71c4>1024</span><span>, </span><span style=color:#268bd2>activation </span><span style=color:#657b83>= </span><span>'</span><span style=color:#2aa198>relu</span><span>'</span><span style=color:#657b83>)</span><span>,
</span><span>  </span><span style=color:#586e75>## SOFTMAX
</span><span>  tf.keras.layer.</span><span style=color:#b58900>Dense</span><span style=color:#657b83>(</span><span style=color:#268bd2>units </span><span style=color:#657b83>= </span><span style=color:#6c71c4>10</span><span>, </span><span style=color:#268bd2>activation </span><span style=color:#657b83>= </span><span>'</span><span style=color:#2aa198>softmax</span><span>'</span><span style=color:#657b83>)
</span><span>  </span><span style=color:#657b83>])
</span><span>
</span><span>  </span><span style=color:#859900>return </span><span>model
</span><span>
</span></code></pre><h2 id=lecture-4-deep-generative-modeling><a aria-label="Anchor link for: lecture-4-deep-generative-modeling" class=zola-anchor href=#lecture-4-deep-generative-modeling>Lecture 4 - Deep Generative Modeling</a></h2><p>Deep Generative Modeling is part of <strong>unsupervised learning: the models receive the data but not the respective labels</strong>. The goal is to take as input the training samples from some distribution and learn a model that represents that distribution.<p>Another way to define this goal is to <strong>find ways to learn the underlying and hidden latent variables</strong> in the data even when the generative model is only given the representation of the variables.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_images_generated.png><p>Deep generative models are very useful to create synthetic samples using the probability density function of the samples provided.<h3 id=4-1-use-examples><a aria-label="Anchor link for: 4-1-use-examples" class=zola-anchor href=#4-1-use-examples>4.1 Use examples</a></h3><ul><li>Debiasing image recognition</ul><p>Let's say that you have a silly algorithm that takes facial expressions and the goal is classifying between <em>pretty</em> or <em>non pretty</em>. However, all your faces are either <em>white-blond-people smiling at the camera</em> or portraits of <em>drug addicts</em>. This algorithm won't create a boundary between pretty and not, it would define a boundary between white-blond-smiling people and drug users. Generative models can follow the facial distribution of the existing sample to create new samples of portraits with different skin tones, postures and attributes.<ul><li>Outlier detection in images</ul><p>Rare events in tail distributions, such as people crossing the street in red, accidents, or sudden impacts can be created by generative models as samples to train the model of self-driving cars. The benefit is that the car would know what to do in these extreme scenarios even if it hasn't seen it before in the sample.<h3 id=4-2-autoencoding><a aria-label="Anchor link for: 4-2-autoencoding" class=zola-anchor href=#4-2-autoencoding>4.2 Autoencoding</a></h3><p>Autoencoding means <strong>auto</strong>matically <strong>enconding</strong> data. In Generative Modeling, the <em>Encoder</em> learns to map from the data $x$ into a low-dimensional vector $z$:<p><img title="Autoencoders: background" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_autoencoder.png><p><strong>Autoencoding is a form of compression</strong>. A smaller dimensionality of the latent space means that we can compress the data into smaller latent factors that keep the feature representation.<p>However, the dimensionality of the latent space will also influence the reconstruction quality. The smaller the latent space the poorer and less quality the generated images have, as will force a larger training to bottleneck.<p>But wait, the input data has no labeled. Therefor, $z$ cannot be a <em>feature map</em> of the attributes of 2s, as this algorithm doesn't know is a 2 in the first place!<p><strong>What is this $z$ then?</strong><p><strong>$z$ is vector of latent variables</strong>. It represent the features of the image in a lower dimensional vector space, in this case the features of a 2.<p>The model uses the features created in this latent space $z$ to construct a new observations $\hat{x}$ following the features of the original $x$. It "decodes" the original images to create new images.<p><strong>How the algorithm knows that the atributes in $z$ are right?</strong><p>The model learns by comparing the difference between the new synthetic image and the original image in terms of pixels.<p><img title="Autoencoders: mapping the latent space" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_autoencoder2.png><p>Therefore, can be trained to minimize the Mean Squared Error between the sample inputs $x$ and ouput synthetic samples $\hat{x}$.<h3 id=4-3-variational-autoencoders-vaes><a aria-label="Anchor link for: 4-3-variational-autoencoders-vaes" class=zola-anchor href=#4-3-variational-autoencoders-vaes>4.3 Variational Autoencoders (VAEs)</a></h3><p>In the previous image, the latent space $z$ acts as a "normal" layer in a Neural Network. Is deterministic in the sense that it would yield the same latent variable space $z$ every time we use the same image as input.<p>In contrast, <strong>VAEs impose a variational or stochastic</strong> twist to the architecture to generate smoother and different representations of the images:<p><img title="Variational Autoencoders" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_autoencoder3.png><p>For each variable, the VAE learns a mean and a variance associated with that latent variable. Instead of using the vector latent variables $z$ straight, the model uses the vector of means and a vector of variances to define the probability distributions for each of the latent variables.<p>The goal of this twist is to generate slightly different new images from the samples, not to imitate perfectly them perfectly.<h3 id=4-4-vae-operations><a aria-label="Anchor link for: 4-4-vae-operations" class=zola-anchor href=#4-4-vae-operations>4.4 VAE Operations</a></h3><p>VAEs optimization process can be dividing into: <strong>encoding and decoding</strong>.<p><img title="Variational Autoencoders Optimization" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_autoencoder4.png><ol><li><strong>Encoding</strong>.</ol><p>The first part of the process is called <em>encoding</em>, as it encode or define the latent space $z$ given $x$ observations.<p>Learning the structure of the input images by deconstruction, comparing the differences between the distribution of features input images and new images (log-likelihood). Optimizing the $q_{\phi}$ weights.<ol start=2><li><strong>Decoding</strong>.</ol><p>The second part of the process is called <em>decoding</em>, as it decodes or extract the features of the latent space $z$ to make new observations $\hat{x}$.<h3 id=4-5-the-vae-regularization><a aria-label="Anchor link for: 4-5-the-vae-regularization" class=zola-anchor href=#4-5-the-vae-regularization>4.5 The VAE regularization</a></h3><p>The training phase will change as a result of these two different tasks. The loss function cannot be only calculated as the difference in similarity between input and output images, as they must be different by definition. This is the stochastic <em>twist</em> necessary to create new images, not just copies.<p>The optimization function must include a new term, the <strong>VAE loss:</strong><p><strong><center>VAE Loss function = (reconstruction loss) + (regularization term)</center></strong><p>As in any other neural network, the regularization term avoids overfitting. In this neural network architecture overfitting would mean replicating the same exact images of $x$ into $\hat{x}$. We don't want the same images, we want different images that follows the latent varaibles of the original sample.<p><img title="Variational Autoencoders Optimization: Loss function" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_autoencoder5.png><p>By adding this new parameter $D$ to the loss function, the Neural Network will try to reduce not only the errors extracting the latent variables (reconstruction loss) but also avoid overfitting the model so it doesn't create identical copies of the input images (regularization term).<p>Let's analyze this regularization term analytically: $D\left(q_{\phi}(\mathrm{z} \mid x) | p(z)\right)$<p>$D$ is a function of:<ul><li><p>$q_{\phi}(z \mid x)$: the encoding. Imposes to the new synthetic images $\hat{x}$ to follow a inferred latent distribution of the latent variables $z$.</p><li><p>$p(z)$: the decoding. Imposes to the new synthetic images $\hat{x}$ to follow a prior <strong>fixed prior</strong> distribution of $z$</p><li><p>Finally, the two vertical lines between the elements of the function is a reciprocal math operator. Effectively, it means that $D$ is function of the difference between the two elements, the <strong>inferrerd</strong> and the <strong>fixed prior</strong> distribution.</p></ul><p>In other words, $D$ is a parameter that represents the divergence of what the encoder is trying to infer and a prior distribution of $z$.<p>The <strong>inferred</strong> distribution of $z$ is easy to understand, as it is just the latent variables of the images created by using the mean and standard deviation of each input.<p>However...<strong>What is a fixed prior distribution? How it calculates the $p(z)$ ?</strong><h3 id=4-6-priors-on-the-latent-distribution><a aria-label="Anchor link for: 4-6-priors-on-the-latent-distribution" class=zola-anchor href=#4-6-priors-on-the-latent-distribution>4.6 Priors on the latent distribution</a></h3><p>The usual prior distribution choice is the <strong>normal Gaussian distribution</strong> (means equal 0 and standard deviations equals 1). In practical terms, this prior makes the model cut the features that are way out of a normal distribution, such as outliers or edge cases in the data.<p>The new samples generated $\hat{x}$ follows the inferred distribution but also this fixed prior. The loss funtion optimize the inferred latent distribution and also penalize extreme cases outside the normal distribution (<em>weird or non-common elements in the images</em>)<p><img title="Normal Gaussian distribution used to setting prior distribution" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_normaldis.png><p>We said that the regularization term <em>D</em> is a function of the difference between the inferred latent distribution and a Gaussian prior distribution. This difference is called <strong>KL-divergence</strong>(Kullback-Leibler) or <strong>relative entropy</strong>.<p>$$ D\left(q_{\phi}(\mathrm{z} \mid x) | p(z)\right) =-\frac{1}{2} \sum_{j=0}^{k-1}\left(\sigma_{j}+\mu_{j}^{2}-1-\log \sigma_{j}\right) $$<p>While the form of the function looks <em>unfriendly</em>, it is just a measure of how one probability distribution is different from a second reference probability distribution.<h3 id=4-7-why-vae-regularization-is-important><a aria-label="Anchor link for: 4-7-why-vae-regularization-is-important" class=zola-anchor href=#4-7-why-vae-regularization-is-important>4.7 Why VAE regularization is important?</a></h3><p>The VAE regularization creates:<ol><li><strong>Continuity</strong>. Data points that are similar in the latent space should result in similar content after the decoding.<li><strong>Completeness</strong>. New samples out of the latent space should resemble meaningful content after decoding.</ol><p>Without regularization (a loss function that just tries to minimize the encoding loss), the model could group images that are similar in real life in different clusters because of the small variations. We want input images with close latent features to have very similar distributions that the model can use to create new ones.<p><img title="Not regularized vs regularized data points in latent and output space" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_regularization.png><p>The normal prior force the latent learned distribution to overlap. For example, if we want to create faces with VAEs the fixed distribution forces images of faces to place the eyes, mouth, and ears within the same regions.<p><img title="Regularized data points in latent and output space clustered in close distributions" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_regularization2.png><h3 id=4-8-vae-backpropagation-re-parametrization><a aria-label="Anchor link for: 4-8-vae-backpropagation-re-parametrization" class=zola-anchor href=#4-8-vae-backpropagation-re-parametrization>4.8 VAE Backpropagation: Re-parametrization</a></h3><p>Backpropagation in Neural Networks requires deterministic nodes and layers (constant weights). Weights need to remain constant to calculate the chain rule to optimize the loss by gradient descent.<p>But remember that <strong>VAs impose a variational or stochastic</strong> twist in the forward propagation to generate new images and therefore you cannot backpropagate a sampling layer.<p><img title="Backpropagation on $z$ original form" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_backpropagation.png><p>Well, you actually can. The hidden latent space $z$ variation is not stochastic itself, it includes a random constant $\varepsilon$. Therefore, $z$ can be <em>reparametrized</em>, as $\varepsilon$ is just a constant that follows a normal distribution.<p><img title="Backpropagation on $z$ reparametrized form" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_backpropagation2.png><p>Notice that $z$ goes from being a stochastic node (left) to being a deterministic one (rigth). Again, this is because $z$ can be derived taking $\varepsilon$ as a random constant that follows a normal distribution. The function loss can be minimized since the chain rule can be applied to optimize the weigths of the encoding loss $q_{\phi}$.<h3 id=4-9-generative-adversarial-networks-gans><a aria-label="Anchor link for: 4-9-generative-adversarial-networks-gans" class=zola-anchor href=#4-9-generative-adversarial-networks-gans>4.9 Generative Adversarial Networks (GANs)</a></h3><p>GANs is another architecture to generate new data following the same distribution of the input data. The <em>Adversarial</em> part comes because in <strong>this architecture two neural networks contesting with each other in a zero-sum game where one network gain is the other network loss</strong>.<ol><li><strong>Generator Network</strong></ol><p>This network is trained to get random noise data and <strong>produce new (fake) samples that represent the noise distribution</strong> as much as possible. The random noise can be created sample out of a Gaussian distribution.<p>There are no encoding loss, the new features extracted comes from noise. Therefore, the distribution that the model is trying to learn comes from a random sampling, not real images.<p>Here in the next image, the Generator Network $G$ learns from a normal Gaussian distribution $z$ and creates new samples $X_{fake}$ that follows this distribution.<p><img title="Generator Network producing a feature latent space out of noise" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_generator.png><ol start=2><li><strong>Discriminator Network</strong></ol><p>This network takes the fake features from the Generator Network and real features from real images data. With both inputs, <strong>the Discriminator task is to identify the fake features from the real ones</strong>.<p><img title="Generative Adversarial Network structure" alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L4_gan.png><p><em>G</em> tries to synthesize fake instances that fool <em>D</em>, and <em>D</em> tries to identify these from real ones.<p>The two networks interact with each other, <strong>the better the Generator Network gets the hardest is for the Discriminator to tell apart</strong> fake from real features.<h3 id=4-10-gans-loss><a aria-label="Anchor link for: 4-10-gans-loss" class=zola-anchor href=#4-10-gans-loss>4.10 GANs Loss</a></h3><p>As they have different goals, the Generator and Discriminator network have different loss functions that combines into a <em>total</em> GAN loss function.<ol><li>Discriminator Network Loss</ol><p>$$ \arg \max_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))] $$<p>It maximizes the probability of the fake data to be identified as fake: $\log D(G(\mathbf{z}))$, and the real data being identified as real: $\log (1-D(\mathbf{x})$.<ol start=2><li>Generator Netowork Loss</ol><p>$$ \arg \min_{G} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))] $$<p>It minimizes the probability of the Discriminator Network <em>D</em> to identify fake data as fake: $\log D(G(\mathbf{z})$, and the real data being identified as real: $\log (1-D(\mathbf{x}))$.<p>We can combine both loss functions as the GANs Loss function:<p>$$ \arg \min_{G} \max_{D} \mathbb{E}_{\mathbf{z}, \mathbf{x}}[\log D(G(\mathbf{z}))+\log (1-D(\mathbf{x}))] $$<h2 id=lecture-6-reinforced-learning><a aria-label="Anchor link for: lecture-6-reinforced-learning" class=zola-anchor href=#lecture-6-reinforced-learning>Lecture 6 - Reinforced Learning</a></h2><p>In Reinforced Learning the data is not a matrix of features and set of targets (supervised learning), neither a matrix of features without targets that an algorithm has to cluster (unsupervised learning). The input data is a state-action pairs.<p>The algorithm <strong>learns from the consequences of some action in a certain state</strong>. If the action contributes to a reward function, then is encourage and if it doesn't the action is avoided. The <strong>goal</strong> of the algorithm is to maximize the total reward of the agent over time.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_classes.png><p>The <strong>key concepts</strong> to understand the mechanics of Reinforced Learning are:<ul><li><strong>Actions</strong>: behaviors that the system takes when it sees those states.<li><strong>Agent</strong>: It takes decisions in the environment.<li><strong>Environment</strong>: Where the agent takes action.<li><strong>Action space A</strong>: the set of possible actions an agent can make in the environment.</ul><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_actions.png><ul><li><strong>Observations</strong>: who the environment reacts to the agent's actions.<li><strong>State</strong>: a situation which the agent perceives.</ul><p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_observations.png><ul><li><strong>Reward</strong>: feedback that measures the success or failure of the agent's action.</ul><p>The <strong>Total Reward</strong> is a function of the sum of rewards through time:<p>$$ R_{t}=\sum_{i=t}^{\infty} r_{i}=r_{t}+r_{t+1} \ldots+r_{t+n}+\cdots $$<p>However, the time of the reward is an important element in the choice of action of the agent. Is not the same $20 today that $20 in 10 years.<p>The <strong>Discounted Total Reward</strong> is calculated as the sum of rewards through time, times a discount factor between 0 and 1:<p>$$ R_{t}=\sum_{i=t}^{\infty} \gamma^{i} r_{i}=\gamma^{t} r_{t}+\gamma^{t+1} r_{t+1} \ldots+\gamma^{t+n} r_{t+n}+\cdots $$<p>This reward function is also called the <em>Q-function</em>.<h3 id=6-1-the-q-function><a aria-label="Anchor link for: 6-1-the-q-function" class=zola-anchor href=#6-1-the-q-function>6.1 The Q-function</a></h3><p>The Q-function <strong>captures the expected total future reward</strong> $R$ an agent in the state $s$ can receive by executing a certain action $a$:<p>$$ Q\left(s_{t}, a_{t}\right)=\mathbb{E}\left[R_{t} \mid s_{t}, a_{t}\right] $$<p>When the agent has to decide on an environment, the agent considers the possible Q-functions to compare rewards. The higher the Q-function the better, as the reward is higher. We will call all the possible alternatives <em>policies</em>, denoted by ($\pi$), that the algorithm compares.<p>The optimal policy is inferred by the best action $a$. In formal terms, this represents the <strong>strategy</strong> for the agent. Choosing the optimal policy that maximizes the total discounted future reward:<p>$$ \pi^{*}(s)=\underset{a}{\operatorname{argmax}} Q(s, a) $$<p>Therefore, the agent chooses actions $a$ that maximize the Q-function.<h3 id=6-2-deep-q-networks-dqn><a aria-label="Anchor link for: 6-2-deep-q-networks-dqn" class=zola-anchor href=#6-2-deep-q-networks-dqn>6.2 Deep Q Networks (DQN)</a></h3><p>Imagine the following Deep Neural Network in which the state is an Atari videogame and the agent has the actions of moving right or left to destroy the bricks (reward). The goal of the game is to destroy all the bricks using the least possible moves.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_atari.png><p>The <strong>first solution</strong> that comes to mind is making a Neural Network that iterates over <strong>all the possible state-action pairs</strong> and takes the one that maximizes the expected reward returns.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_state_actions.png><p>However, even for this simple environment <strong>the amount of actions $a$ would be enormous</strong>. It would need to calculate the rewards over the time of the entire game, every time it chooses right or left. Every time, for every move $t$, calculating the optimal action. This method to find a strategy is highly inefficient and computationally very expensive.<p><strong>A more efficient way to input only the state and let the Neural Network give all the possible set of Q-functions for each action</strong>. Using this method, the Neural Network does not have to compare over Q-functions, it returns all of them for every possible action.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_dqn.png><p>Notice that we don't feed the state, only actions, so the possibilities do not grow exponentially.<p>The main issue that comes here is how to train this Neural Network. If the Network does not compare between the Q-functions, how it knows which one is better so the model can be trained?<p>We can introduce a benchmark that the algorithm compares with. A perfect score that the algorithm has to reach by comparing Q-functions. In other words, <strong>Creating a ground-truth Q-target</strong>. Therefore, the distance with this perfect Q-target is our loss function (MSE) to train the Neural Network:<p>$$ \mathcal{L}=\mathbb{E}[| \overbrace{\left(r+\gamma \max_{a \prime} Q\left(s^{\prime}, a^{\prime}\right)\right.}^{\text {target }})-\overbrace{\left.Q(s, a) |^{2}\right]}^{\text {predicted }} $$<p>The Network predicts state-action pairs $Q(s,a_{n})$ that closes the Q-target as much as possible. The better match with the target function, the smaller is the loss and the better is the policy.<p>Going back to the <strong>Atari example</strong>:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_atari_2.png><p>For a given state $s_t$ the ball is coming towards the slider. The Neural Network creates 3 Q-functions: one for the slider going left, one for the slider staying, and one for the slider going right.<p>The action $a_1$ (slider to the left) has a better payoff reward, so the optimal policy $\pi$ or strategy for this state $s_t$ is going left.<p>Then it stats over for the state $s_{t+1}$.<h3 id=6-3-downsides-of-q-learning><a aria-label="Anchor link for: 6-3-downsides-of-q-learning" class=zola-anchor href=#6-3-downsides-of-q-learning>6.3 Downsides of Q-Learning</a></h3><ul><li>Complexity</ul><p><strong>The models can only use scenarios where the action space is discrete and small</strong>. That is why Q-learning is so popular in video games because is a controlled environment with a limited set of actions. As an example of a continuous and infinite space, consider a driving car with the possibility to go towards any direction (infinite angles of directions).<ul><li>Flexibility</ul><p>The optimal policy is <strong>deterministically computed from the Q-function</strong> by maximizing the reward. It cannot compute the reward of any distribution of data outside de range of possible environment scenarios. It cannot learn stochastic policies that might get better rewards.<h3 id=6-4-policy-learning><a aria-label="Anchor link for: 6-4-policy-learning" class=zola-anchor href=#6-4-policy-learning>6.4 Policy Learning</a></h3><p>Until now, we were taking the state as fixed and looking for the best set of actions that maximizes the rewards. The algorithm learns using the possible actions. Instead of optimizing for actions, we can optimize for policies.<p>The difference is subtle but important. To illustrate we go back to the <strong>Atari example</strong>:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_atari_3.png><p>The Neural Network does not compare actions, it compares the probability of the actions being the correct and sample from that. In this Atari example, going left has a 90% probability of reaching the optimal policy. Therefore, the algorithm choose $a_1$ (slider left) 90% of the time, $a_2$ (center) 10% of the time and $a_3$ (right) 0% of the time. The probabilities sum to 1. This is called <strong>Policy gradient</strong>, as it directly optimizes the policy without calculating the rewards.<p><strong>What advantages has Policy Learning over Q-Learning?</strong><p>With policy learning, the models can handle continuous spaces. We can calculate not only the left or right (discrete choice) but the speed of the slider (continuous choice). For example, the slider can go 1 meter, 2 meters, or 20 meters to the left. Each continuous set of actions has attached a probability of being the best policy.<p>In the following image, the highest probability action is going to the -1 meters/second in a continuous probability space with many other rates:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L5_atari_4.png><p>From this probability, the optimal policy derived is going -0.8 meters/second, taking into account the variance.<p>This opens the possibility of computing in environments in which the actions are continuous like autonomous vehicles.<h3 id=6-5-policy-learning-gradients><a aria-label="Anchor link for: 6-5-policy-learning-gradients" class=zola-anchor href=#6-5-policy-learning-gradients>6.5 Policy Learning Gradients</a></h3><p>The loss function is calculated by multiplying 2 terms: the probability of an action given the state, and the reward attached to that action. These elements are captured by the negative log-likelihood of the policy ($-log...$) and the total discounted returns ($R_t$)<p>$$ \mathcal{L}= -log P(a_t|a_s)R_t $$<p>The algorithm wants to minimize this loss function as much as possible. Therefore, the larger the probability of the policy and the reward, the more negative is the loss function.<p>Also, it amplifies the actions with high reward but low probability, and low reward but a high probability of happening.<p>The gradient descent or weight updating of the neural network works like in the vanilla neural network gradient descent: computes the gradient and updates the weights according to the direction of the gradient. As we said, the gradient descent is made with respect to the policy, not the actions:<p>$$ w' = w - \nabla\mathcal{L} $$<p>$$ w' = w + \nabla \underbrace{\log P(a_t|a_s)R_t}_\text{Policy gradient} $$<p>Notice that by using the <strong>negative</strong> log-likelihood instead of positive, it turns the weight updating to positive. Hence, the network weighs more the set of actions with a combination of high probability/high reward.<h2 id=lecture-7-deep-learning-limitations-and-new-frontiers><a aria-label="Anchor link for: lecture-7-deep-learning-limitations-and-new-frontiers" class=zola-anchor href=#lecture-7-deep-learning-limitations-and-new-frontiers>Lecture 7 - Deep Learning Limitations and New Frontiers</a></h2><p>The rise and hype of Deep Learning led to the general public to see Machine Learning, Deep Learning, and the whole AI field like some kind of <strong>alchemy</strong>. Any problem in which we have data can be solved by AI. The reality is that only very specific problems can be solved by AI, and feeding poor data in a random network architecture will produce no value at all.<p><strong>What Deep Learning is good at?</strong><p>Deep Neural Networks are extremely good at finding a pattern <strong>in the existing data</strong>. A recent paper by Google Brain/Deepmind researchers<sup class=footnote-reference><a href=#1>1</a></sup> shows that a neural network can be 100% accurate fed <strong>by random label images</strong>:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_understanding.png> <img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_understanding_1.png><p>A model is as good as the data you feed it. If you have trained a model with a banana image and a tree image labeled as "dog", every time that the model sees <strong>those exact images</strong> it will classify them as the label you have used. The problem comes when it sees other bananas or tree images. The accuracy could be 100% in the training set and close-to-random accuracy in the test set:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_understanding_2.png><p>Random labeling led to random accuracy in the test data. The model overfitted the specific images to the specific label and has <strong>no generalization power</strong> to predict new unseen data. <strong>Without generalization, any neural network is worthless.</strong> A Neural Network can approximate any seen distribution, but how do we know what and how it is going to predict in unseen data?<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_approximation.png><h3 id=7-1-limitations-uncertainty><a aria-label="Anchor link for: 7-1-limitations-uncertainty" class=zola-anchor href=#7-1-limitations-uncertainty>7.1 Limitations: Uncertainty</a></h3><p>Part of the new frontiers in AI tries to solve <strong>the problem of uncertainty</strong>. Or how to make models that infer the right choice when it faces data that it has not being trained to interact with. For example, this is especially important in the field of autonomous vehicles, in which a change in the construction of a road can lead to terrible results:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_uncertainty.png><p>Autonomous cars should be able to identify this <strong>uncertainty</strong> or unseen state of the road and not crash the car, even if the car is being trained to go in that direction.<p>Let's take the classical toy model of <strong>classifying an image of a dog vs cat</strong>. The model takes only inputs of cat and dog images and returns the probability of being a cat vs a dog. What happens if we ask the model to predict an image of a dog and a cat together? What happens if we ask the model to predict the probability of a cat/dog feeding the image of a horse?<p>By definition, <strong>the model gives just the probability of dog and cat</strong>. It cannot output the probability of random data or the confidence in that prediction.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_horse.png><p>We need an uncertainty metric to assess the noise inherent to the data (<em>aleatoric uncertainty</em>) and to <strong>assess the network's confidence</strong> in its predictions (<em>epistemic uncertainty</em>).<h3 id=7-2-frontiers-evidential-neural-networks><a aria-label="Anchor link for: 7-2-frontiers-evidential-neural-networks" class=zola-anchor href=#7-2-frontiers-evidential-neural-networks>7.2 Frontiers: Evidential Neural Networks</a></h3><p>New research<sup class=footnote-reference><a href=#2>2</a></sup> using <em>adversarial attacks</em> tries to introduce perturbations into the data so the networks it is <strong>not only optimized by modifying the weights but also optimized by modifying the input images</strong>. Given an input, the network is trained to predict the parameters of what they called an evidential distribution.<p>The network can model a higher-order probability distribution over the individual likelihood parameters. Taking the cat/dog model example, this means that a network trained with photos of cats and dogs and fed with a horse image can output cat probability of 0 and a dog probability of 0.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_deep_regression.png><p>Evidential regression simultaneously learns a continuous target along with aleatoric uncertainty from the data and epistemic uncertainty from the model.<h3 id=7-3-frontiers-automated-machine-learning><a aria-label="Anchor link for: 7-3-frontiers-automated-machine-learning" class=zola-anchor href=#7-3-frontiers-automated-machine-learning>7.3 Frontiers: Automated Machine Learning</a></h3><p>Standard deep neural networks are optimized <strong>for a single task</strong>. It often requires expert knowledge to build an architecture for any task. What if we could build a learning algorithm or system that <strong>learns which model</strong> to use to solve a given problem?<p>Automated Machine Learning (AutoML) is a growing field of AI<sup class=footnote-reference><a href=#3>3</a></sup> that uses auto-detection of network architectures, so it relies less on human choice and expertise as it learns the model architectures directly on the dataset of interest.<p>The <strong>concept</strong> of this method is simple to understand. It is a system <strong>has a <em>controller network</em> and a <em>child network</em></strong>. The controller samples an initial architecture and the child uses that architecture in a dataset. For every architecture sample, the child network gets an accuracy value that is used to update the controller.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_autoML.png><p>The better the architecture and parameters proposed by the controller is, the better the results of the child network, and the more the controller knows is getting good architecture proposals.<p>This last step is key. <strong>How the controller learns?</strong><p>The controller is a Recurrent Neural Network, with <em>N</em> layers corresponding to different architectures and parameters to choose from. One layer represents a combination of model/parameters to try.<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_autoML_1.png><ol><li>The controller samples these different networks with a parametrization.<li>The controller feed variations to the child network.<li>The child produces an accuracy <em>R</em> that is used to train the weights of the controller.<li>Once it has the best parameter, the one with better accuracy in the child network, that layer is optimized and jumps to the next one.<li>Repeat until all the layers (parameters) converge.</ol><p>While AutoML can be seen as a shortcut, <strong>this system can produce state-of-the-art results</strong><sup class=footnote-reference><a href=#3>3</a></sup> in image recognition, getting better results and being more efficient than human-created network architectures:<p><img alt src=https://www.pipegalera.com/mostly_books/course-mit-intro-deep-learning/./images/L6_autoML_2.png><p><em>NASNet</em> stands for <em>Neural Architecture Search Network</em><div class=footnote-definition id=1><sup class=footnote-definition-label>1</sup><p>C. Zhang et al. (2016) - Understanding deep learning requires rethinking generalization: https://arxiv.org/abs/1611.03530</div><div class=footnote-definition id=2><sup class=footnote-definition-label>2</sup><p>A Amini et al. (2019) - Deep Evidential Regression: https://arxiv.org/abs/1910.02600</div><div class=footnote-definition id=3><sup class=footnote-definition-label>3</sup><p>B. Zoph (2017) - Learning Transferable Architectures for Scalable Image Recognition: https://arxiv.org/abs/1707.07012</div></section></article></main><div class=giscus></div><script async crossorigin issue-term=pathname repo=YOUR_NAME/YOUR_REPO src=https://utteranc.es/client.js theme=github-light></script><footer><div class=footer-content><p>100% human written content by Pipe Galera © 2025. Theme: <a href=https://github.com/not-matthias/apollo>Apollo</a>.</div></footer></div>