<!doctype html><html class="dark light" lang=en><head><meta charset=UTF-8><meta content="IE=edge" http-equiv=X-UA-Compatible><meta content="width=device-width,initial-scale=1.0" name=viewport><meta content=https://www.pipegalera.com/ name=base><title>
            
                Course Notes - Neural Networks and Deep Learning
            
        </title><meta content="Course Notes - Neural Networks and Deep Learning" property=og:title><meta content="My personal notes of DeepLearning.AI deep learning course." property=og:description><meta content="My personal notes of DeepLearning.AI deep learning course." name=description><link href=https://cdn.jsdelivr.net/npm/jetbrains-mono@1.0.6/css/jetbrains-mono.min.css rel=stylesheet><link href=https://cdn.jsdelivr.net/npm/@fontsource/space-grotesk@4.5.8/index.min.css rel=stylesheet><script src=https://www.pipegalera.com/js/codeblock.js></script><script src=https://www.pipegalera.com/js/toc.js></script><script src=https://www.pipegalera.com/js/note.js></script><script>MathJax = {
              tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
              }
            };</script><script async id=MathJax-script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><link href=https://www.pipegalera.com/atom.xml rel=alternate title=~/.pipe_galera type=application/atom+xml><link href=https://www.pipegalera.com/theme/light.css rel=stylesheet><link href=https://www.pipegalera.com/main.css media=screen rel=stylesheet><script src="https://www.pipegalera.com/js/searchElasticlunr.min.js?h=3626c0ef99daa745b31e" defer></script><body><div class=content><header><div class=main><a href=https://www.pipegalera.com/>~/.pipe_galera</a><div class=socials><a class=social href=https://www.linkedin.com/in/pipegalera/ rel=me> <img alt=linkedin src=https://www.pipegalera.com/icons/social/linkedin.svg> </a><a class=social href=https://github.com/pipegalera/ rel=me> <img alt=github src=https://www.pipegalera.com/icons/social/github.svg> </a></div></div><nav><a href=https://www.pipegalera.com/posts style=margin-left:.25em>/posts</a><a href=https://www.pipegalera.com/mostly_books style=margin-left:.25em>/mostly_books</a><a href=https://www.pipegalera.com/tags style=margin-left:.25em>/tags</a><button title="$SHORTCUT to open search" class=search-button id=search-button><img alt=Search class=search-icon src=https://www.pipegalera.com/icons/search.svg></button><div class="search-modal js" aria-labelledby=modalTitle id=searchModal role=dialog><div id=modal-content><h1 class=page-header id=modalTitle>Search</h1><div id=searchBar><input aria-controls=results-container aria-expanded=false autocomplete=off id=searchInput placeholder=Search... role=combobox spellcheck=false><button title="Clear search" class=clear-button id=clear-search><svg viewbox="0 -960 960 960" xmlns=http://www.w3.org/2000/svg><path d="m256-200-56-56 224-224-224-224 56-56 224 224 224-224 56 56-224 224 224 224-56 56-224-224-224 224Z"/></svg></button></div><div id=results-container><div id=results-info><span id=zero_results style=display:none>No results</span><span id=one_result style=display:none>1 result</span><span id=many_results style=display:none>$NUMBER results</span></div><div id=results role=listbox></div></div></div></div></nav></header><main><article><div class=title><div class=page-header>Course Notes - Neural Networks and Deep Learning<span class=primary-color style=font-size:1.6em>.</span></div><div class=meta>Posted on <time>2024-05-10</time> :: 3013 Words <span class=tags-label>:: Tags:</span><span class=tags> <a class=post-tag href=https://www.pipegalera.com/tags/courses/>courses</a> , <a class=post-tag href=https://www.pipegalera.com/tags/machine-learning/>machine learning</a> , <a class=post-tag href=https://www.pipegalera.com/tags/deep-learning/>deep learning</a> , <a class=post-tag href=https://www.pipegalera.com/tags/python/>python</a> </span></div></div><div class=toc-container><h1 class=toc-title>Table of Contents</h1><ul class=toc-list><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#1-introduction-to-deep-learning>1. Introduction to Deep Learning</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#1-1-what-is-a-neural-network>1.1 What is a Neural Network?</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#1-2-supervised-learning-with-neural-networks>1.2 Supervised Learning with Neural Networks</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-neural-network-basics>2. Neural Network Basics</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-1-logistic-regression-as-neural-network>2.1 Logistic Regression as Neural Network</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-2-evaluating-the-model-with-log-loss>2.2 Evaluating the model with Log-Loss</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-3-gradient-descent-and-logistic-regression>2.3 Gradient Descent and Logistic Regression</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-4-vectorization>2.4 Vectorization</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-5-broadcasting>2.5 Broadcasting</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#2-6-neural-network-from-the-scratch-in-python>2.6 Neural Network from the scratch in Python</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#3-shallow-neural-network>3. Shallow Neural Network</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#3-1-neural-networks-overview-and-vectorized-implementation>3.1 Neural Networks Overview and Vectorized Implementation</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#3-2-activation-functions>3.2 Activation functions</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#3-3-gradient-descent>3.3 Gradient Descent</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#3-4-random-initialization>3.4 Random Initialization</a></ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#4-deep-neural-networks>4. Deep Neural Networks</a> <ul><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#4-1-forward-propagation-in-deep-networks>4.1 Forward Propagation in Deep Networks</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#4-2-getting-the-dimensions-of-a-neural-network>4.2 Getting the dimensions of a Neural Network</a><li><a href=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/#4-3-parameters-vs-hyperparameters>4.3 Parameters vs Hyperparameters</a></ul></ul></div><section class=body><h2 id=1-introduction-to-deep-learning><a aria-label="Anchor link for: 1-introduction-to-deep-learning" class=zola-anchor href=#1-introduction-to-deep-learning>1. Introduction to Deep Learning</a></h2><h3 id=1-1-what-is-a-neural-network><a aria-label="Anchor link for: 1-1-what-is-a-neural-network" class=zola-anchor href=#1-1-what-is-a-neural-network>1.1 What is a Neural Network?</a></h3><p>Neural Networks are collection of connected units or nodes called "neurons", which loosely model the neurons in a biological brain. The combination of neurons <em>learn</em> to perform tasks by considering examples, without being programmed with task-specific rules.<p>As an example, consider that we have to predict the price of houses using their size. We have 6 houses in the dataset. Every neuron applies the function in grey:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/1.png><p>All the neuron does is taking the size of the house as input, apply a transformation equation called ReLU and the output is the estimated the price.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/2.png><p>ReLU, or Rectified Linear Unit, is a simple function that takes the input and returns the same value if positive, otherwise returns zero:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>relu</span><span style=color:#657b83>(</span><span style=color:#268bd2>input</span><span style=color:#657b83>):
</span><span>	</span><span style=color:#859900>if input </span><span style=color:#657b83>> </span><span style=color:#6c71c4>0</span><span style=color:#657b83>:
</span><span>		</span><span style=color:#859900>return input
</span><span>	</span><span style=color:#859900>else</span><span style=color:#657b83>:
</span><span>		</span><span style=color:#859900>return </span><span style=color:#6c71c4>0
</span></code></pre><p>All the neurons pass ReLU, or other similar kind of functions called “activation functions”.<p>A <strong>Neural Network (NN)</strong> is created by stacking neurons together. Each of the neurons stacked implements ReLU, or some other slightly non-linear function.<p>A NN needs a large number of inputs and their output, a lot of examples to find the pattern that explains the output.<p>In the example, a large dataset of houses prices associated with their size. The "<strong>hidden layers</strong>" are made by the neurons themselves.<p>The first layer, the <strong>input layer</strong>, and the hidden layers are connected: every input feature is connected to every "hidden" feature.<p>The NN feeds the features size, the number of bedroom, zipcode, wealth and so to "hidden units" that explains the prize. This hidden units are made by the neurons without explicitly telling them. Therefore, some times you can find these models refered as "<strong>black boxes</strong>” due to the process not being explicit in the hidden layers:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/3.png><p>For example, the NN is modelling “family size” given the combination of size of size and number of bedrooms.<h3 id=1-2-supervised-learning-with-neural-networks><a aria-label="Anchor link for: 1-2-supervised-learning-with-neural-networks" class=zola-anchor href=#1-2-supervised-learning-with-neural-networks>1.2 Supervised Learning with Neural Networks</a></h3><p>There are different types of neural networks besides the standard architecture seen before. This classical standard architecture is still useful for tabular data, such as user information to predict online shopping. But there are other kind of problems that require other kind of neural networks.<p>For example Convolution Neural Network (CNN) used often for image application. Recurrent Neural Networks (RNN) are used for one-dimensional sequence data such as translating English to Chinese or a temporal component such as text transcript. Hybrid Neural Networks architecture can be used for autonomous driving model.<p>For now the only difference that we have to know is between structured and unstructured data:<ul><li><strong>Structured data</strong>: it has a defined label, such as price and size. Also called tabular data.<li><strong>Unstructured data</strong>: it does not have a define meaning by itself. Like a pixel, raw audio or text.</ul><h2 id=2-neural-network-basics><a aria-label="Anchor link for: 2-neural-network-basics" class=zola-anchor href=#2-neural-network-basics>2. Neural Network Basics</a></h2><h3 id=2-1-logistic-regression-as-neural-network><a aria-label="Anchor link for: 2-1-logistic-regression-as-neural-network" class=zola-anchor href=#2-1-logistic-regression-as-neural-network>2.1 Logistic Regression as Neural Network</a></h3><p>In a <strong>binary classification problem</strong>, the result is a discrete value output: a 1 or a 0. For example, trying to explain a catastrophe survival rate: a person survived (1) or did not (0).<p>The feature matrix shape is made "stacking" the number of features($n_x$) in different columns, one for every observation ($m$): $X.shape = (n_x, m)$.<p>The output shape is a 1 by $m$ dimensional matrix: $y.shape = (1,m)$.<p>The prediction $\hat{y}$ (0 or 1) is determined by the probability of a real $y$ given factors $X$:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/4.png><p>The probability is calculated by a <code>sigmoid</code> activation function that takes a linear regression $z$ as input:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/6.png><p>Lastly, $Z$ is just a linear regresion.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/5.png><p>Here I took the notation of "Betas" because this is what I am used to in Economics, but from here on I’ll refer at them as “Weights” and “bias” instead. It is the same with different notation.<p>Please notice that a linear regression that goes into a sigmoid activation is a “Logistic regression” with different wording.<p>Think about a Neural Network as a collection of “neurons” or nodes that take inputs and apply a sigmoid function. The sigmoid function is simply a logistic transformation that wraps the features($n_x$) so the values of $y$ are forced to be 0 or 1.<p>NN = a lot of logistic regressions averaged.<p>Example:<p>We want to calculate the probability of tomorrow being sunny (1) or rainy (0). All the days in the training set have associated temperatures ($X$) to those days. Since sunny days tend to be warmer, the linear regression will give $\hat{y}$ closer to 1 when the temperatures are high, and close to zero when it rains. Sigmoid transforms the closer numbers to 1 to exactly 1 and the closer to 0 to exactly 0, and that’s the prediction $\hat{y}$.<h3 id=2-2-evaluating-the-model-with-log-loss><a aria-label="Anchor link for: 2-2-evaluating-the-model-with-log-loss" class=zola-anchor href=#2-2-evaluating-the-model-with-log-loss>2.2 Evaluating the model with Log-Loss</a></h3><p>✍️ NOTE: added info from https://mlu-explain.github.io/logistic-regression . It assumes familiarity with linear algebra.<p>For now, lets continue with a simple logistic regression as a base NN.<p>Please notice that $z$ calculation (linear regression) only relies on $X$ that is a matrix given by the data itself and the weights $W$.<p>How do we set the $W$ then and how does it now that $W$ are well defined?<p>To explain it in an intuitive way, we can use a “comparison mechanism” to set $W$. The mechanism or function compares the $\hat{y}$ and $y$ difference. If they are the same the $W$ are good for the data, and if not it tries again with new $W$.<p>This comparison mechanism or equation is called “Loss function”. For logistic regression, a suitable loss function is Log-Loss, also called binary cross-entropy:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/7.png><p>Notice that the equation is only a function of $y$ and $\hat{y}$ - it “compares” them.<p>It is made that way so if the real label and the predicted label are the same, it equals zero. The worse the model the higher the result is (try replacing $y=0$ and $/haty=1$ for all $i$).<p>That’s why it tells how well the logistics regression is doing. The lower the function value, the better the logistic regression is predicting labels.<h3 id=2-3-gradient-descent-and-logistic-regression><a aria-label="Anchor link for: 2-3-gradient-descent-and-logistic-regression" class=zola-anchor href=#2-3-gradient-descent-and-logistic-regression>2.3 Gradient Descent and Logistic Regression</a></h3><p>From Log-Loss, the logistic regression “knows” which coefficients $W$ are better for the inputs $X$, but... How it sets the values of $W$ in the first place, and how exactly “update” them ?<p>A common way to estimate coefficients is to use gradient descent. The gradient calculates where the Log-Loss function is increasing, so going in the opposite direction leads us to the minimum of our function.<p>In simple terms:<ol><li>Let’s start with random weights $W$ for every observation.<li>The weights determine $z$, that gives you a $\hat{y}$. With $\hat{y}$ and $y$, the Log-Loss will drop real number.<li>Let’s change the $W$ a tiny amount (think +0.00001) and calculate again the Log-Loss. The number went up or down?<li>If the number went down keep increasing the $W$ value, otherwise change direction (decrease tiny bit and see the Log-Loss).</ol><p>This only works because the shape of the Log-Loss equation is convex, so the slope of the curve guides to the minimum of the equation.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/8.png><p>Given it’s shape, the slope of Log-Loss can be used to determine the best set of weights.<p>Programmatically, the scope is calculated by derivation of the function given a set of $W$. And the gradient descent algorithm is designed to iterate over tiny increments for every observation.<p>At each iteration, the $W$ value is updated by a tiny amount (the gradient), scaled by the step size $\alpha$ (learning rate). The bigger the learning rate, the higher the steps.<p>Aside from gradient descent, Maximum Likelihood Estimation (MLE) can be used to estimate the $W$.<h3 id=2-4-vectorization><a aria-label="Anchor link for: 2-4-vectorization" class=zola-anchor href=#2-4-vectorization>2.4 Vectorization</a></h3><p>As a side note, gradient descent calculations demonstrate why the rise of Deep Learning could not have happened in the past. It needs a huge computational power by hardware that we are lucky to have today. Notice that the logistic function output, the change in the coefficients, and the Log-Loss function have to be calculated again and again.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/9.png><p>We also have had progress in the way we do the computation iteration.<p>In the section above I mentioned: <em>“And the algorithm iterates over tiny increments for every observation.”</em><p>The iteration doesn’t use loops since it would be too computationally costly - it uses vectorization.<p>Gradient descent needs to compute over and over $z$ to get the prediction for all the values of $i$.<p>This can be done via loop or via vectorization. For loops takes +1000 times more time:<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#cb4b16>import </span><span>numpy </span><span style=color:#cb4b16>as </span><span>np
</span><span style=color:#cb4b16>import </span><span>time
</span><span style=color:#586e75># Data
</span><span>X </span><span style=color:#657b83>= </span><span>np.random.</span><span style=color:#b58900>rand</span><span style=color:#657b83>(</span><span style=color:#6c71c4>100000000</span><span style=color:#657b83>)
</span><span>W </span><span style=color:#657b83>= </span><span>np.random.</span><span style=color:#b58900>rand</span><span style=color:#657b83>(</span><span style=color:#6c71c4>100000000</span><span style=color:#657b83>)
</span><span>
</span><span style=color:#586e75># For loop
</span><span>tic </span><span style=color:#657b83>= </span><span>time.</span><span style=color:#b58900>time</span><span style=color:#657b83>()
</span><span style=color:#859900>for </span><span>i </span><span style=color:#859900>in range</span><span style=color:#657b83>(</span><span style=color:#6c71c4>100000000</span><span style=color:#657b83>):
</span><span>  z </span><span style=color:#657b83>+= </span><span>X</span><span style=color:#268bd2>[</span><span>i</span><span style=color:#268bd2>]</span><span style=color:#657b83>*</span><span>W</span><span style=color:#268bd2>[</span><span>i</span><span style=color:#268bd2>]
</span><span style=color:#859900>print</span><span style=color:#657b83>(</span><span>z</span><span style=color:#657b83>)
</span><span>toc </span><span style=color:#657b83>= </span><span>time.</span><span style=color:#b58900>time</span><span style=color:#657b83>()
</span><span style=color:#859900>print</span><span style=color:#657b83>(</span><span>'</span><span style=color:#2aa198>Loop took: </span><span>' </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>((</span><span>toc</span><span style=color:#657b83>-</span><span>tic</span><span style=color:#657b83>)*</span><span style=color:#6c71c4>1000</span><span style=color:#657b83>) + </span><span>'</span><span style=color:#2aa198>ms</span><span>'</span><span style=color:#657b83>)
</span><span> </span><span style=color:#657b83>--> </span><span>Loop took: </span><span style=color:#6c71c4>18357.781887054443</span><span>ms
</span><span>
</span><span style=color:#586e75># Vectorization
</span><span>tic </span><span style=color:#657b83>= </span><span>time.</span><span style=color:#b58900>time</span><span style=color:#657b83>()
</span><span>z </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>dot</span><span style=color:#657b83>(</span><span>X,W</span><span style=color:#657b83>)
</span><span style=color:#859900>print</span><span style=color:#657b83>(</span><span>z</span><span style=color:#657b83>)
</span><span>toc </span><span style=color:#657b83>= </span><span>time.</span><span style=color:#b58900>time</span><span style=color:#657b83>()
</span><span style=color:#859900>print</span><span style=color:#657b83>(</span><span>'</span><span style=color:#2aa198>Vectorization took: </span><span>' </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>((</span><span>toc</span><span style=color:#657b83>-</span><span>tic</span><span style=color:#657b83>)*</span><span style=color:#6c71c4>1000</span><span style=color:#657b83>) + </span><span>'</span><span style=color:#2aa198>ms</span><span>'</span><span style=color:#657b83>)
</span><span> </span><span style=color:#657b83>--> </span><span>Vectorization took: </span><span style=color:#6c71c4>18.301010131835938</span><span>ms
</span></code></pre><p>Section 3 it will show that by stacking $X$ and $W$ vectorisation the neural network is more efficient computationally.<h3 id=2-5-broadcasting><a aria-label="Anchor link for: 2-5-broadcasting" class=zola-anchor href=#2-5-broadcasting>2.5 Broadcasting</a></h3><p>Another convenient process that happens in the background is “broadcasting”. It’s a matrix transformation that Python automatically applies to do faster matrix multiplications.<p>Python transform the constant (or 1x1 matrix) “$b$” and expand to a “$1xm$” matrix when we use it in matrix operations.<p>As a "General Principle": When you sum, subtract, divide or multiply $(m,n)$ matrix with a $(1,n)$ matrix the $(1,n)$ matrix will be expanded to a $(m,n)$ matrix by copying the row $m$ times to match the shape.<p>This allows to write quite a flexible code, but it also allows to start creating product matrices that create bugs difficult to track.<p>TIP: Specify the matrix shape and don't use rank 1 matrices. Use $np.random.randn(5,1)$ instead of $np.random.randn(5)$<h3 id=2-6-neural-network-from-the-scratch-in-python><a aria-label="Anchor link for: 2-6-neural-network-from-the-scratch-in-python" class=zola-anchor href=#2-6-neural-network-from-the-scratch-in-python>2.6 Neural Network from the scratch in Python</a></h3><p>In the exercise section it propose a simple logistic regression model identifying cats from non-cats images. They provide already the images.<p>Instead of the pre-made data given, I took images from Cats and Dogs from Kaggle dataset: https://www.kaggle.com/datasets/erkamk/cat-and-dog-images-dataset?resource=download<p>Visually, the model looks like:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/10.png><ul><li><strong>1. Create arrays of cats and dogs from Kaggle dataset</strong></ul><pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>extract_images</span><span style=color:#657b83>(</span><span style=color:#268bd2>foldername</span><span style=color:#657b83>):
</span><span>    </span><span style=color:#586e75># Training data from Kaggle Cats and Dogs dataset
</span><span>    all_images </span><span style=color:#657b83>= []
</span><span>
</span><span>    </span><span style=color:#859900>for </span><span>filename </span><span style=color:#859900>in </span><span>os.</span><span style=color:#b58900>listdir</span><span style=color:#657b83>(</span><span style=color:#268bd2>f</span><span>'</span><span style=color:#2aa198>train/</span><span style=color:#657b83>{</span><span>foldername</span><span style=color:#657b83>}</span><span>'</span><span style=color:#657b83>):
</span><span>        f </span><span style=color:#657b83>= </span><span>os.path.</span><span style=color:#b58900>join</span><span style=color:#657b83>(</span><span style=color:#268bd2>f</span><span>'</span><span style=color:#2aa198>train/</span><span style=color:#657b83>{</span><span>foldername</span><span style=color:#657b83>}</span><span>', filename</span><span style=color:#657b83>)
</span><span>
</span><span>        </span><span style=color:#859900>if </span><span>os.path.</span><span style=color:#b58900>isfile</span><span style=color:#657b83>(</span><span>f</span><span style=color:#657b83>):
</span><span>            image </span><span style=color:#657b83>= </span><span>Image.</span><span style=color:#b58900>open</span><span style=color:#657b83>(</span><span>f</span><span style=color:#657b83>)</span><span>.</span><span style=color:#b58900>resize</span><span style=color:#657b83>((</span><span style=color:#6c71c4>64</span><span>,</span><span style=color:#6c71c4>64</span><span style=color:#657b83>))
</span><span>            image_array </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>array</span><span style=color:#657b83>(</span><span>image</span><span style=color:#657b83>)
</span><span>            all_images.</span><span style=color:#b58900>append</span><span style=color:#657b83>(</span><span>image_array</span><span style=color:#657b83>)
</span><span>
</span><span>    </span><span style=color:#586e75># Arry format and normalize pixels
</span><span>    all_images </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>array</span><span style=color:#657b83>(</span><span>all_images</span><span style=color:#657b83>) / </span><span style=color:#6c71c4>255
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>all_images
</span><span>
</span><span>cats </span><span style=color:#657b83>= </span><span style=color:#b58900>extract_images</span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Cat</span><span>"</span><span style=color:#657b83>)
</span><span>dogs </span><span style=color:#657b83>= </span><span style=color:#b58900>extract_images</span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Dog</span><span>"</span><span style=color:#657b83>)
</span><span>labels </span><span style=color:#657b83>= </span><span>pd.</span><span style=color:#b58900>read_pickle</span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>train/y.pickle</span><span>"</span><span style=color:#657b83>)
</span></code></pre><ul><li><strong>2. Create a train and test set</strong></ul><p>The data is not divided into test and train, so you have to create the sets.<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span>X </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>concatenate</span><span style=color:#657b83>((</span><span>cats,dogs</span><span style=color:#657b83>)</span><span>, </span><span style=color:#268bd2>axis</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0</span><span style=color:#657b83>)
</span><span>y </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>array</span><span style=color:#657b83>(</span><span>labels</span><span style=color:#657b83>)</span><span>.</span><span style=color:#b58900>reshape</span><span style=color:#657b83>(-</span><span style=color:#6c71c4>1</span><span>, </span><span style=color:#6c71c4>1</span><span style=color:#657b83>)
</span><span>
</span><span style=color:#586e75># Shuffle all the indexes
</span><span>np.random.</span><span style=color:#b58900>seed</span><span style=color:#657b83>(</span><span style=color:#6c71c4>4208</span><span style=color:#657b83>)
</span><span>indexes </span><span style=color:#657b83>= </span><span>np.random.</span><span style=color:#b58900>choice</span><span style=color:#657b83>(</span><span>X.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]</span><span>, </span><span style=color:#6c71c4>1000</span><span>, </span><span style=color:#268bd2>replace</span><span style=color:#657b83>=</span><span style=color:#b58900>False</span><span style=color:#657b83>)
</span><span>
</span><span style=color:#586e75># First 800 in the training set, 200 resting in the test set
</span><span>train_index </span><span style=color:#657b83>= </span><span>indexes</span><span style=color:#268bd2>[</span><span>:</span><span style=color:#6c71c4>800</span><span style=color:#268bd2>]
</span><span>test_index </span><span style=color:#657b83>= </span><span>indexes</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>800</span><span>:</span><span style=color:#268bd2>]
</span><span>
</span><span>X_train </span><span style=color:#657b83>= </span><span>X</span><span style=color:#268bd2>[</span><span>train_index</span><span style=color:#268bd2>]
</span><span>y_train </span><span style=color:#657b83>= </span><span>y</span><span style=color:#268bd2>[</span><span>train_index</span><span style=color:#268bd2>]</span><span>.T
</span><span>X_test </span><span style=color:#657b83>= </span><span>X</span><span style=color:#268bd2>[</span><span>test_index</span><span style=color:#268bd2>]
</span><span>y_test </span><span style=color:#657b83>= </span><span>y</span><span style=color:#268bd2>[</span><span>test_index</span><span style=color:#268bd2>]</span><span>.T
</span><span>
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Number of training examples: m_train = </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>X_train.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]</span><span style=color:#657b83>))
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Number of testing examples: m_test = </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>X_test.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]</span><span style=color:#657b83>))
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>X_train shape: </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>X_train.shape</span><span style=color:#657b83>))
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>y_train shape: </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>y_train.shape</span><span style=color:#657b83>))
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>X_test shape: </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>X_test.shape</span><span style=color:#657b83>))
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>y_test shape: </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>y_test.shape</span><span style=color:#657b83>))
</span><span>
</span><span style=color:#586e75># Flatten the arrays to (num_px∗num_px∗3, 1)
</span><span>X_train </span><span style=color:#657b83>= </span><span>X_train.</span><span style=color:#b58900>reshape</span><span style=color:#657b83>(</span><span>X_train.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]</span><span>, </span><span style=color:#657b83>-</span><span style=color:#6c71c4>1</span><span style=color:#657b83>)</span><span>.T
</span><span>X_test  </span><span style=color:#657b83>= </span><span>X_test.</span><span style=color:#b58900>reshape</span><span style=color:#657b83>(</span><span>X_test.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]</span><span>, </span><span style=color:#657b83>-</span><span style=color:#6c71c4>1</span><span style=color:#657b83>)</span><span>.T
</span><span>
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>X_train new shape: </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>X_train.shape</span><span style=color:#657b83>))
</span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>X_test new shape: </span><span>" </span><span style=color:#657b83>+ </span><span style=color:#859900>str</span><span style=color:#657b83>(</span><span>X_test.shape</span><span style=color:#657b83>))
</span></code></pre><p>The print statements should say:<pre class=language-bash data-lang=bash style=color:#839496;background-color:#002b36><code class=language-bash data-lang=bash><span style=color:#b58900>Number</span><span> of training examples: m_train = 800
</span><span style=color:#b58900>Number</span><span> of testing examples: m_test = 200
</span><span>
</span><span style=color:#b58900>X_train</span><span> shape: (800, 64, 64, 3)
</span><span style=color:#b58900>y_train</span><span> shape: (1, 800)
</span><span>
</span><span style=color:#b58900>X_test</span><span> shape: (200, 64, 64, 3)
</span><span style=color:#b58900>y_test</span><span> shape: (1, 200)
</span><span>
</span><span style=color:#b58900>X_train</span><span> new shape: (12288, 800)
</span><span style=color:#b58900>X_test</span><span> new shape: (12288, 200)
</span></code></pre><ul><li><strong>3. Components of the Neural Network</strong></ul><p><strong>Define Sigmoid function</strong><pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>sigmoid</span><span style=color:#657b83>(</span><span style=color:#268bd2>z</span><span style=color:#657b83>):
</span><span>    </span><span style=color:#859900>return </span><span style=color:#657b83>(</span><span style=color:#6c71c4>1 </span><span style=color:#657b83>/ (</span><span style=color:#6c71c4>1</span><span style=color:#657b83>+</span><span>np.</span><span style=color:#b58900>exp</span><span style=color:#657b83>(-</span><span>z</span><span style=color:#657b83>)))
</span></code></pre><p><strong>Initialize weights</strong><p>This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. In further lectures they initiate randomly (<code>np.random.rand()</code>).<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>initialize_with_zeros</span><span style=color:#657b83>(</span><span style=color:#268bd2>dim</span><span style=color:#657b83>):
</span><span>		w </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>expand_dims</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>zeros</span><span style=color:#657b83>(</span><span>dim</span><span style=color:#657b83>)</span><span>, </span><span style=color:#6c71c4>1</span><span style=color:#657b83>)
</span><span>    b </span><span style=color:#657b83>= </span><span style=color:#859900>float</span><span style=color:#657b83>(</span><span style=color:#6c71c4>0</span><span style=color:#657b83>)
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>w, b
</span></code></pre><p><strong>Forward Propagation and calculate derivatives</strong><p><code>propagate</code> function.<ul><li>The inputs $w$ and $b$ are initially zeros, $X$ and $Y$ is the data and its labels.<li>It creates a vector with all the weights and bias: $A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$<li>It defines the cost function: $J = -\frac{1}{m}\sum_{i=1}^{m}(y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)}))$<li>It derives $J$ that later will be used to update $w$ and $b$.</ul><pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>propagate</span><span style=color:#657b83>(</span><span style=color:#268bd2>w</span><span>, </span><span style=color:#268bd2>b</span><span>, </span><span style=color:#268bd2>X</span><span>, </span><span style=color:#268bd2>Y</span><span style=color:#657b83>):
</span><span>    </span><span style=color:#586e75>"""
</span><span style=color:#586e75>    Arguments:
</span><span style=color:#586e75>    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
</span><span style=color:#586e75>    b -- bias, a scalar
</span><span style=color:#586e75>    X -- data of size (num_px * num_px * 3, number of examples)
</span><span style=color:#586e75>    Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)
</span><span style=color:#586e75>
</span><span style=color:#586e75>    Return:
</span><span style=color:#586e75>    grads -- dictionary containing the gradients of the weights and bias
</span><span style=color:#586e75>            (dw -- gradient of the loss with respect to w, thus same shape as w)
</span><span style=color:#586e75>            (db -- gradient of the loss with respect to b, thus same shape as b)
</span><span style=color:#586e75>    cost -- negative log-likelihood cost for logistic regression
</span><span style=color:#586e75>    """
</span><span>
</span><span>    </span><span style=color:#586e75>#
</span><span>    m </span><span style=color:#657b83>= </span><span>X.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>1</span><span style=color:#268bd2>]
</span><span>    A </span><span style=color:#657b83>= </span><span style=color:#b58900>sigmoid</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>dot</span><span style=color:#657b83>(</span><span>w.T, X</span><span style=color:#657b83>) + </span><span>b</span><span style=color:#657b83>)
</span><span>    cost </span><span style=color:#657b83>= (-</span><span style=color:#6c71c4>1</span><span style=color:#657b83>/</span><span>m</span><span style=color:#657b83>)* (</span><span>np.</span><span style=color:#b58900>dot</span><span style=color:#657b83>(</span><span>Y,np.</span><span style=color:#b58900>log</span><span style=color:#657b83>(</span><span>A</span><span style=color:#657b83>)</span><span>.T</span><span style=color:#657b83>)+ </span><span>np.</span><span style=color:#b58900>dot</span><span style=color:#657b83>((</span><span style=color:#6c71c4>1</span><span style=color:#657b83>-</span><span>Y</span><span style=color:#657b83>)</span><span>,</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>log</span><span style=color:#657b83>(</span><span style=color:#6c71c4>1</span><span style=color:#657b83>-</span><span>A</span><span style=color:#657b83>)</span><span>.T</span><span style=color:#657b83>)))
</span><span>
</span><span>    </span><span style=color:#586e75># Find grad (derivation of the function)
</span><span>    dw </span><span style=color:#657b83>= </span><span style=color:#6c71c4>1</span><span style=color:#657b83>/</span><span>m </span><span style=color:#657b83>* </span><span>np.</span><span style=color:#b58900>dot</span><span style=color:#657b83>(</span><span>X,</span><span style=color:#657b83>(</span><span>A</span><span style=color:#657b83>-</span><span>Y</span><span style=color:#657b83>)</span><span>.T</span><span style=color:#657b83>)
</span><span>    db </span><span style=color:#657b83>= </span><span style=color:#6c71c4>1</span><span style=color:#657b83>/</span><span>m </span><span style=color:#657b83>* </span><span>np.</span><span style=color:#b58900>sum</span><span style=color:#657b83>(</span><span>A</span><span style=color:#657b83>-</span><span>Y</span><span style=color:#657b83>)
</span><span>
</span><span>    </span><span style=color:#586e75># Format
</span><span>    cost </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>squeeze</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>array</span><span style=color:#657b83>(</span><span>cost</span><span style=color:#657b83>))
</span><span>    grads </span><span style=color:#657b83>= {</span><span>"</span><span style=color:#2aa198>dw</span><span>": dw,
</span><span>             "</span><span style=color:#2aa198>db</span><span>": db</span><span style=color:#657b83>}
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>grads, cost
</span></code></pre><p><strong>Minimize Log-Loss function (J)</strong><p><code>optimize</code> function does gradient descent.<ul><li>Runs <code>propagate</code> and takes the derivatives of $w$ and $b$ ($dw$, $db$) to update new values for $w$ and $b$.<li>It runs as many times as we want given by the parameter $num_itrations$.<li>Every new updated value of $w$ and $b$ is closer and closer to the optimal value that minimize the cost function (remember that the derivate is the slope os the cost function)</ul><pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>optimize</span><span style=color:#657b83>(</span><span style=color:#268bd2>w</span><span>, </span><span style=color:#268bd2>b</span><span>, </span><span style=color:#268bd2>X</span><span>, </span><span style=color:#268bd2>Y</span><span>,
</span><span>             </span><span style=color:#268bd2>num_iterations</span><span style=color:#657b83>=</span><span style=color:#6c71c4>100</span><span>,
</span><span>             </span><span style=color:#268bd2>learning_rate</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.009</span><span>,
</span><span>             </span><span style=color:#268bd2>print_cost</span><span style=color:#657b83>=</span><span style=color:#b58900>False</span><span style=color:#657b83>):
</span><span>    </span><span style=color:#586e75>"""
</span><span style=color:#586e75>    Arguments:
</span><span style=color:#586e75>    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
</span><span style=color:#586e75>    b -- bias, a scalar
</span><span style=color:#586e75>    X -- data of shape (num_px * num_px * 3, number of examples)
</span><span style=color:#586e75>    Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)
</span><span style=color:#586e75>    num_iterations -- number of iterations of the optimization loop
</span><span style=color:#586e75>    learning_rate -- learning rate of the gradient descent update rule
</span><span style=color:#586e75>
</span><span style=color:#586e75>    Returns:
</span><span style=color:#586e75>    params -- dictionary containing the weights w and bias b
</span><span style=color:#586e75>    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function
</span><span style=color:#586e75>    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.
</span><span style=color:#586e75>    """
</span><span>
</span><span>    w </span><span style=color:#657b83>= </span><span>copy.</span><span style=color:#b58900>deepcopy</span><span style=color:#657b83>(</span><span>w</span><span style=color:#657b83>)
</span><span>    b </span><span style=color:#657b83>= </span><span>copy.</span><span style=color:#b58900>deepcopy</span><span style=color:#657b83>(</span><span>b</span><span style=color:#657b83>)
</span><span>
</span><span>    costs </span><span style=color:#657b83>= []
</span><span>
</span><span>    </span><span style=color:#859900>for </span><span>i </span><span style=color:#859900>in range</span><span style=color:#657b83>(</span><span>num_iterations</span><span style=color:#657b83>):
</span><span>        grads, cost </span><span style=color:#657b83>= </span><span style=color:#b58900>propagate</span><span style=color:#657b83>(</span><span>w, b, X, Y</span><span style=color:#657b83>)
</span><span>
</span><span>        </span><span style=color:#586e75># Retrieve derivatives from grads
</span><span>        dw </span><span style=color:#657b83>= </span><span>grads</span><span style=color:#268bd2>[</span><span>"</span><span style=color:#2aa198>dw</span><span>"</span><span style=color:#268bd2>]
</span><span>        db </span><span style=color:#657b83>= </span><span>grads</span><span style=color:#268bd2>[</span><span>"</span><span style=color:#2aa198>db</span><span>"</span><span style=color:#268bd2>]
</span><span>
</span><span>        </span><span style=color:#586e75># update weights and bias
</span><span>        w </span><span style=color:#657b83>= </span><span>w </span><span style=color:#657b83>- </span><span>learning_rate</span><span style=color:#657b83>*</span><span>dw
</span><span>        b </span><span style=color:#657b83>= </span><span>b </span><span style=color:#657b83>- </span><span>learning_rate</span><span style=color:#657b83>*</span><span>db
</span><span>
</span><span>        </span><span style=color:#586e75># Record the costs
</span><span>        </span><span style=color:#859900>if </span><span>i </span><span style=color:#657b83>% </span><span style=color:#6c71c4>100 </span><span style=color:#657b83>== </span><span style=color:#6c71c4>0</span><span style=color:#657b83>:
</span><span>            costs.</span><span style=color:#b58900>append</span><span style=color:#657b83>(</span><span>cost</span><span style=color:#657b83>)
</span><span>
</span><span>            </span><span style=color:#586e75># Print the cost every 100 training iterations
</span><span>            </span><span style=color:#859900>if </span><span>print_cost</span><span style=color:#657b83>:
</span><span>                </span><span style=color:#859900>print </span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Cost after iteration </span><span style=color:#cb4b16>%i</span><span style=color:#2aa198>: </span><span style=color:#cb4b16>%f</span><span>" </span><span style=color:#657b83>%(</span><span>i, cost</span><span style=color:#657b83>))
</span><span>
</span><span>    params </span><span style=color:#657b83>= {</span><span>"</span><span style=color:#2aa198>w</span><span>": w,
</span><span>              "</span><span style=color:#2aa198>b</span><span>": b</span><span style=color:#657b83>}
</span><span>
</span><span>    grads </span><span style=color:#657b83>= {</span><span>"</span><span style=color:#2aa198>dw</span><span>": dw,
</span><span>             "</span><span style=color:#2aa198>db</span><span>": db</span><span style=color:#657b83>}
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>params, grads, costs
</span></code></pre><p><strong>Calculate predictions</strong><p>The previous function will output the learned $w$ and $b$. We are able to use $w$ and $b$ to predict the labels for a dataset $X$.<p><code>predict()</code> function:<ul><li>Takes the optimized $w$ and $b$ from <code>optimize</code> function.<li>Calculates prediction: $\hat{Y} = A = \sigma(w^T X + b)$<li>Converts the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5) and stores the predictions in a vector <code>Y_prediction</code>.</ul><pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>predict</span><span style=color:#657b83>(</span><span style=color:#268bd2>w</span><span>, </span><span style=color:#268bd2>b</span><span>, </span><span style=color:#268bd2>X</span><span style=color:#657b83>):
</span><span>    </span><span style=color:#586e75>'''
</span><span style=color:#586e75>    Arguments:
</span><span style=color:#586e75>    w -- weights, a numpy array of size (num_px * num_px * 3, 1)
</span><span style=color:#586e75>    b -- bias, a scalar
</span><span style=color:#586e75>    X -- data of size (num_px * num_px * 3, number of examples)
</span><span style=color:#586e75>
</span><span style=color:#586e75>    Returns:
</span><span style=color:#586e75>    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X
</span><span style=color:#586e75>    '''
</span><span>
</span><span>    m </span><span style=color:#657b83>= </span><span>X.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>1</span><span style=color:#268bd2>]
</span><span>    Y_prediction </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>zeros</span><span style=color:#657b83>((</span><span style=color:#6c71c4>1</span><span>, m</span><span style=color:#657b83>))
</span><span>    w </span><span style=color:#657b83>= </span><span>w.</span><span style=color:#b58900>reshape</span><span style=color:#657b83>(</span><span>X.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]</span><span>, </span><span style=color:#6c71c4>1</span><span style=color:#657b83>)
</span><span>
</span><span>    A </span><span style=color:#657b83>= </span><span style=color:#b58900>sigmoid</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>dot</span><span style=color:#657b83>(</span><span>w.T,X</span><span style=color:#657b83>) + </span><span>b</span><span style=color:#657b83>)
</span><span>
</span><span>   </span><span style=color:#859900>for </span><span>i </span><span style=color:#859900>in range</span><span style=color:#657b83>(</span><span>A.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>1</span><span style=color:#268bd2>]</span><span style=color:#657b83>):
</span><span>        </span><span style=color:#859900>if </span><span>A</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span>,i</span><span style=color:#268bd2>] </span><span style=color:#657b83>> </span><span style=color:#6c71c4>0.5</span><span style=color:#657b83>:
</span><span>            Y_prediction</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span>,i</span><span style=color:#268bd2>] </span><span style=color:#657b83>= </span><span style=color:#6c71c4>1
</span><span>        </span><span style=color:#859900>else</span><span style=color:#657b83>:
</span><span>            Y_prediction</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span>,i</span><span style=color:#268bd2>] </span><span style=color:#657b83>= </span><span style=color:#6c71c4>0
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>Y_prediction
</span></code></pre><ul><li><strong>4. Logistic Regression Model combining all functions</strong></ul><p>Builds the logistic regression model by calling the functions.<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>model</span><span style=color:#657b83>(</span><span style=color:#268bd2>X_TRAIN</span><span>, </span><span style=color:#268bd2>Y_TRAIN</span><span>,
</span><span>          </span><span style=color:#268bd2>X_TEST</span><span>, </span><span style=color:#268bd2>Y_TEST</span><span>,
</span><span>          </span><span style=color:#268bd2>num_iterations</span><span style=color:#657b83>=</span><span style=color:#6c71c4>2000</span><span>,
</span><span>          </span><span style=color:#268bd2>learning_rate</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.5</span><span>,
</span><span>          </span><span style=color:#268bd2>print_cost</span><span style=color:#657b83>=</span><span style=color:#b58900>False</span><span style=color:#657b83>):
</span><span>    </span><span style=color:#586e75>"""
</span><span style=color:#586e75>		Arguments:
</span><span style=color:#586e75>    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)
</span><span style=color:#586e75>    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)
</span><span style=color:#586e75>    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)
</span><span style=color:#586e75>    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)
</span><span style=color:#586e75>    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters
</span><span style=color:#586e75>    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()
</span><span style=color:#586e75>    print_cost -- Set to True to print the cost every 100 iterations
</span><span style=color:#586e75>
</span><span style=color:#586e75>    Returns:
</span><span style=color:#586e75>    d -- dictionary containing information about the model.
</span><span style=color:#586e75>    """
</span><span>
</span><span>    dim </span><span style=color:#657b83>= </span><span style=color:#268bd2>X_TRAIN</span><span>.shape</span><span style=color:#268bd2>[</span><span style=color:#6c71c4>0</span><span style=color:#268bd2>]
</span><span>    w, b </span><span style=color:#657b83>= </span><span style=color:#b58900>initialize_with_zeros</span><span style=color:#657b83>(</span><span>dim</span><span style=color:#657b83>)
</span><span>
</span><span>    params, grads, costs </span><span style=color:#657b83>= </span><span style=color:#b58900>optimize</span><span style=color:#657b83>(</span><span>w, b, </span><span style=color:#268bd2>X_TRAIN</span><span>, </span><span style=color:#268bd2>Y_TRAIN</span><span>,
</span><span>                                    num_iterations,
</span><span>                                    learning_rate,
</span><span>                                    print_cost</span><span style=color:#657b83>)
</span><span>
</span><span>    w </span><span style=color:#657b83>= </span><span>params</span><span style=color:#268bd2>[</span><span>"</span><span style=color:#2aa198>w</span><span>"</span><span style=color:#268bd2>]
</span><span>    b </span><span style=color:#657b83>= </span><span>params</span><span style=color:#268bd2>[</span><span>"</span><span style=color:#2aa198>b</span><span>"</span><span style=color:#268bd2>]
</span><span>
</span><span>    y_prediction_test </span><span style=color:#657b83>= </span><span style=color:#b58900>predict</span><span style=color:#657b83>(</span><span>w, b, </span><span style=color:#268bd2>X_TEST</span><span style=color:#657b83>)
</span><span>    y_prediction_train </span><span style=color:#657b83>= </span><span style=color:#b58900>predict</span><span style=color:#657b83>(</span><span>w, b, </span><span style=color:#268bd2>X_TRAIN</span><span style=color:#657b83>)
</span><span>
</span><span>    </span><span style=color:#586e75># Print train/test Errors
</span><span>    </span><span style=color:#859900>if </span><span>print_cost</span><span style=color:#657b83>:
</span><span>        </span><span style=color:#859900>print</span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Train accuracy: </span><span style=color:#cb4b16>{}</span><span style=color:#2aa198> %</span><span>".</span><span style=color:#b58900>format</span><span style=color:#657b83>(</span><span style=color:#6c71c4>100 </span><span style=color:#657b83>- </span><span>np.</span><span style=color:#b58900>mean</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>abs</span><span style=color:#657b83>(</span><span>y_prediction_train </span><span style=color:#657b83>- </span><span style=color:#268bd2>Y_TRAIN</span><span style=color:#657b83>)) * </span><span style=color:#6c71c4>100</span><span style=color:#657b83>))
</span><span>        </span><span style=color:#859900>print</span><span style=color:#657b83>(</span><span>"</span><span style=color:#2aa198>Test accuracy: </span><span style=color:#cb4b16>{}</span><span style=color:#2aa198> %</span><span>".</span><span style=color:#b58900>format</span><span style=color:#657b83>(</span><span style=color:#6c71c4>100 </span><span style=color:#657b83>- </span><span>np.</span><span style=color:#b58900>mean</span><span style=color:#657b83>(</span><span>np.</span><span style=color:#b58900>abs</span><span style=color:#657b83>(</span><span>y_prediction_test </span><span style=color:#657b83>- </span><span style=color:#268bd2>Y_TEST</span><span style=color:#657b83>)) * </span><span style=color:#6c71c4>100</span><span style=color:#657b83>))
</span><span>
</span><span>
</span><span>    d </span><span style=color:#657b83>= {</span><span>"</span><span style=color:#2aa198>costs</span><span>": costs,
</span><span>         "</span><span style=color:#2aa198>Y_prediction_test</span><span>": y_prediction_test,
</span><span>         "</span><span style=color:#2aa198>Y_prediction_train</span><span>" : y_prediction_train,
</span><span>         "</span><span style=color:#2aa198>w</span><span>" : w,
</span><span>         "</span><span style=color:#2aa198>b</span><span>" : b,
</span><span>         "</span><span style=color:#2aa198>learning_rate</span><span>" : learning_rate,
</span><span>         "</span><span style=color:#2aa198>num_iterations</span><span>": num_iterations</span><span style=color:#657b83>}
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>d
</span><span>
</span><span>logistic_regression_model </span><span style=color:#657b83>= </span><span style=color:#b58900>model</span><span style=color:#657b83>(</span><span>X_train,
</span><span>                                  y_train,
</span><span>                                  X_test,
</span><span>                                  y_test,
</span><span>                                  </span><span style=color:#268bd2>num_iterations</span><span style=color:#657b83>=</span><span style=color:#6c71c4>10000</span><span>,
</span><span>                                  </span><span style=color:#268bd2>learning_rate</span><span style=color:#657b83>=</span><span style=color:#6c71c4>0.001</span><span>,
</span><span>                                  </span><span style=color:#268bd2>print_cost</span><span style=color:#657b83>=</span><span style=color:#b58900>True</span><span style=color:#657b83>)
</span></code></pre><p>The model should print (if you kept the seed):<pre class=language-bash data-lang=bash style=color:#839496;background-color:#002b36><code class=language-bash data-lang=bash><span style=color:#b58900>[..............]
</span><span style=color:#b58900>Cost</span><span> after iteration 9600: 0.401098
</span><span style=color:#b58900>Cost</span><span> after iteration 9700: 0.399880
</span><span style=color:#b58900>Cost</span><span> after iteration 9800: 0.398671
</span><span style=color:#b58900>Cost</span><span> after iteration 9900: 0.397471
</span><span style=color:#b58900>train</span><span> accuracy: 86.125 </span><span style=color:#859900>%
</span><span style=color:#b58900>test</span><span> accuracy: 59.5 </span><span style=color:#859900>%
</span></code></pre><h2 id=3-shallow-neural-network><a aria-label="Anchor link for: 3-shallow-neural-network" class=zola-anchor href=#3-shallow-neural-network>3. Shallow Neural Network</a></h2><h3 id=3-1-neural-networks-overview-and-vectorized-implementation><a aria-label="Anchor link for: 3-1-neural-networks-overview-and-vectorized-implementation" class=zola-anchor href=#3-1-neural-networks-overview-and-vectorized-implementation>3.1 Neural Networks Overview and Vectorized Implementation</a></h3><p>Until now, we saw the case of logistic regression as a single layer Neural Network.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/10.png><p>“Deep” from Deep Learning refers to the layers of a neural network - They are “deep” hidden layers that are stacked. The only layer that receives the $X$ values as inputs is the <code>Input layer</code>. After that, the output of each layer is passed to the next one.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/11.png><p>Each node computes $a$, the output of the sigmoid activation function passed through the linear regression. This calculation can be vectorized into matrix operations:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/12.png><p>By vectorization, sigmoid transformation doesn’t have to loop each node - all $i$ weights and bias can stacked horizontally. The output $Z^1$ of the first layer goes through the activation function and $A^1$ passes to the second layer as input.<p>The 2-layer NN above only needs to calculate $/sigmoid(Z^1)$ and $/sigmoid(Z^2)$.<h3 id=3-2-activation-functions><a aria-label="Anchor link for: 3-2-activation-functions" class=zola-anchor href=#3-2-activation-functions>3.2 Activation functions</a></h3><p>When you build your Neural Network, one of the choices you get to make is what activation function to use in the hidden layers.<p>Tanh or ReLU are recommended as a default, but different ones for your application.<p><strong>Tanh</strong><p>The sigmoid function goes within zero and one. An activation function that almost always works better than the sigmoid function is the tangent function or also called <strong>hyperbolic tangent function (Tanh)</strong>:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/14.png><p>It goes between 1 and - 1. The tanh function is almost always strictly superior. The main difference is that using tanh instead of sigmoid outputs the data with mean zero (centering the data), which makes the learning for the next layer easier.<p>The <strong>one exception</strong> is for the output layer because if y is either 0 or 1, then it makes sense for y hat to be a number, the one to output that's between 0 and 1 rather than between minus 1 and 1.<p>One of the downsides of both the sigmoid function and the tanh function is that if $z$ is either very large or very small, then the gradient or the derivative or the slope of this function becomes very small.<p><strong>ReLU</strong><p>Another choice that is very popular in machine learning is what's called the <strong>rectify linear unit (ReLU)</strong>. So the value function looks like:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/15.png><p>The derivative is 1 as $z$ is positive. And the derivative or slope is 0 when $z is negative. Is an increasingly popular default choice of activation function.<p>In practice, using the ReLU activation function, your neural network will often <strong>learn much faster</strong> than tanh or sigmoid activation function. The main reason is that there is less of these effects of the slope of the function going to 0, which slows down learning.<p><strong>Leaky ReLU Function</strong><p>It is similar to ReLU, but instead of returning zero for negative inputs, it returns a small negative value. This helps to avoid the "dying ReLU" problem, where some neurons can become permanently inactive during training.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/16.png><h3 id=3-3-gradient-descent><a aria-label="Anchor link for: 3-3-gradient-descent" class=zola-anchor href=#3-3-gradient-descent>3.3 Gradient Descent</a></h3><p>The left equations describe the derivation, the right ones the equivalent in python vectorization.<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/13.png><p>Once the parameters are set, they update based on a learning rate $$\alpha: \theta = \theta - \alpha \frac{\partial J }{ \partial \theta }$$<h3 id=3-4-random-initialization><a aria-label="Anchor link for: 3-4-random-initialization" class=zola-anchor href=#3-4-random-initialization>3.4 Random Initialization</a></h3><p>When you change your neural network, it's important to initialize the weights randomly. Random initialization of the model is a common practice.<p>We can initialize the weights randomly and the bias at zero.<p>If we initialize weights and bias to zeros, all activations functions ($a_{i,j,...}$) will be equal:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/17.png><p>All the neurons units would start off computing the same function and therefore all the hidden neurons completely identical. Using more hidden layers would be useless, after the first layer all the sequent layers calculate the same with the same weight optimisation.<p>Instead, NNs starts with random initialization (bias still can be zero).<pre class=language-python data-lang=python style=color:#839496;background-color:#002b36><code class=language-python data-lang=python><span style=color:#859900>def </span><span style=color:#b58900>random_initialization</span><span style=color:#657b83>(</span><span style=color:#268bd2>dim</span><span style=color:#657b83>):
</span><span>		w </span><span style=color:#657b83>= </span><span>np.</span><span style=color:#b58900>expand_dims</span><span style=color:#657b83>(</span><span>np.random.</span><span style=color:#b58900>randn</span><span style=color:#657b83>(</span><span>dim</span><span style=color:#657b83>)*</span><span style=color:#6c71c4>0.01</span><span>, </span><span style=color:#6c71c4>1</span><span style=color:#657b83>)
</span><span>    b </span><span style=color:#657b83>= </span><span style=color:#859900>float</span><span style=color:#657b83>(</span><span style=color:#6c71c4>0</span><span style=color:#657b83>)
</span><span>
</span><span>    </span><span style=color:#859900>return </span><span>w, b
</span></code></pre><p><code>0.01</code> or a very small number is chosen to not end up in the extremes of the activation function at the beginning, making the calculation of the derivations in very small steps (not that much slope).<p>With a small number is more likely that $w$ starts in the range of the activation function that there is more slope (think in the middle of the tanh function), and the derivations are in bigger steps making a quicker convergence.<h2 id=4-deep-neural-networks><a aria-label="Anchor link for: 4-deep-neural-networks" class=zola-anchor href=#4-deep-neural-networks>4. Deep Neural Networks</a></h2><p>Deep Neural Networks have the same structure of “shallow models” with more layers.<p>The number of layers is another hyperparameter that we can modify to improve the accuracy of the model.<h3 id=4-1-forward-propagation-in-deep-networks><a aria-label="Anchor link for: 4-1-forward-propagation-in-deep-networks" class=zola-anchor href=#4-1-forward-propagation-in-deep-networks>4.1 Forward Propagation in Deep Networks</a></h3><p>Same as before. In <strong>Forward propagation</strong>, the weight vectors and bias vectors of the next layer depends on the vectors of the previous one:<p>$$Z^{[1]} = W^{[1]} X + b^{[1]}\tag{1}$$ $$A^{[1]} = \tanh(Z^{[1]})\tag{2}$$ $$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\tag{3}$$ $$A^{[2]} = \tanh(Z^{[2]})\tag{4}$$ $$Z^{[3]} = W^{[2]} A^{[2]} + b^{[3]}\tag{5}$$ $$A^{[3]} = \tanh(Z^{[3]})\tag{6}$$ $$... \tag{7}$$ $$\hat{Y} = A^{[n]} = \sigma(Z^{[n]})\tag{8}$$<h3 id=4-2-getting-the-dimensions-of-a-neural-network><a aria-label="Anchor link for: 4-2-getting-the-dimensions-of-a-neural-network" class=zola-anchor href=#4-2-getting-the-dimensions-of-a-neural-network>4.2 Getting the dimensions of a Neural Network</a></h3><p>The shape of the matrix derivates are equal to the original matrixes.<p>For the vectorized implementation, instead of going 1 by 1 observation, it stack the observations columnise. Therefore the shapes are:<p><img alt src=https://www.pipegalera.com/mostly_books/course-deeplearning-neural-networks/./images/18.png><p>Generalizing:<ul><li>$W^i$ will always have a shape $(n^l, n^{(l-1)})$<li>$Z^i$ will always have a shape $(n^l,1)$<li>$b^i$ will always have a shape $(n^l,1)$<li>$X^i$ will always have a shape $(n^{(l-1)},1)$</ul><p>Otherwise the matrix multiplication will be either wrong or they won't run at all in code.<h3 id=4-3-parameters-vs-hyperparameters><a aria-label="Anchor link for: 4-3-parameters-vs-hyperparameters" class=zola-anchor href=#4-3-parameters-vs-hyperparameters>4.3 Parameters vs Hyperparameters</a></h3><ul><li><p>Parameters: These are the parameters in the model that must be determined using the training data set. These are the fitted parameters $W$ and $b$.</p><li><p>Hyperparameters: These are adjustable parameters that must be tuned in order to obtain a model with optimal performance: learning rate, number of iterations, the choice of the activation function, number of hidden layers, number of units in each hidden layer...</p></ul></section></article></main><div class=giscus></div><script async crossorigin issue-term=pathname repo=YOUR_NAME/YOUR_REPO src=https://utteranc.es/client.js theme=github-light></script><footer><div class=footer-content><p>100% human written content by Pipe Galera © 2025. Theme: <a href=https://github.com/not-matthias/apollo>Apollo</a>.</div></footer></div>